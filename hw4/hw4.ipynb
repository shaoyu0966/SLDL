{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning and Deep Learning HW4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (463715, 90)\n",
      "X_subtrain shape =  (417344, 90)\n",
      "X_valid shape =  (46371, 90)\n",
      "\n",
      "Y_train shape =  (463715,)\n",
      "Y_subtrain shape =  (417344,)\n",
      "Y_valid shape =  (46371,)\n",
      "\n",
      "X_test shape =  (51630, 90)\n",
      "Y_test shape =  (51630,)\n"
     ]
    }
   ],
   "source": [
    "#load packages\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "# Load data\n",
    "with open('msd_full.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "doscaling = 1\n",
    "if doscaling == 1:\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    # standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test'].astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_mean = Y_train.mean()\n",
    "Y_train_keep = Y_train.copy()\n",
    "Y_test_keep = Y_test.copy()\n",
    "Y_train = Y_train - y_mean\n",
    "Y_test = Y_test - y_mean\n",
    "\n",
    "\n",
    "# validation is the last 10% of training, subtraining is the first 90% of training\n",
    "nvalid = int(X_train.shape[0] * 0.1)\n",
    "nsubtrain = X_train.shape[0] - nvalid\n",
    "\n",
    "X_subtrain = X_train[0:nsubtrain, :].astype('float32')\n",
    "X_valid = X_train[nsubtrain:, :].astype('float32')\n",
    "Y_subtrain = Y_train[0:nsubtrain].astype('float32')\n",
    "Y_valid = Y_train[nsubtrain:].astype('float32')\n",
    "\n",
    "Y_subtrain_keep = Y_train_keep[0:nsubtrain].astype('float32')\n",
    "Y_valid_keep = Y_train_keep[nsubtrain:].astype('float32')\n",
    "\n",
    "print(\"X_train shape = \", X_train.shape)\n",
    "print(\"X_subtrain shape = \", X_subtrain.shape)\n",
    "print(\"X_valid shape = \", X_valid.shape)\n",
    "print()\n",
    "print(\"Y_train shape = \", Y_train.shape)\n",
    "print(\"Y_subtrain shape = \", Y_subtrain.shape)\n",
    "print(\"Y_valid shape = \", Y_valid.shape)\n",
    "print()\n",
    "print(\"X_test shape = \", X_test.shape)\n",
    "print(\"Y_test shape = \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Oridinary Least Square (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "ols = sm.OLS(Y_train, X_train)\n",
    "ols_result = ols.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 parameters: [ 5.30975265 -2.88088114 -1.53234348  0.05737583 -0.33952889]\n"
     ]
    }
   ],
   "source": [
    "print(f'The first 5 parameters: {ols_result.params[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted Y is [-5.81070695  0.03250657  5.13960445 ... -1.39829429 -0.26047668\n",
      "  0.05193056]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "Y_predict = ols_result.predict(X_test)\n",
    "print(f'The predicted Y is {Y_predict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 90.44315624585404\n"
     ]
    }
   ],
   "source": [
    "# RMSE\n",
    "print(f'RMSE = {mean_squared_error(Y_test, Y_predict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. MLP with Four Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.7.1rc2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "print(\"torch version:\", torch.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "class Dataset(data.Dataset):\n",
    "    \n",
    "  def __init__(self, Xnp, Ynp):\n",
    "        self.labels = Ynp\n",
    "        self.nobs = Xnp.shape[0]        \n",
    "        self.Xnp = Xnp\n",
    "        self.Ynp = Ynp\n",
    "        \n",
    "  def __len__(self):\n",
    "        return self.nobs\n",
    "    \n",
    "  def __getitem__(self, index):     \n",
    "        X = self.Xnp[index]\n",
    "        y = self.Ynp[index]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtrain length 417344\n",
      "valid length 46371\n",
      "test length 51630\n"
     ]
    }
   ],
   "source": [
    "# create dataloader\n",
    "subtrain_set = Dataset(X_subtrain, Y_subtrain)    \n",
    "valid_set = Dataset(X_valid, Y_valid)\n",
    "test_set = Dataset(X_test, Y_test)\n",
    "print('subtrain length', len(subtrain_set))\n",
    "print('valid length', len(valid_set))\n",
    "print('test length', len(test_set))\n",
    "\n",
    "batch_size = 1000\n",
    "subtrain_loader = data.DataLoader(subtrain_set, batch_size=batch_size)\n",
    "valid_loader = data.DataLoader(valid_set, batch_size=batch_size)\n",
    "test_loader = data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "# create MLP model\n",
    "d_hidden = 45\n",
    "d_input = subtrain_set.Xnp.shape[1]\n",
    "d_output = 1\n",
    "CUDA_VISIBLE_DEVICES = 4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'using {device}')\n",
    "\n",
    "MLP = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d_input, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_output)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "lr = 0.00001\n",
    "momentum = 0\n",
    "weight_decay = 0\n",
    "optimizer = torch.optim.SGD(MLP.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# loss\n",
    "loss_func = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 418: Loss = 33725.8671875\n",
      "Epoch 1, Step 836: Loss = 33955.67578125\n",
      "Epoch 2, Step 1254: Loss = 33308.0703125\n",
      "Epoch 3, Step 1672: Loss = 32549.05859375\n",
      "Epoch 4, Step 2090: Loss = 32314.919921875\n",
      "Epoch 5, Step 2508: Loss = 32432.322265625\n",
      "Epoch 6, Step 2926: Loss = 32580.04296875\n",
      "Epoch 7, Step 3344: Loss = 32655.34765625\n",
      "Epoch 8, Step 3762: Loss = 32587.314453125\n",
      "Epoch 9, Step 4180: Loss = 32320.896484375\n",
      "Epoch 10, Step 4598: Loss = 32263.677734375\n",
      "Epoch 11, Step 5016: Loss = 31648.2421875\n",
      "Epoch 12, Step 5434: Loss = 31666.12890625\n",
      "Epoch 13, Step 5852: Loss = 31723.912109375\n",
      "Epoch 14, Step 6270: Loss = 31790.080078125\n",
      "Epoch 15, Step 6688: Loss = 31371.58984375\n",
      "Epoch 16, Step 7106: Loss = 31610.505859375\n",
      "Epoch 17, Step 7524: Loss = 31531.337890625\n",
      "Epoch 18, Step 7942: Loss = 31178.9375\n",
      "Epoch 19, Step 8360: Loss = 31465.39453125\n",
      "Epoch 20, Step 8778: Loss = 30891.0546875\n",
      "Epoch 21, Step 9196: Loss = 30806.0625\n",
      "Epoch 22, Step 9614: Loss = 30433.31640625\n",
      "Epoch 23, Step 10032: Loss = 29745.01953125\n",
      "Epoch 24, Step 10450: Loss = 29775.30859375\n",
      "Epoch 25, Step 10868: Loss = 29735.740234375\n",
      "Epoch 26, Step 11286: Loss = 29358.455078125\n",
      "Epoch 27, Step 11704: Loss = 29393.34375\n",
      "Epoch 28, Step 12122: Loss = 30004.130859375\n",
      "Epoch 29, Step 12540: Loss = 29830.6953125\n",
      "Epoch 30, Step 12958: Loss = 29427.91796875\n",
      "Epoch 31, Step 13376: Loss = 28767.67578125\n",
      "Epoch 32, Step 13794: Loss = 29359.36328125\n",
      "Epoch 33, Step 14212: Loss = 28595.37109375\n",
      "Epoch 34, Step 14630: Loss = 28350.20703125\n",
      "Epoch 35, Step 15048: Loss = 28795.759765625\n",
      "Epoch 36, Step 15466: Loss = 28829.18359375\n",
      "Epoch 37, Step 15884: Loss = 28271.255859375\n",
      "Epoch 38, Step 16302: Loss = 28264.626953125\n",
      "Epoch 39, Step 16720: Loss = 27705.23046875\n",
      "Epoch 40, Step 17138: Loss = 27854.796875\n",
      "Epoch 41, Step 17556: Loss = 28334.71484375\n",
      "Epoch 42, Step 17974: Loss = 29021.369140625\n",
      "Epoch 43, Step 18392: Loss = 28268.83984375\n",
      "Epoch 44, Step 18810: Loss = 27632.494140625\n",
      "Epoch 45, Step 19228: Loss = 27365.443359375\n",
      "Epoch 46, Step 19646: Loss = 27212.892578125\n",
      "Epoch 47, Step 20064: Loss = 27481.615234375\n",
      "Epoch 48, Step 20482: Loss = 28919.08203125\n",
      "Epoch 49, Step 20900: Loss = 28217.330078125\n",
      "Epoch 50, Step 21318: Loss = 28313.474609375\n",
      "Epoch 51, Step 21736: Loss = 28979.73046875\n",
      "Epoch 52, Step 22154: Loss = 27895.29296875\n",
      "Epoch 53, Step 22572: Loss = 28926.875\n",
      "Epoch 54, Step 22990: Loss = 28683.458984375\n",
      "Epoch 55, Step 23408: Loss = 28285.416015625\n",
      "Epoch 56, Step 23826: Loss = 27260.193359375\n",
      "Epoch 57, Step 24244: Loss = 27726.953125\n",
      "Epoch 58, Step 24662: Loss = 28318.05859375\n",
      "Epoch 59, Step 25080: Loss = 28022.37109375\n",
      "Epoch 60, Step 25498: Loss = 27691.41796875\n",
      "Epoch 61, Step 25916: Loss = 27816.98046875\n",
      "Epoch 62, Step 26334: Loss = 28135.837890625\n",
      "Epoch 63, Step 26752: Loss = 28195.6015625\n",
      "Epoch 64, Step 27170: Loss = 27957.22265625\n",
      "Epoch 65, Step 27588: Loss = 27981.73828125\n",
      "Epoch 66, Step 28006: Loss = 28360.73046875\n",
      "Epoch 67, Step 28424: Loss = 28530.36328125\n",
      "Epoch 68, Step 28842: Loss = 29093.49609375\n",
      "Epoch 69, Step 29260: Loss = 29506.044921875\n",
      "Epoch 70, Step 29678: Loss = 29206.150390625\n",
      "Epoch 71, Step 30096: Loss = 28191.375\n",
      "Epoch 72, Step 30514: Loss = 27906.9296875\n",
      "Epoch 73, Step 30932: Loss = 27823.537109375\n",
      "Epoch 74, Step 31350: Loss = 28038.296875\n",
      "Epoch 75, Step 31768: Loss = 28315.263671875\n",
      "Epoch 76, Step 32186: Loss = 27965.96484375\n",
      "Epoch 77, Step 32604: Loss = 27959.71875\n",
      "Epoch 78, Step 33022: Loss = 28502.16796875\n",
      "Epoch 79, Step 33440: Loss = 29302.53125\n",
      "Epoch 80, Step 33858: Loss = 29656.25\n",
      "Epoch 81, Step 34276: Loss = 29035.376953125\n",
      "Epoch 82, Step 34694: Loss = 28184.8984375\n",
      "Epoch 83, Step 35112: Loss = 28631.6015625\n",
      "Epoch 84, Step 35530: Loss = 28258.533203125\n",
      "Epoch 85, Step 35948: Loss = 28308.6484375\n",
      "Epoch 86, Step 36366: Loss = 28160.732421875\n",
      "Epoch 87, Step 36784: Loss = 27334.052734375\n",
      "Epoch 88, Step 37202: Loss = 27256.484375\n",
      "Epoch 89, Step 37620: Loss = 28712.8828125\n",
      "Epoch 90, Step 38038: Loss = 27245.248046875\n",
      "Epoch 91, Step 38456: Loss = 27889.861328125\n",
      "Epoch 92, Step 38874: Loss = 26922.48046875\n",
      "Epoch 93, Step 39292: Loss = 27206.55078125\n",
      "Epoch 94, Step 39710: Loss = 28811.724609375\n",
      "Epoch 95, Step 40128: Loss = 27815.0546875\n",
      "Epoch 96, Step 40546: Loss = 26645.107421875\n",
      "Epoch 97, Step 40964: Loss = 27023.248046875\n",
      "Epoch 98, Step 41382: Loss = 26227.3671875\n",
      "Epoch 99, Step 41800: Loss = 27207.064453125\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "max_epoch = 100\n",
    "max_step = 5000\n",
    "valid_interval = 100\n",
    "step_count = 0\n",
    "best_step_count = 0\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for batch, (inputs, targets) in enumerate(subtrain_loader):\n",
    "        targets = targets.reshape((-1, 1))\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        MLP.to(device)\n",
    "        MLP.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = MLP(inputs)\n",
    "        loss = loss_func(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step_count += 1\n",
    "    print(f'Epoch {epoch}, Step {step_count}: Loss = {loss.item()}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
