{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning and Deep Learning HW4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (463715, 90)\n",
      "X_subtrain shape =  (417344, 90)\n",
      "X_valid shape =  (46371, 90)\n",
      "\n",
      "Y_train shape =  (463715,)\n",
      "Y_subtrain shape =  (417344,)\n",
      "Y_valid shape =  (46371,)\n",
      "\n",
      "X_test shape =  (51630, 90)\n",
      "Y_test shape =  (51630,)\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "# load data\n",
    "with open('msd_full.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "doscaling = 1\n",
    "if doscaling == 1:\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    # standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test'].astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_mean = Y_train.mean()\n",
    "Y_train_keep = Y_train.copy()\n",
    "Y_test_keep = Y_test.copy()\n",
    "Y_train = Y_train - y_mean\n",
    "Y_test = Y_test - y_mean\n",
    "\n",
    "\n",
    "# validation is the last 10% of training, subtraining is the first 90% of training\n",
    "nvalid = int(X_train.shape[0] * 0.1)\n",
    "nsubtrain = X_train.shape[0] - nvalid\n",
    "\n",
    "X_subtrain = X_train[0:nsubtrain, :].astype('float32')\n",
    "X_valid = X_train[nsubtrain:, :].astype('float32')\n",
    "Y_subtrain = Y_train[0:nsubtrain].astype('float32')\n",
    "Y_valid = Y_train[nsubtrain:].astype('float32')\n",
    "\n",
    "Y_subtrain_keep = Y_train_keep[0:nsubtrain].astype('float32')\n",
    "Y_valid_keep = Y_train_keep[nsubtrain:].astype('float32')\n",
    "\n",
    "print(\"X_train shape = \", X_train.shape)\n",
    "print(\"X_subtrain shape = \", X_subtrain.shape)\n",
    "print(\"X_valid shape = \", X_valid.shape)\n",
    "print()\n",
    "print(\"Y_train shape = \", Y_train.shape)\n",
    "print(\"Y_subtrain shape = \", Y_subtrain.shape)\n",
    "print(\"Y_valid shape = \", Y_valid.shape)\n",
    "print()\n",
    "print(\"X_test shape = \", X_test.shape)\n",
    "print(\"Y_test shape = \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Oridinary Least Square (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "ols = sm.OLS(Y_train, X_train, hasconst=True)\n",
    "ols_result = ols.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 parameters: [ 5.30975265 -2.88088114 -1.53234348  0.05737583 -0.33952889]\n"
     ]
    }
   ],
   "source": [
    "print(f'The first 5 parameters: {ols_result.params[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted Y is [-5.81070695  0.03250657  5.13960445 ... -1.39829429 -0.26047668\n",
      "  0.05193056]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "Y_predict = ols_result.predict(X_test)\n",
    "print(f'The predicted Y is {Y_predict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 9.510160684544402\n"
     ]
    }
   ],
   "source": [
    "# RMSE\n",
    "print(f'RMSE = {mean_squared_error(Y_test, Y_predict, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. MLP with Four Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.7.1rc2\n",
      "using cuda\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('torch version:', torch.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'using {device}')\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "class Dataset(data.Dataset):\n",
    "    \n",
    "  def __init__(self, Xnp, Ynp):\n",
    "        self.labels = Ynp\n",
    "        self.nobs = Xnp.shape[0]        \n",
    "        self.Xnp = Xnp\n",
    "        self.Ynp = Ynp\n",
    "        \n",
    "  def __len__(self):\n",
    "        return self.nobs\n",
    "    \n",
    "  def __getitem__(self, index):     \n",
    "        X = self.Xnp[index]\n",
    "        y = self.Ynp[index]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtrain length 417344\n",
      "valid length 46371\n",
      "test length 51630\n"
     ]
    }
   ],
   "source": [
    "# create dataloader\n",
    "subtrain_set = Dataset(X_subtrain, Y_subtrain)    \n",
    "valid_set = Dataset(X_valid, Y_valid)\n",
    "test_set = Dataset(X_test, Y_test)\n",
    "print('subtrain length', len(subtrain_set))\n",
    "print('valid length', len(valid_set))\n",
    "print('test length', len(test_set))\n",
    "\n",
    "batch_size = 1000\n",
    "subtrain_loader = data.DataLoader(subtrain_set, batch_size=batch_size)\n",
    "valid_loader = data.DataLoader(valid_set, batch_size=batch_size)\n",
    "test_loader = data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "def RMSE(model, data_loader):\n",
    "    n_obs = 0\n",
    "    sse = 0\n",
    "    with torch.no_grad():\n",
    "        for _batch, (_inputs, _targets) in enumerate(data_loader):\n",
    "            _targets = _targets.reshape((-1, 1))\n",
    "            _inputs, _targets = _inputs.to(device), _targets.to(device)\n",
    "            _outputs = model(_inputs)\n",
    "            n_obs += _targets.shape[0]\n",
    "            sse += (_targets - _outputs).pow(2).sum(0)\n",
    "    return np.sqrt(sse.cpu().numpy()[0] / n_obs)\n",
    "\n",
    "    \n",
    "def train(model, optim, loss_f, max_epoch, max_step, valid_interval, weight_path, train_rmse):\n",
    "    \n",
    "    step_count = 0\n",
    "    best_step_count = 0\n",
    "    best_valid_rmse = np.inf\n",
    "    all_train_rmse = []\n",
    "    all_valid_rmse = []\n",
    "    all_step = []\n",
    "\n",
    "    for epoch in range(1, max_epoch+1):\n",
    "        for batch, (inputs, targets) in enumerate(subtrain_loader):\n",
    "\n",
    "            targets = targets.reshape((-1, 1))\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            model.to(device)\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_f(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            step_count += 1\n",
    "\n",
    "            # check train/validation RMSE\n",
    "            if (step_count % valid_interval == 0):\n",
    "\n",
    "                # subtrain, validation RMSE\n",
    "                if train_rmse:\n",
    "                    train_rmse = RMSE(model, subtrain_loader)\n",
    "                    all_train_rmse.append(train_rmse)\n",
    "                valid_rmse = RMSE(model, valid_loader)\n",
    "                all_valid_rmse.append(valid_rmse)\n",
    "                all_step.append(step_count)\n",
    "\n",
    "                # update weight\n",
    "                if valid_rmse < best_valid_rmse:\n",
    "                    best_step_count = step_count\n",
    "                    best_valid_rmse = valid_rmse\n",
    "                    torch.save(model, weight_path)\n",
    "\n",
    "                # early stopping\n",
    "                elif (step_count - best_step_count >= max_step):\n",
    "                    print(f'early stopping, step {step_count}, validation RMSE = {valid_rmse}')\n",
    "                    return all_step, all_train_rmse, all_valid_rmse\n",
    "                \n",
    "        print(f'Epoch {epoch}, Step {step_count}: Loss = {loss.item()}, best_valid_rmse = {best_valid_rmse}')\n",
    "    return all_step, all_train_rmse, all_valid_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 34811.2265625, best_valid_rmse = 8.94553009496888\n",
      "Epoch 2, Step 836: Loss = 34226.890625, best_valid_rmse = 8.71017680179246\n",
      "Epoch 3, Step 1254: Loss = 34025.21875, best_valid_rmse = 8.688642361852033\n",
      "Epoch 4, Step 1672: Loss = 33730.0390625, best_valid_rmse = 8.613268462556709\n",
      "Epoch 5, Step 2090: Loss = 33249.12890625, best_valid_rmse = 8.603137778883292\n",
      "Epoch 6, Step 2508: Loss = 33170.2578125, best_valid_rmse = 8.603137778883292\n",
      "Epoch 7, Step 2926: Loss = 33348.11328125, best_valid_rmse = 8.603116472186596\n",
      "Epoch 8, Step 3344: Loss = 33474.8828125, best_valid_rmse = 8.597652382522488\n",
      "Epoch 9, Step 3762: Loss = 33425.3046875, best_valid_rmse = 8.596214084982677\n",
      "Epoch 10, Step 4180: Loss = 33322.63671875, best_valid_rmse = 8.596214084982677\n",
      "Epoch 11, Step 4598: Loss = 32925.5390625, best_valid_rmse = 8.596214084982677\n",
      "Epoch 12, Step 5016: Loss = 33085.90625, best_valid_rmse = 8.58240239366959\n",
      "Epoch 13, Step 5434: Loss = 32851.0703125, best_valid_rmse = 8.58240239366959\n",
      "Epoch 14, Step 5852: Loss = 32281.1875, best_valid_rmse = 8.58240239366959\n",
      "Epoch 15, Step 6270: Loss = 32069.12109375, best_valid_rmse = 8.576209122962839\n",
      "Epoch 16, Step 6688: Loss = 32177.71875, best_valid_rmse = 8.576209122962839\n",
      "Epoch 17, Step 7106: Loss = 32468.765625, best_valid_rmse = 8.576209122962839\n",
      "Epoch 18, Step 7524: Loss = 32280.25, best_valid_rmse = 8.576209122962839\n",
      "Epoch 19, Step 7942: Loss = 32291.38671875, best_valid_rmse = 8.576209122962839\n",
      "Epoch 20, Step 8360: Loss = 32116.17578125, best_valid_rmse = 8.576209122962839\n",
      "Epoch 21, Step 8778: Loss = 32028.76171875, best_valid_rmse = 8.576209122962839\n",
      "Epoch 22, Step 9196: Loss = 31936.49609375, best_valid_rmse = 8.576209122962839\n",
      "Epoch 23, Step 9614: Loss = 31729.8359375, best_valid_rmse = 8.576209122962839\n",
      "Epoch 24, Step 10032: Loss = 31334.64453125, best_valid_rmse = 8.576209122962839\n",
      "Epoch 25, Step 10450: Loss = 30936.8828125, best_valid_rmse = 8.576209122962839\n",
      "Epoch 26, Step 10868: Loss = 30678.36328125, best_valid_rmse = 8.576209122962839\n",
      "early stopping, step 11100, validation RMSE = 8.643195686989584\n"
     ]
    }
   ],
   "source": [
    "# create MLP model\n",
    "d_hidden = 45\n",
    "d_input = subtrain_set.Xnp.shape[1]\n",
    "d_output = 1\n",
    "\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d_input, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_output)\n",
    ")\n",
    "mlp = mlp.float()\n",
    "mlp.to(device)\n",
    "weight_path = './MLP45_weight'\n",
    "\n",
    "# optimizer\n",
    "lr = 0.00001\n",
    "momentum = 0\n",
    "weight_decay = 0\n",
    "sgd_optimizer = torch.optim.SGD(mlp.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# loss\n",
    "l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# train\n",
    "all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=sgd_optimizer, loss_f=l2_loss, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=True)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'H=45, Training & Validation RMSE')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABC5ElEQVR4nO3dd3hUVfrA8e+bZEJo0psgzQIKSEBEBUEQCyiLZS1gA91V0bXh7lrXsuvPtuuqy9rWrliw4iKKqCjq2hCkCNKb0ntPSJn398d7J5mESSXJJOH9PM88mdvOPXcmc997zrn3HFFVnHPOufwS4p0B55xzlZMHCOecczF5gHDOOReTBwjnnHMxeYBwzjkXkwcI55xzMXmAcFWeiMwVkX5lvW5lJiL9RGRl1HSBx5V/3VLs6ykRuaO027uqywNENSQiy0XkpHzzRojI//YhzUNFJF1EXoma11ZEVER2Rr2KPJGISOt826iI7Iqa7lOSvKlqJ1WdUtbrlpSItBORL0RkR/AdXFLIuikislVEToyx7BERebsk+y6r44r1f6KqI1X1nn1NO8a+7haRzOA73yoi34jIcVHL+wX/G+/m265rMH9K1LwzRGSmiGwXkY0iMllE2sbYT+S1tayPpzryAOGK63HghwKW1VfVOsGryBOJqv4StX6dYHbXqHlfRdYVkaQyyHtFuQ9YDjQEjgV+LmhFVU0H3gDyBBERSQSGAS+VWy4rlzeC/4HGwOfAW/mWbwB6iUijqHnDgYWRCRE5BHgZ+CNQD2gHPAGE8+8n6lW/zI+kGvIA4YokIkOBrcDkCtjXCBH5OriK3gzcLSIHi8hnIrIpuDp8VUTqR22TU2IKrhbfFJGXgyv5uSLSo5TrdheRGcGyt0TkDRH5v0KynwWsVNVMVV2rqtOKONyXgN+KSK2oeadiv8uJInKpiMwL9r9URK4s5HOLPq6aIvKiiGwRkZ+Bo/Ote4uILAnS/VlEzgrmHw48BRwXfZUdpPV/UdtfLiKLRWSziIwXkQOjlqmIjBSRRcH+HxcRKeJzQFWzgFeBliLSJGpRBvAeMDRIPxE4L1g3IhVYpqqT1exQ1XdU9Zei9usK5wFiPyUiE4JifazXhKj1DgD+hl2dFWSFiKwUkRdEpHEZZO8YYCnQFLgXEOB+4EDgcOAg4O5Cth8CjAXqA+OBx0q6rogkA+OAF7ESwevAWUXkeyrwJxEZWMR6AKjqN8Aa4Oyo2RcDrwUnzPXAYOAA4FLgERHpXoyk7wIODl6nYlfc0ZYAfbCr7b8Cr4hIC1WdB4wEvi3oKjuoErsfO0m3AFZgn1+0wVhQ6hqsd2pRGQ4+70uATcCWfItfJrekdSowF1gdtfxHoGNwUdFfROrgyoQHiOrrveiTPlbkzqGqg1W1fgGvwVGr3gM8p6q/xtjHRuxE0AY4CqhL3iu70lqtqv9W1SxVTVPVxar6iaruUdUNwMPACYVs/z9V/VBVs4Ex2ImqpOseCyQBo4MSwbtYAIhJRHoDNwKnAM+KyKnB/EODUk9BV9E5J78gGJ9BUL2kqh+o6pLgqvgL4GPsxF6U84B7VXVz8L2Njl6oqm+p6mpVDavqG8AioGcx0gW4EHheVX9U1T3ArViJo23UOg+o6tbgCv5z7Aq/wLwG/59pwOXAOUFwjM7vN0BDEemAfVYv51u+FOgHtATeBDYGpZ7oQHFevougz4t5vPs1DxDV15nRJ33g6pImICKpwEnAI7GWq+pOVZ0WnMjXAdcApwQnun2RJxiJSFMRGSsiq0RkO/AKVmddkLVR73cDKYW0ZRS07oHAKs3bm2WsIBlxDTAmOJGfBYwJgkQvYHK+dKK9DPQXkZbAOcBiVZ0BICKDROS7oCpnK3AahR93xIH58roieqGIXCLWoBu5eOhczHQjaeekp6o7sav+llHr5P9MC7uifzP4/2wGzMEuNGIZg33G/bGSXR6q+p2qnqeqTbAg2he4Pf9+ol79C8mTC1SlBkBXhkRkIgVfjX6lqoOwq7K2wC/BBXAdIFFEjlDVWFUdkZNgkXXORch/Mr0/mHekqm4SkTMpvNqoLKzB6sMl6uR+EFY9E0sS1gaBqv4g1m7zLlZdMqSgnajqLyLyFXZlPojg6lhEagDvYFfM/1XVTBF5j+J9tmuCvM4NpltHFohIG+AZYABWlZQtIjOj0i2qe+fVWIkxkl5toBGwqhj5KpCqbgzaWH4QkddUdU2+VcYAi4GXVXV3Yc0awef/Lhb43D7wEsR+SlUH5burI/o1KFjtaaweOzV4PQV8QFCnLCLHiEgHEUkQu8tkNDBFVbcFy0eIyPIyyG5dYCewNbjS/nMZpFmUb4Fs4BoRSRKRMyi8GuYt4DoR6SsiCdhJejl2ZRwqYl8vYVfHvcmtoksGamB38WSJyCCs+qo43gRuFZEGItIKuDZqWW0sCGwAEJFLyXsiXQe0CtoEYnkNuFREUoMgdh/wvaouL2beCqSq84FJwE0xli3DqhVvz79MRI4PGs6bBtMdsaD83b7maX/nAcIVSFV3B3fjrFXVtdhJOj1oBwBoD3wE7MCqB/Zgt2hGHAR8XQZZ+SvQHdiGBah3C19936lqBtZ4/DvsDq6LgAnYMcZa/03gFiyobsUatR/BgtkEEWkda7vA20ADrCpqTZDeDuA67GS/BbgAa0Qvjr9i1UDLsHaLMVH5/Bn4JxYA1wFdyPsdfYaVPNaKyMYYxzkZuAMr3azBLiCGFjNfxfEP4IrIyT7fvv+nqqtjbLMVCwg/ichO7H9yHPD3qHXOl7zPQeyMtQ+XlxRcNercvhGRj4Hrg7tjqjwR+R54SlVfiHdenKsIHiCcK4CInAAswO7WuhCrYmsfo37cuWrJG6mdK1gHrIqnDtY4fY4HB7c/8RKEc865mLyR2jnnXEzVqoqpcePG2rZt23hnwznnqozp06dvDB4w3Eu1ChBt27Zl2rSi+kdzzjkXISIrClrmVUzOOedi8gDhnHMuJg8QzjnnYqpWbRDOubKVmZnJypUrSU9Pj3dW3D5KSUmhVatWhEJFdQ2WywOEc65AK1eupG7durRt25ZiDAznKilVZdOmTaxcuZJ27doVezuvYnLOFSg9PZ1GjRp5cKjiRIRGjRqVuCToAcI5VygPDtVDab5HDxDAPffApEnxzoVzzlUuHiCABx+ETz6Jdy6cc/lt2rSJ1NRUUlNTad68OS1btsyZzsjIKHTbadOmcd111xW5j169epVJXqdMmUK9evXo1q0bHTt25E9/+lPOshdffBERYfLkyTnzxo0bh4jw9ttvAzBhwgS6detG165dOeKII/jPf/4DwN13353nuFNTU9m6dWuZ5Lko3kgNpCbMpubmRuQdVtc5F2+NGjVi5syZgJ0o69Spk+fEm5WVRVJS7NNYjx496NGjR5H7+Oabb8okrwB9+vRhwoQJpKWl0a1bN8466yx69+4NQJcuXXj99dcZMGAAAGPHjqVr166A3S12xRVXMHXqVFq1asWePXtYvnx5TrqjRo3Kc9wVxUsQwMc7jqXvj4/GOxvOuWIYMWIEN954I/379+fmm29m6tSp9OrVi27dutGrVy8WLFgA2BX94MGDAQsul112Gf369aN9+/aMHj06J706derkrN+vXz/OOeccOnbsyIUXXkikt+sPP/yQjh07cvzxx3PdddflpFuQmjVrkpqayqpVuUN19+nTh6lTp5KZmcnOnTtZvHgxqampAOzYsYOsrCwaNWoEQI0aNejQoUPZfGD7wEsQQJaEIDMz3tlwrlK74QYILubLTGoqPPpoybdbuHAhn376KYmJiWzfvp0vv/ySpKQkPv30U2677TbeeeedvbaZP38+n3/+OTt27KBDhw5cddVVez0TMGPGDObOncuBBx5I7969+frrr+nRowdXXnklX375Je3atWPYsGF7pZ3fli1bWLRoEX379s2ZJyKcdNJJTJo0iW3btjFkyBCWLVsGQMOGDRkyZAht2rRhwIABDB48mGHDhpGQYNfwjzzyCK+88goADRo04PPPPy/5h1YKXoIAMiUZyfIA4VxVce6555KYmAjAtm3bOPfcc+ncuTOjRo1i7ty5Mbc5/fTTqVGjBo0bN6Zp06asW7dur3V69uxJq1atSEhIIDU1leXLlzN//nzat2+f8/xAYQHiq6++4sgjj6R58+YMHjyY5s2b51k+dOhQxo4dy9ixY/dK59lnn2Xy5Mn07NmThx56iMsuuyxn2ahRo5g5cyYzZ86ssOAAXoIAIFtCSFbhDV7O7e9Kc6VfXmrXrp3z/o477qB///6MGzeO5cuX069fv5jb1KhRI+d9YmIiWVlZxVqnJIOqRdogFi5cyPHHH89ZZ52VU40EFoDmzJlDzZo1Oeyww/bavkuXLnTp0oWLL76Ydu3a8eKLLxZ73+XBSxBAVkLISxDOVVHbtm2jZUu7waQ8TqgdO3Zk6dKlOY3Gb7zxRpHbHHbYYdx66608+OCDey27//77ue+++/LM27lzJ1OmTMmZnjlzJm3atNmnfJcFL0EAWQnJJHoJwrkq6aabbmL48OE8/PDDnHjiiWWefs2aNXniiScYOHAgjRs3pmfPnsXabuTIkTz00EM57QwRgwYN2mtdVeXvf/87V155JTVr1qR27dp5gl10GwTAe++9R0UMjlatxqTu0aOHlmbAoGU1j+DXep3ou/atcsiVc1XXvHnzOPzww+OdjbjbuXMnderUQVX5wx/+wKGHHsqoUaPina0Si/V9ish0VY15P7BXMQHZCSESsr2KyTkX2zPPPENqaiqdOnVi27ZtXHnllfHOUoUotyomEXkeGAysV9XOwbyGwBtAW2A5cJ6qbomx7XJgB5ANZBUU3cpKVmIyidlexeSci23UqFFVssSwr8qzBPEiMDDfvFuAyap6KDA5mC5If1VNLe/gABBO9BKEc87lV24BQlW/BDbnm30G8FLw/iXgzPLaf0lkJyaT6AHCOefyqOg2iGaqugYg+Nu0gPUU+FhEpovIFeWdqXBiiKSwVzE551y0ynqba29VXS0iTYFPRGR+UCLZSxBArgBo3bp1qXZmAcJLEM45F62iSxDrRKQFQPB3fayVVHV18Hc9MA4o8MZjVX1aVXuoao8mTZqUKlPhpGQvQThXCfXr149J+QZrefTRR7n66qsL3SZyu/tpp50Ws2vsu+++m4ceeqjQfb/33nv8/PPPOdN33nknn376aQlyH1tV6ha8ogPEeGB48H448N/8K4hIbRGpG3kPnALMKc9MaWKIRPUShHOVzbBhwxg7dmyeebH6MSrIhx9+SP369Uu17/wB4m9/+xsnnXRSqdLKr0+fPsyYMYMZM2YwYcIEvv7665xlkW7BI2J1C/7+++8za9YsZsyYkadrkeg+m2bOnFnqY48otwAhIq8D3wIdRGSliPwOeAA4WUQWAScH04jIgSLyYbBpM+B/IjILmAp8oKoflVc+AcKhZJI8QDhX6ZxzzjlMmDCBPXv2ALB8+XJWr17N8ccfz1VXXUWPHj3o1KkTd911V8zt27Zty8aNGwG499576dChAyeddFJOl+BgzzgcffTRdO3ald/+9rfs3r2bb775hvHjx/PnP/+Z1NRUlixZwogRI3Ku4idPnky3bt3o0qULl112WU7+2rZty1133UX37t3p0qUL8+fPL/T4Knu34OXWBqGqBYX4ATHWXQ2cFrxfCnQtr3zFokkhQl7F5Fzh4tDfd6NGjejZsycfffQRZ5xxBmPHjuX8889HRLj33ntp2LAh2dnZDBgwgNmzZ3PkkUfGTGf69OmMHTuWGTNmkJWVRffu3TnqqKMAOPvss7n88ssB+Mtf/sJzzz3Htddey5AhQxg8eDDnnHNOnrTS09MZMWIEkydP5rDDDuOSSy7hySef5IYbbgCgcePG/PjjjzzxxBM89NBDPPvsswUeX2XvFtyfpAZICnkJwrlKKrqaKbp66c0336R79+5069aNuXPn5qkOyu+rr77irLPOolatWhxwwAEMGTIkZ9mcOXPo06cPXbp04dVXXy2wu/CIBQsW0K5du5zeWIcPH86XX+beQ3P22WcDcNRRR+UZFS5/fqpCt+CV9S6mCqWhZEJ4CcK5QsWpv+8zzzyTG2+8kR9//JG0tDS6d+/OsmXLeOihh/jhhx9o0KABI0aMID09vdB0RCTm/BEjRvDee+/RtWtXXnzxxTy9qsZSVP91kS7DC+pSHKpOt+BeggAIhQiRSTXqt9C5aqNOnTr069ePyy67LOdqevv27dSuXZt69eqxbt06Jk6cWGgaffv2Zdy4caSlpbFjxw7ef//9nGU7duygRYsWZGZm8uqrr+bMr1u3Ljt27NgrrY4dO7J8+XIWL14MwJgxYzjhhBNKdWyVvVtwL0EAGgSIzExITo53bpxz+Q0bNoyzzz47p6qpa9eudOvWjU6dOtG+fXt69+5d6Pbdu3fn/PPPJzU1lTZt2tCnT5+cZffccw/HHHMMbdq0oUuXLjlBYejQoVx++eWMHj06p3EaICUlhRdeeIFzzz2XrKwsjj76aEaOHFnqY6vM3YJ7d9/AtwNu5+jPHiR9RxbB+OXOOby77+rGu/suBQmFSCKbzD3heGfFOecqDQ8QkFOvlLHL72RyzrkIDxCAJIcAyErzAOFcftWpGnp/Vprv0QMEuQEic7cHCOeipaSksGnTJg8SVZyqsmnTJlJSUkq0nd/FBEiKVTFl7fZnIZyL1qpVK1auXMmGDRvinRW3j1JSUmjVqlWJtvEAASR4CcK5mEKhEO3atYt3NlyceBUTkBCUILLTPUA451yEBwiiGqm9isk553J4gAASUyxAeAnCOedyeYAgqoopzUsQzjkX4QGC3BKEPwfhnHO5PEAACTUsQIT3eIBwzrkIDxBAUi2vYnLOufw8QJBbxeQlCOecy+UBAkisaSWIcLqXIJxzLsIDBJBU00oQmuElCOeci/AAQW6A8Com55zL5QGC3EZq3eNVTM45F+EBAgjVCkoQXsXknHM5PECQW4LASxDOOZfDAwS5JQhvpHbOuVweIMh9kppMDxDOORfhAQIgOahiyvAqJueci/AAARDyEoRzzuXnAQIgMdH+eoBwzrkcHiAARNhDMpLpVUzOORfhASKQJSHI8hKEc85FeIAIZEoyCV6CcM65HB4gAlkSQrwE4ZxzOcotQIjI8yKyXkTmRM1rKCKfiMii4G+DArYdKCILRGSxiNxSXnmMlp3gAcI556KVZwniRWBgvnm3AJNV9VBgcjCdh4gkAo8Dg4AjgGEickQ55hOArIRkErK9isk55yLKLUCo6pfA5nyzzwBeCt6/BJwZY9OewGJVXaqqGcDYYLtylS0hErwE4ZxzOSq6DaKZqq4BCP42jbFOS+DXqOmVwbyYROQKEZkmItM2bNhQ6oxlJSaT6CUI55zLURkbqSXGPC1oZVV9WlV7qGqPJk2alHqn2QkhErK9BOGccxEVHSDWiUgLgODv+hjrrAQOippuBawu74yFE0MkhD1AOOdcREUHiPHA8OD9cOC/Mdb5AThURNqJSDIwNNiuXGV7FZNzzuVRnre5vg58C3QQkZUi8jvgAeBkEVkEnBxMIyIHisiHAKqaBVwDTALmAW+q6tzyymdEODFEopcgnHMuR1J5JayqwwpYNCDGuquB06KmPwQ+LKesxRRODJEU3lWRu3TOuUqtMjZSx0U4KdlLEM45F8UDREATQyR5gHDOuRweIALhUDJJ6o3UzjkX4QEioEkhktRLEM45F+EBIsIDhHPO5eEBIqChZJLJQAt8Zts55/YvHiAiQiFCZJKVFe+MOOdc5eABIhApQWR6LZNzzgEeIHJIspUgMvxGJuecAzxA5AqqmDIzvBHCOefAA0SuGskkoGSkZcc7J845Vyl4gAhIKARAVpo3QjjnHHiAyCE1LEBk7vJGCOecAw8QOSQ5GfAShHPORXiACCREShC7PUA45xx4gMghNawEkZ3mVUzOOQceIHIkpngjtXPORfMAEYhUMXmAcM454wEikJBiVUzhdK9ics458ACRw6uYnHMuLw8QgUiA8BKEc84ZDxCBxJrBXUzpXoJwzjnwAJEjpwSxxwOEc86BB4gcSbW8kdo556J5gAhEShCa4SUI55wDDxA5QrW8kdo556IVGiBE5MSo9+3yLTu7vDIVD5EqJi9BOOecKaoE8VDU+3fyLftLGeclriIlCA8QzjlnigoQUsD7WNNVWk4JYo9XMTnnHBQdILSA97GmqzRvpHbOubySiljeXkTGY6WFyHuC6XYFb1YFBUOOSqaXIJxzDooOEGdEvX8o37L801VbsjdSO+dctEIDhKp+ET0tIiGgM7BKVdeXZ8YqXFCCINMDhHPOQdG3uT4lIp2C9/WAWcDLwAwRGVbanYrI9SIyR0TmisgNMZb3E5FtIjIzeN1Z2n0Vm1cxOedcHkVVMfVR1ZHB+0uBhap6pog0ByYCr5d0hyLSGbgc6AlkAB+JyAequijfql+p6uCSpl9qCQlkkeglCOecCxR1F1P05fTJwHsAqrp2H/Z5OPCdqu5W1SzgC+CsfUivzGRJCMnyAOGcc1B0gNgqIoNFpBvQG/gIQESSgJql3OccoK+INBKRWsBpwEEx1jtORGaJyMRINVcsInKFiEwTkWkbNmwoZZZMpiR7FZNzzgWKqmK6EhgNNAduiCo5DAA+KM0OVXWeiDwIfALsxNo1svKt9iPQRlV3ishpWMnl0ALSexp4GqBHjx779GyGlyCccy5XUXcxLQQGxpg/CZhU2p2q6nPAcwAich+wMt/y7VHvPxSRJ0SksapuLO0+iyM7IURClpcgnHMOiggQIjK6sOWqel1pdioiTVV1vYi0Bs4Gjsu3vDmwTlVVRHpiVWGbSrOvksiSZBKyvQThnHNQdBXTSKzN4E1gNWXX/9I7ItIIyAT+oKpbRGQkgKo+BZwDXCUiWUAaMFRVy71rj+wEr2JyzrmIogJEC+Bc4HysneAN4B1V3bIvO1XVPjHmPRX1/jHgsX3ZR2lkJSaTmO1VTM45B0XcxaSqm1T1KVXtD4wA6gNzReTiCshbhctOCHkVk3POBYoqQQAgIt2BYdizEBOB6eWZqXgJJ4RIDHsJwjnnoOhG6r8Cg4F5wFjg1uDhtmopOymZxDQvQTjnHBRdgrgDWAp0DV73iQhYY7Wq6pHlm72KFU4MkRj2AOGcc1B0gKheYz4UwQLE7nhnwznnKoWiHpRbEWu+iCQCQ4GYy6uqcFIySeFt8c6Gc85VCkV1932AiNwqIo+JyClirsWqnc6rmCxWHE0MkaTeSO2cc1B0FdMYYAvwLfB74M9AMnCGqs4s36xVvHAomWRvg3DOOaAYY1KrahcAEXkW2Ai0VtUd5Z6zONCkEEl4gHDOOSi6u++cs6WqZgPLqmtwACApRMirmJxzDii6BNFVRCI9qwpQM5iO3OZ6QLnmroJpKJkQmWRnQ2JivHPjnHPxVdRdTPvXaTIUIkQmGRlQs7TDITnnXDVRVBXTfkWTk0kmw4elds45PEDkIVElCOec2995gIgWCnkJwjnnAh4gokiNZJLIJmNPuY9N5JxzlZ4HiGjJIQAyd3sRwjnnPEBESYgEiF3eCOGccx4gotVIBiA73UsQzjnnASJKQg0vQTjnXIQHiCgJQQkiy0eVc845DxDRIiUIr2JyzjkPEHlEAkTWbq9ics45DxBREmp6I7VzzkV4gIhSp4GVINb+6iUI55zzABGl+UFWgvj2Cy9BOOecB4goEjwoN2taJjt3xjkzzjkXZx4gooUsQEhWBp9+Gue8OOdcnHmAiJZsVUz1a2Xy/vtxzotzzsWZB4hoQQni2O4ZfPABhMNxzo9zzsWRB4horVuDCAOb/si6dfDDD/HOkHPOxY8HiGjNmkGfPhwx900SEmDChHhnyDnn4scDRH7nnUfSgp+5MHVuubdDTP/LOH5sORjUByhyzlU+cQkQInK9iMwRkbkickOM5SIio0VksYjMFpHuFZa53/4WEhK4qtGbzJoFX39dfrva9d4ndF/9Adt/3VZ+O3HOuVKq8AAhIp2By4GeQFdgsIgcmm+1QcChwesK4MkKy2Dz5nDCCRyz4k3atFYuvxz27CmfXSVvXgvAxhm/ls8OnHNuH8SjBHE48J2q7lbVLOAL4Kx865wBvKzmO6C+iLSosByedx4JC+cz5uY5zJsHDzxQPruptXMdANvmeIBwzlU+8QgQc4C+ItJIRGoBpwEH5VunJRB91lwZzKsYZ58NCQn0WfMmw4bBfffBvHllv5t6aVaC2L3AA4RzrvKp8AChqvOAB4FPgI+AWUBWvtUk1qax0hORK0RkmohM27BhQ9lksmlT6N8f3niDRx9R6tSB664rm6QjVKFxlgWI8PJfyjZx55wrA3FppFbV51S1u6r2BTYDi/KtspK8pYpWwOoC0npaVXuoao8mTZqUXSYvuQQWLaLp/aO47lpl8mRYv77skt+2aie12Q1A0hovQTjnKp943cXUNPjbGjgbeD3fKuOBS4K7mY4FtqnqmgrN5MUXW7HhX/9i5K+3o6p88EHZJb9p7tqc97U2e4BwzlU+SXHa7zsi0gjIBP6gqltEZCSAqj4FfIi1TSwGdgOXVngOReDRRyE9nWZP38999eoxfvzNXFpGOdm+0ALEZmlIg50eIJxzlU9cAoSq9okx76mo9wr8oUIzFYsIPPkkrFnDDR/dS4tJo0hPTyYlZd+T3rnU7mBaUr8HXbZ8YY0SEqvpxTnn4sOfpC5KQgJceik1M3fQNe1bPv+8bJLN/MVKENs7Hk0Ke9i1vIwa2J1zrox4gCiOE09Ek5L4Tegjxo8vmyTDa9aSTQJJPboB/rCcc67y8QBRHPXqIb16cXatj5gwoWy6TkrcsI7NCU2o3akt4A/LOecqHw8QxTVwIO23zSRr5Rpmzsy7aM8eSnyHU8rWtWxJaU6jVLubN22hPwvhnKtcPEAU18CBAJzKx3tVMz35JAweDLNmFT+5OrvWsqtOM5p3aUI6NQiv8BKEc65y8QBRXF27QrNmXNT4I8aNy7vo/XFZ9OT7Eg0wVH/POvbUb07NWsLqhFaE/GE551wl4wGiuBIS4NRT6b37Y36alc2i4NnvLVvg4K9e5HuOZdXHc4uVVMYepWl4LdlNmgOwqeZB/rCcc67S8QBREgMHUnP3ZnowjXfesVkTJ8LJOgmApO+LN3jEhsXbqEEGCS2aAbC9fmt/WM7Flyp89ZUPXuXy8ABREiefDCL8/sCJOQHi/feyGSCfAdDy1++KNXbE5p/tGYgabawEsafpQTTJXA3Z2eWSbeeK9Mkn0LcvfPNNvHPiKhEPECXRuDH07s1ZWW8xbZqyaBGsnjiThrqZ7FAKPfU75swpOpkdiyxA1GpnJQgOOogksklfVrHdTTmXY/58+/vTT/HNh6tUPECU1AUX0Hj9zxzJbK6/Ho7d+SkAO4f9niOYx+wvtxaZRNpy62ajXgcrQYTa262uG2f4ra4uTpYssb8LFsQ3H65S8QBRUueeC0lJ3Nj0VSZOhFMTPiHcqQsHXHQGAFs/KfpWpsxfrQTRqJMFiLpHWIDY7g/LuXhZutT+eoBwUTxAlFTjxnDqqZyV/jq12MXx/I+EU09Geh5NGKHGzO+KTELWrSWTJJKbNQDIeVgufZEHCBcnkRJEpKrJOTxAlM6FF3LA9pXcJg+QHN4DJ50E9eqxvvERtF37HenphW+etGkdm5Oa2a2zQIuO9dhBHX9YzsVHOAzLlkFiIixfTpH/wG6/4QGiNIYMgdq1uS3pQTQUgj7We3la12Ppqd/z0+y8twqGw/D447BihU3X3L6WbTWb5SyvU1dYlXAQSWs9QLhSeuIJ+PTT0m27Zo0Fhd697TbXxYvLNm+uyvIAURq1a8OZZyKZmchxx0GdOgAccNIxNGYTCycuybP6668pM695hhvPW4kqHLB7LbvrNs+zzi91jqDV6h/8PvTSWr0aPvww3rmIj7Q0GDUK7ruvdNtH2h9OO83+ejWTC3iAKK0LL7S/J52UM6vhaccCkPZ5bjtEWhpMvf5VnuEKrpt6IW+MVRpkrCOjUd4AsazDIBqnryQ8y28zLJV77rEOsTbsh+NqfPstZGTA999DZmbJt4+0PwT9jXlDtYvwAFFap5wC//gHXHllzizpdAS7E+uQMvM7du2yeU88uIObN99EZp36nMCXfDnyVZqxDm3aLE9yjS62q7fVz5ThwNf7kylTrPRV2mqWqiwyitXu3SXrMTJi6VJrDzv8cGjVygOEy+EBorQSE+FPf4KmTfPM296xJ923fcYJR+3ks8+A++/jQNYQmvQBOzr24P+2X0cS2SQemLcE0W9YC6bTnez3yzFATJkCCxeWX/rxsm5dbrXIxx/HNy/x8Pnn0KaNvS/Nk9BLlkDr1pCcDB07eoBwOTxAlLHmf76Ew2U+7y7qwpgBL3BNxsNsO+MS6NWLui89Tn22ApDSNm+AaNwYZrc6nVa/fgubNu2d8MaN8PrrpW+j+PFH6yrk/POrXzvHF1/Y30MOsQBRFY5v1iw46CCK9eh9YXbtgqlTYdgwS+/r4vUHlsfSpdC+vb3v0MGCbVX4DCPS0+G552DnznjnpHytWgVHHw1/+YuVFiuAB4iyNnw48uWXHNg2xAtcBsnJ1HvyAVvWsyfpF/4OgEP7NN9r04TfnE4iYTa/PmnvdG++GS64wO5WKak9e2D4cLudauZMq6uuTqZMsRsF/vQna6z++ed456ho99wDK1fCyy/vWzpff23tDv37211IpS1BHHywve/QAbZvt1JZVfH44/D739uJs7xMmWLtjuvXl98+ijJqFMyYAffea9WB+ccdKA+qWm1eRx11lFYau3er3nOP6ttv552/ZYvqgw+qZmTstclPs7J1HU10Uc8L8i7Ytk3DtWppOBRSTU5WnTEjd1lGhmp2duF5ufVWVVAdO1a1Th3V4cNjr/fkk6q9eqnOn1/U0VUuRxyhOnCg6i+/2HH+85/ls5+nn1a95BLVzMx9S2fePFUR1cRE1XbtVMPh0qd1yy2qoZDqzp2qo0fb8a9Ykbt827bCt9++3ba5/36b/vhjm54ypfR5qkhpaaotWqgmJakmJKjOnFn4+j/+qNqkieqxx6reeafq9OmFrx8Oqz72mH1XoDpoUNG/t2hr16p++aVqVlbxt4ll4kTb/z33qH7xheqRR9r0s8/uW7qqCkzTAs6pcT+pl+WrUgWIUgiHVd+ufYluCzXM+w/19NOqoGckvq97mhyo2qGD6pIlqjffbCf8UaNiJ5idrTphgv1wLrvM5o0cqZqSorppU951ly9XrVnT/iUOOED1gw/K5yDL2rp1lucHHrDpww9XPfXU3OW7d+/bCThiyRLVGjVsXzffvG9pjRhhn/UDD1h6RZ2kCnPMMaq9e9v76dMtvddes+n//MdObOPHF7z9zJm2zZtv2vSKFTb91FOlz1NphcOqzz9vgb64nnrK8vvWW6qNG9tnUdAJfMcO1UMPVW3WzAJEQoJt+/HHsdfPzla94gpbZ/Dg3O/r4YeLztfu3ar33mu/T1Bt08a2z/+7U415sbhXWu3b2+8+PT13m1NOscD4+edF56cQHiCqkGdPeUMVNH3y/3LmZXTvqXMSOiuE9cKWn2s48o8tYv84oVDeq8bNm1X/+EfVgw6y9dq1U926VZ99VvWH52ZpzKvss85SrVXLrnZSUy3tq69W/eabkl0xVbS33rLj+fZbm77+eguAaWmqX39twe6aa/Z9P4MH24/9vPNsf++/v/c64bDq4sWFp7Nihf2or7tOdeNGO4Hfemvp8rR9u23/l7/YdGamau3adrxr16rWq2d5rVfPAlws775r60ybZtPZ2Ra8CrroyC8jQ/Xnn4uf58gJLpbPPrO8HHmknRSLkplp/9vHHGOf/XPP2fYvvhh7/eHDLShESkcbN9oFxYEH2vv8nn/e0vvzn+1zCYdVzzjDfm8FBfWsLNWXXrKAALb+yy+r9u9v082bq/4v+G1nZ1vQCIWsZLpuXew077jDtp08Oe/8LVtUO3ZUbdhQddGiQj+qwniAqEI+fXuL7iGkG7oOsB/JLDuhX8+j+u9/2zf2Wt8nVa+8Un8eN1+vHLRCsxJDqldeaQmEw6qnnWYnod/8RnXMGNXt23P+1w86SDX7uF52JRU58X/4oeapZti1S/X3v7fqrMhGjz22b8XkcNj+iZ95xk5KhVmzRvXf/1bt00e1a1f7wRdUrfOHP9hJMXIV9sEHlue777YTekqKTU+aVPy8Ll1qJ54bbrCT8PjxlsZDD1ngSU1VbdDASl3R/vY3LbLYf9119t1EAvrJJ6seckjpSjmRY40+cZx4omq3bnbCCYWsaqJBA8tzrJPuP/5haWzZkjuva1f7HyqOiy6y7SdOLHy9LVtUL7jASmGffBJ7nfPOs4sUUL388qL3/cortu5//2vT2dmqxx1nVUjr1+ddd8wYW/fOO/PO//FH+5x++9u838HmzbFLJBs3qrZsqVq3ri279FLVu+6y6r0nn1Tt1Mn20727Bbxo06fbdx0KqT7yiP0+wap1QyHV+vUtjeh8TJpkFwEXXRT7M1i8WLVRIytd7NhR9GcWgweIKiQtTfWa2s9rNqJpfU7StIt/r+kk66VD7ArnxhvtWxsyxC7yQfWl2ldZ+8Ty5ZoTRf7975w0v/vOzvWHHGKLvrwi+LGcf75dwRx8sP2D7dmTNzNbt9rVzwkn2PrHHqv60097Zzozc+9to40bl1uaiZR8JkyIve6zz+bW93bqZCcrsJLSbbfZ8k8/zf0xdO5sRe2InTvtxwaqhx1mV84dO6q2amXHk//DHjnSqnyif5QXXmhpiFi+W7Wydo5IEFq0yEomnTurbthg877/3vJds6a9oj+njAwLioMGWZojRuQu+89/LK+zZhX8+cWyfLmll5yc98R/xx25/xiRkkkkkFxyyd6B6KqrLIBEO/98+7yL8oaVdrVWLau2KegK+MsvVVu3ts+nVSsr0eQvdaxZY4Fz1ChrVwE7qS9Zovrqq3tfVKSl2dV/5855T+CzZ1sQGjw491inTbOLiD59Yl9oRKqOogP7VVcV3KYxa5ZdQJ1wgpUIIv/Xkf+5N98suNS9ebN9b2D/Y489ZvmcN8+CO1gwSEuzefXqqXbpYhcqBfniC2vXLGVVqgeIKubbb1WvTHlRs7Ef+msM1dmzbVlamv0mQiELFi+9pNqSXzUrKdlOlDVqqJ5+es4/y6+/Wgm6bVs7l3Xtqnpkh3QNnz807z93QfWwqpbWK6/YFVVSkp1Uf/3VShTPPWeNhHXrWvVO/iqW11+3E0P37qpPPGE/rm7d7ASbvzH87bftR3nKKapz5uTue/x4q0aIVK2BlQyGDLH3992XN52zz7aTx+rVNv3993nbYVRVV61S7dkzN70nnrD5M2bY9C23WPValy42nb+ed/Jky0O3bvZZHHKInQTnz7eT5eGH24/6lVfswwe78rzjjrwNx+vWWd7uuMOm9+wp/Ie+eLEFsMRE+y7uuivv8o8+sn21bm3BMuLuu23+yJF5T16nnKLao0feNP7+d1v3ppsKLjWuXGmBpWdPuzLOf1JWtVLD1VdbwDr4YLtSWb7cPp927fJe5d97r+1zwQI7iffpk/fEC7aOquW/sKq+Rx+1ZY8/bqXBZs3sO1izJvaxZGXlXgSdfrqVWEXs/7k4MjPtWCJ5L0pWll0YRKr1IsJha4SOXIwdcohq06Z7l1TLmAeIKujbb1V/nzJGN9FAbznhmzzLNm+236eq/VYOPlj1rWZ/sK+zaVPduXSdPvywar9+dh6pVSv3AvW112y1994LEtu6NecfcNeuvE0Ze9mwwX7woZCdEA47zBI75hjVYcPshCWievzxdlX26KN28uvbN+8V0IoVVg3QoYMdyK5dVu2QnGzF7V27Yu8/I0N12TIrdl97rUW+WI28e/bsfQUXuZOrZUu7UmvRwq4q333XTpK1alnJYOBAO/FFqlwyMgpuV5g40T6L2rXtuCN12598YtMNGtg+U1PtAy/o5NG/vwXfrl3tM+zUKTe4RaSl2Um+Rg3b3403xm7M3bHDTtoffZR3fjhsjeuR6pvI53PIIXayzf/5XX21rXvKKaoLF9qJduFC+6wnT1YdMMBKSgsW2DaRk/LVV6v+619WXdm8uX3/112X9/v//nsLrkcdZceQlWUBbcCA3HVWr7btnnjCqoEiVVmjR1v7Glj1WCzhsH2PKSn242jQwK7GC7NjhwWgyHfWrNneJc6K8vbb9v+YnGztaOXMA0QV9e23qif0yda5cwtfb/Ro1Ras0u1deunu9z/R44+3b7ZzZzsvRi7GVfdu14tYtcrWr1Ejtw2tQMuWWd3r0UfbrbORhFatspNYaqrmXPWdeGLeK9mIKVPsZBh9hdi5s0W/4srOLviqML89e6ze95JL7OrsmGNyo+avv1pRvn37wk88sbz3nh1H/obm+++3UsQrrxTdyP/mm3aCPPVUO/HXqWPBc80a+2zfeSe3fnDo0L2DR3GFw6q3354bKA8+2AJZQY3kzzyTW10X6xV9p1M4bDc6RC/v0WPvq+SI99+342zUSPVPf9KcO5EKkpmpeuaZuWlfc03hJa21a+0ipEYNq+Iqrm3b7C6lkmxTHubPt0BaAQoLEGLLq4cePXrotGnT4p2NCrdzp3Wh068f7Nhhz/S89po9NB3Lk0/C1VfDHXfA5Zfbg6innGL93DVpYml89509mFxqq1bB7NmWqZo1Y6/z7bfwww+54w8MHw7NmsVetwhPPw3/+pc9oJyUVIoEXnkFLr7YPshFiyAlpfjbbt0K9eqBSCl2HMNXX8GgQfZkdL169mDjEUfYAUZ1DlkqqvZh/e9/9l4Ebr0VPfwIfvgBevbMt/6cOfbPEApZ9zJ160L9+nDggXDooXunvWVL7nSDBoV/JgsX2giNs2dDixbWH34oVPD6e/bYd1SzJjz/vOWnMPPn24+jR4/C19vPich0VY39IRUUOariq7qVIEoiUuoWsba9wuzebTUHkfVr17YajqlTrRahUSO7ySnWnX+V1XHH2fF88UUpEwiHreRQ6gTK2Bdf2BfTsqW18+zrw3lFmDTJPr+vvirX3ext9267+aCw0oMrV3gVU/X3yy9Wo/H888XfZskS1b/+1e5ojK6i/eorq/6MtMFefLGdQMriebPysH597o07N90U79yUoXXrrO2hGI45pvSPU6jmthE/+mjp03BVU2EBwvtiqiYOOsi6ILr00uJv07493HknfPCBdeIZcfzxMHkyXHWVVTl99BGceioMGGD9wu0LVcjO3rc08ps40dJt2dKOpdpo2rRYVV2LF1st1L6MlxTpJXz27NKn4aofDxAupuOPh4cfhkmT4NdfYfRoq44+5hjo1g0eeMCmN26ErKzipbl5M5x4ovUz9ssvZZfXCROsCnvUKJg7N3do18ps61a4/XbrF29fTZxof+fMIWcckpKKBAYPEC5aXAKEiIwSkbkiMkdEXheRlHzL+4nINhGZGbzujEc+nalRA6691jr9fPRRu6i99Vbo0sVKGKEQtG0LI0bASy/ZiWrPnrxprFhhQeebb6yj0H79Sn8i373bSgxgA6lNmgSnn24DykHVGHn0xhtthNDXXtv3tD780NqCs7Ots8+SSkuz9uKkJPvuyrqE56quCg8QItISuA7ooaqdgURgaIxVv1LV1OD1twrNpIupbl24/nq7+WjZMrvxZ/RouOsuu1FkwgQLEl262LDdhx4KffrAmWfCscfCmjU2XMPkyXazywkn5B0OQdXSuPJK67n56aftBppIMAiH7Uaehg3hj3+0ef/7n12FDx4Mhx1mvVZX9mqmyZPhhRfs/X//u29p7d5t4wWdd55Nl6YK8Oef7bM97TS7oWzx4n3Lk6s+SnNDYFntt6aIZAK1gNVxyocrpbZt7RUtHLaTzezZMG+eXZWuX2/j0bRpY2O6dOpk606ebHdsdulit1aedhq8+65tW7eunfgiV7JHHgkjR8L48dYe0q4dPPKIzZ81y0o4AwbYVfTpp8Mzz9hV8apVcMkl0Lcv/N//lfL21yiTJ8Obb9qduC1awNlnl/yu3LQ0C4CHHGK3Fj/7rN1WXLdu6fI0ZYqV1i67zAJ3aQJEpP3hoovsM54924aFcC4udxsB1wM7gQ3AqzGW9wM2AbOAiUCnQtK6ApgGTGvdunWZt/C78rNmjfV/F3mu7vDDreuQjAy7q3PFCntWK9L1fUqKPVibkWHP39WoYT0RDByYm2bkds3bbrNOLiN9v/XtW/xn6hYssGfm/vWv3Hlff237q1Urt8ePrl1L3tFt5GHmzz6zO1kht6ft0vjDHyxPaWmq55xTvC6U8rv+ektj1668ncO6/QOV6TZXoAHwGdAECAHvARflW+cAoE7w/jRgUXHS3p9vc63q1q4t+GQbDqv+8IM9wB2xYUNuF0ePPZY7Py0tNyh06GC9ZIwZY71CNG9uvTZEW74875Pm4XBun2lgJ8tFi+w5kUMOsf1mZub2BP3KK8U/xqlT7QQc6RIqM9OeObnwwuKnES0cts9g8GCbjnShFOk/cO1a67y0qOdZ+ve322RVrU/CIUNKlx9XNVW2AHEu8FzU9CXAE0VssxxoXFTaHiD2L7Nn2zMc+TsR/eMfrVeG6F47Zs+2jlkbNMgNEpMmWZ+BKSm5PVC//LLm9PP2u9/Z+7p1rTSycGFuetnZVvJp1y63I9u1a63bk1i9au/ebSWkVq3y9qw9fLj18lzUmDGxzJunefoZ/Pxzm/7wQ5u+/HKbbtcudie8qhZkGjXK7V176FALOm7/UdkCxDHAXKztQYCXgGvzrdMccroB6Qn8Epku7OUBwhVm6VLr8qhBA6uCSky06qsuXSxIRAYlO+643PFhbrnFummK9YRxZBTI0aOtW6fWrW367LP37gQ18qR7/mEpIuP15B8LpjgiQzlESlbbt9sDg3ffbQ9OhkLWs3SLFtbtUayB5VauzFsKu+8+m45XP3Wu4lWqAGH54a/AfGAOMAaoAYwERgbLrwmCyCzgO6BXcdL1AOGKEgkSYKWP7dvtSezIOC+JiXsPzVBQj9fhsPWY27ChnYAPPNDGGAL7G/HZZ3bivuqqvdPYscPaNorbs3Rkv48/bk+79+yZd1mnTnZc115rfQguX25B4KijbD/5O6aNjBUV6ZtuwgSbLrLDxkqmsOESXOEqXYAor5cHCFccv/xibQjR3RutXWsDhEUG1Suu776zX1H37rldsF9/vc274AKbD9ZxakEDfp1+ul3lv/FG0e0F27ZZz+pgpYP86196qZV4UlL2Hv6iTh3bV7TIWDmRaq9fftE81VZVwbhxVlqKDCznSsYDhHPlaM6cvO0OWVl2R5GIBZ0HHyy8h+5PP80dPlrE7o66/HK7g2vOnNzqrtdft0CSkGBVQbEa9Z980tJJSMjbZqJqd4xB3qqmCy6wElVEOGxtIiNHluaTqHjbt1t/hmAN7PsyKu7+ygOEcxUsO7tk9fiZmTaA3d132zDV9etrzp1UDRvmDmx31FF2N1RBpk/PLb3kl5FhJ9G2bXMDWqdOuXdBRfTta+M2FSUyAFqLFjbMREEjjkbs3Gm3CxfWIL97d8k6hbz+eguqkTaeonoydnvzAOFcFRMO2/MYzz9vVUXHHGPVPkVdIWdnWzVZpLorv88+s1/9oYfmDhN+++1517nmGmuv+Oc/846Omj9/kUH6OnWyk3SNGqq//a2NnjpmjLX3REydavsEC1BPPLF3R7Wff263KLdvb73yfvONVY2lp8fOw/TpVlK66io77q5dbdtIAPrpJ7uB4KabbEC6hx5S3bTJlmVl2Y0Bo0db9WJRpkyxLvLLefTPUlm82EqXpeUBwjmX4/bbVU86yW6xve22vYPJ0qW5QzTXq2cN8R062PsuXezhvMsus+VXXGEn5/nzrVqsffu8Q4d37263CycmWkD6979tQD+wW34jt+R+840Nf9Ghgw2sl3+wwcMOsxEWI3791QJC8+a57Sfvv2/r/u1vdmyRLuCTk3OroVJSbOC7yDRYYBs50rq/j+Wnn3KrAHv3LvehOUpk3DjLW7NmBbdxFcUDhHOuxKZOtaqqXr1Uzz3XAsMpp9iJHOxOqVjVQXv2WNvJP/6RGwyGDs19LiUctpLMEUfYsvPOs5PcIYfkttVs2mS3AD/5pJ3w27a1IPPAAzavbl0rbbz/fu5+w+Hc/SUnq/75z1YCieRx9mwLBM2aWbXa2LE274orbP1QyKqqop9TWbnSAlmLFrkN+nfdFfvzWr/ejjEcts9gzBirEmzePG8+S2vxYqvS+/vf7QHNUaMsPz165H2ItKQ8QDjnykxGhl3BF7etIPIgYX7p6fakelKSNZSvWFFwGlu2WJCKXPUPGBD7in/2bDtxlvSEuWqVlXRE7FmY886zkka7dnb314wZtt7FF1sJ6dNPbZu5c+0Zkt69c/NWs2ZuiePww3Pbj+68c+8bCzIyrPqqoHaZ7dstuPzmN7kloujX1VcXXAVXXIUFCB+T2jkXV0uWWGeFTZsWvp4qvPqqvb/wwrIbAjzajBk2TsfSpZCcbL0S33uvjWMC1rFit26W52idOtkY8LVrWyeR27fDOedYh4zp6Tb41ksvWa/Hw4dbR4+TJlknkkuXwsCB8NZbUKeOdXr57LPW4+8PP1inlU2aWCePV11ln9Xq1bbe4Yfv+zEXNia1BwjnnCuB5cvhnXfsZH7AAdC5s/VKXBhVePFFG4Qruov7o46y0RofeMDejx4Nt9wCX3xhgWjQIOjf38ZSKcbggqXiAcI55yqJOXOsW/UuXWwcExGbPv98K23Uqwf//Kd14V4epaT8CgsQ8RoPwjnn9kudO9sr2pAhNvDT66/DTTfZ+OqVgQcI55yrBI491l6VSVzGpHbOOVf5eYBwzjkXkwcI55xzMXmAcM45F5MHCOecczF5gHDOOReTBwjnnHMxeYBwzjkXU7XqakNENgArSrBJY2BjOWUn3qrzsUH1Pj4/tqqrKh5fG1VtEmtBtQoQJSUi0wrqg6Sqq87HBtX7+PzYqq7qdnxexeSccy4mDxDOOedi2t8DxNPxzkA5qs7HBtX7+PzYqq5qdXz7dRuEc865gu3vJQjnnHMF8ADhnHMupv0yQIjIQBFZICKLReSWeOenOETkIBH5XETmichcEbk+mN9QRD4RkUXB3wZR29waHOMCETk1av5RIvJTsGy0SEUMbFg0EUkUkRkiMiGYrk7HVl9E3haR+cF3eFx1OT4RGRX8T84RkddFJKUqH5uIPC8i60VkTtS8MjseEakhIm8E878XkbYVeoAloar71QtIBJYA7YFkYBZwRLzzVYx8twC6B+/rAguBI4C/A7cE828BHgzeHxEcWw2gXXDMicGyqcBxgAATgUHxPr4gXzcCrwETgunqdGwvAb8P3icD9avD8QEtgWVAzWD6TWBEVT42oC/QHZgTNa/Mjge4GngqeD8UeCPe/58FfhbxzkAcvvzjgElR07cCt8Y7X6U4jv8CJwMLgBbBvBbAgljHBUwKjr0FMD9q/jDgP5XgeFoBk4ETyQ0Q1eXYDghOopJvfpU/viBA/Ao0xIYwngCcUtWPDWibL0CU2fFE1gneJ2FPXkt5Hcu+vPbHKqbIP3TEymBelREUSbsB3wPNVHUNQPC3abBaQcfZMniff368PQrcBISj5lWXY2sPbABeCKrQnhWR2lSD41PVVcBDwC/AGmCbqn5MNTi2fMryeHK2UdUsYBvQqNxyvg/2xwARq16zytzrKyJ1gHeAG1R1e2GrxpinhcyPGxEZDKxX1enF3STGvEp5bIEkrMriSVXtBuzCqikKUmWOL6iLPwOrXjkQqC0iFxW2SYx5lfLYiqk0x1NljnV/DBArgYOiplsBq+OUlxIRkRAWHF5V1XeD2etEpEWwvAWwPphf0HGuDN7nnx9PvYEhIrIcGAucKCKvUD2ODSxfK1X1+2D6bSxgVIfjOwlYpqobVDUTeBfoRfU4tmhleTw524hIElAP2FxuOd8H+2OA+AE4VETaiUgy1kg0Ps55KlJwB8RzwDxVfThq0XhgePB+ONY2EZk/NLhjoh1wKDA1KB7vEJFjgzQvidomLlT1VlVtpaptse/jM1W9iGpwbACquhb4VUQ6BLMGAD9TPY7vF+BYEakV5GkAMI/qcWzRyvJ4otM6B/t/r5QliLg3gsTjBZyG3QW0BLg93vkpZp6Px4qhs4GZwes0rO5yMrAo+Nswapvbg2NcQNQdIUAPYE6w7DEqUQMZ0I/cRupqc2xAKjAt+P7eAxpUl+MD/grMD/I1Brujp8oeG/A61p6SiV3t/64sjwdIAd4CFmN3OrWP93dY0Mu72nDOORfT/ljF5Jxzrhg8QDjnnIvJA4RzzrmYPEA455yLyQOEc865mDxAOFdCIpItIjNFZJaI/CgivYpYv76IXF2MdKeISLUZ8N5VfR4gnCu5NFVNVdWuWGdt9xexfn2sB0/nqhQPEM7tmwOALWD9ZInI5KBU8ZOInBGs8wBwcFDq+Eew7k3BOrNE5IGo9M4VkakislBE+lTsoTiXV1K8M+BcFVRTRGZiT8S2wLooB0gHzlLV7SLSGPhORMZjHfN1VtVUABEZBJwJHKOqu0WkYVTaSaraU0ROA+7C+jpyLi48QDhXcmlRJ/vjgJdFpDPWS+d9ItIX67a8JdAsxvYnAS+o6m4AVY3uqC3SCeN0bEwC5+LGA4Rz+0BVvw1KC02wvrGaAEepambQO21KjM2Egrt33hP8zcZ/ny7OvA3CuX0gIh2xYWw3Yd02rw+CQ3+gTbDaDmyY2IiPgctEpFaQRnQVk3OVhl+hOFdykTYIsNLAcFXNFpFXgfdFZBrW2+58AFXdJCJfi8gcYKKq/llEUoFpIpIBfAjcVtEH4VxRvDdX55xzMXkVk3POuZg8QDjnnIvJA4RzzrmYPEA455yLyQOEc865mDxAOOeci8kDhHPOuZj+HwAajd/h6BnqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot subtrain / validation RMSE\n",
    "plt.plot(all_step, all_train_rmse, c='b', label='Training RMSE')\n",
    "plt.plot(all_step, all_valid_rmse, c='r', label='Validation RMSE')\n",
    "plt.legend()\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('H=45, Training & Validation RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above, we can see that both training RMSE and validation RMSE reduce but fluctuate through the training process. While the two curves are quite close in the beginning, training RMSE turns out to be lower than training RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 8.841294436539107\n"
     ]
    }
   ],
   "source": [
    "# test RMSE\n",
    "saved_mlp = torch.load(weight_path)\n",
    "test_rmse = RMSE(saved_mlp, test_loader)\n",
    "print(f'Test RMSE = {test_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. H = 90, 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 34200.109375, best_valid_rmse = 8.901226091145327\n",
      "Epoch 2, Step 836: Loss = 33780.6875, best_valid_rmse = 8.674241658673425\n",
      "Epoch 3, Step 1254: Loss = 33153.56640625, best_valid_rmse = 8.638080569142724\n",
      "Epoch 4, Step 1672: Loss = 32139.08203125, best_valid_rmse = 8.562022940231628\n",
      "Epoch 5, Step 2090: Loss = 31482.390625, best_valid_rmse = 8.562022940231628\n",
      "Epoch 6, Step 2508: Loss = 30781.51171875, best_valid_rmse = 8.562022940231628\n",
      "Epoch 7, Step 2926: Loss = 30700.87109375, best_valid_rmse = 8.544342587750187\n",
      "Epoch 8, Step 3344: Loss = 29986.6953125, best_valid_rmse = 8.544342587750187\n",
      "Epoch 9, Step 3762: Loss = 29589.697265625, best_valid_rmse = 8.524440732241779\n",
      "Epoch 10, Step 4180: Loss = 29720.140625, best_valid_rmse = 8.524440732241779\n",
      "Epoch 11, Step 4598: Loss = 28300.787109375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 12, Step 5016: Loss = 27845.724609375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 13, Step 5434: Loss = 28651.138671875, best_valid_rmse = 8.524440732241779\n",
      "Epoch 14, Step 5852: Loss = 27804.57421875, best_valid_rmse = 8.524440732241779\n",
      "Epoch 15, Step 6270: Loss = 27145.83203125, best_valid_rmse = 8.524440732241779\n",
      "Epoch 16, Step 6688: Loss = 27365.318359375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 17, Step 7106: Loss = 25914.62109375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 18, Step 7524: Loss = 25514.162109375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 19, Step 7942: Loss = 26703.62109375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 20, Step 8360: Loss = 25954.685546875, best_valid_rmse = 8.524440732241779\n",
      "early stopping, step 8500, validation RMSE = 8.666315084529058\n",
      "H = 90, Test RMSE = 8.82367738460801\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34003.7265625, best_valid_rmse = 8.821839537051991\n",
      "Epoch 2, Step 836: Loss = 33052.890625, best_valid_rmse = 8.644408196072064\n",
      "Epoch 3, Step 1254: Loss = 31986.494140625, best_valid_rmse = 8.618648213139085\n",
      "Epoch 4, Step 1672: Loss = 31146.46484375, best_valid_rmse = 8.547684545341538\n",
      "Epoch 5, Step 2090: Loss = 30769.58203125, best_valid_rmse = 8.547684545341538\n",
      "Epoch 6, Step 2508: Loss = 30652.318359375, best_valid_rmse = 8.547684545341538\n",
      "Epoch 7, Step 2926: Loss = 29517.716796875, best_valid_rmse = 8.536382234634074\n",
      "Epoch 8, Step 3344: Loss = 28933.384765625, best_valid_rmse = 8.530303760692343\n",
      "Epoch 9, Step 3762: Loss = 28975.279296875, best_valid_rmse = 8.530303760692343\n",
      "Epoch 10, Step 4180: Loss = 28712.99609375, best_valid_rmse = 8.530303760692343\n",
      "Epoch 11, Step 4598: Loss = 27377.40234375, best_valid_rmse = 8.530303760692343\n",
      "Epoch 12, Step 5016: Loss = 27813.775390625, best_valid_rmse = 8.530303760692343\n",
      "Epoch 13, Step 5434: Loss = 25784.716796875, best_valid_rmse = 8.530303760692343\n",
      "Epoch 14, Step 5852: Loss = 23894.630859375, best_valid_rmse = 8.530303760692343\n",
      "Epoch 15, Step 6270: Loss = 24050.05078125, best_valid_rmse = 8.530303760692343\n",
      "Epoch 16, Step 6688: Loss = 24439.98828125, best_valid_rmse = 8.530303760692343\n",
      "Epoch 17, Step 7106: Loss = 24895.626953125, best_valid_rmse = 8.530303760692343\n",
      "Epoch 18, Step 7524: Loss = 21453.94140625, best_valid_rmse = 8.530303760692343\n",
      "Epoch 19, Step 7942: Loss = 21510.59765625, best_valid_rmse = 8.530303760692343\n",
      "early stopping, step 8000, validation RMSE = 8.731816505287078\n",
      "H = 180, Test RMSE = 8.900774927254158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d_hidden in [90, 180]:\n",
    "    \n",
    "    d_input = subtrain_set.Xnp.shape[1]\n",
    "    d_output = 1\n",
    "\n",
    "    mlp = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d_input, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(d_hidden, d_output)\n",
    "    )\n",
    "    \n",
    "    # optimizer\n",
    "    lr = 0.00001\n",
    "    momentum = 0\n",
    "    weight_decay = 0\n",
    "    sgd_optimizer = torch.optim.SGD(mlp.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # loss\n",
    "    l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    mlp = mlp.float()\n",
    "    mlp.to(device)\n",
    "    weight_path = f'./MLP{d_hidden}_weight'\n",
    "    all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=sgd_optimizer, loss_f=l2_loss, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=False)\n",
    "    saved_mlp = torch.load(weight_path)\n",
    "    test_rmse = RMSE(saved_mlp, test_loader)\n",
    "    print(f'H = {d_hidden}, Test RMSE = {test_rmse}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(hidden dimension = 45: test RMSE = 8.841)  \n",
    "**hidden dimension = 90: test RMSE = 8.824**  \n",
    "**hidden dimension = 180: test RMSE = 8.901**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With test RMSE shown above, we suggest that providing 90 hidden nodes in the linear hidden layers performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 MLP with Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 34912.4140625, best_valid_rmse = 8.939762386907391\n",
      "Epoch 2, Step 836: Loss = 34052.6328125, best_valid_rmse = 8.719163570821374\n",
      "Epoch 3, Step 1254: Loss = 33766.41796875, best_valid_rmse = 8.70004401025287\n",
      "Epoch 4, Step 1672: Loss = 33187.953125, best_valid_rmse = 8.622452527263516\n",
      "Epoch 5, Step 2090: Loss = 32928.0703125, best_valid_rmse = 8.612548300323168\n",
      "Epoch 6, Step 2508: Loss = 32510.919921875, best_valid_rmse = 8.612548300323168\n",
      "Epoch 7, Step 2926: Loss = 32025.77734375, best_valid_rmse = 8.609259075554823\n",
      "Epoch 8, Step 3344: Loss = 31523.86328125, best_valid_rmse = 8.579808557178483\n",
      "Epoch 9, Step 3762: Loss = 31441.578125, best_valid_rmse = 8.579808557178483\n",
      "Epoch 10, Step 4180: Loss = 31203.77734375, best_valid_rmse = 8.579808557178483\n",
      "Epoch 11, Step 4598: Loss = 31422.44921875, best_valid_rmse = 8.579808557178483\n",
      "Epoch 12, Step 5016: Loss = 31260.291015625, best_valid_rmse = 8.569624163502223\n",
      "Epoch 13, Step 5434: Loss = 32024.271484375, best_valid_rmse = 8.569624163502223\n",
      "Epoch 14, Step 5852: Loss = 31735.267578125, best_valid_rmse = 8.569624163502223\n",
      "Epoch 15, Step 6270: Loss = 31538.45703125, best_valid_rmse = 8.569624163502223\n",
      "Epoch 16, Step 6688: Loss = 31671.77734375, best_valid_rmse = 8.569624163502223\n",
      "Epoch 17, Step 7106: Loss = 31534.8046875, best_valid_rmse = 8.569624163502223\n",
      "Epoch 18, Step 7524: Loss = 31360.6875, best_valid_rmse = 8.569624163502223\n",
      "Epoch 19, Step 7942: Loss = 31117.5859375, best_valid_rmse = 8.569624163502223\n",
      "Epoch 20, Step 8360: Loss = 31096.337890625, best_valid_rmse = 8.569624163502223\n",
      "Epoch 21, Step 8778: Loss = 31077.04296875, best_valid_rmse = 8.569624163502223\n",
      "Epoch 22, Step 9196: Loss = 31229.375, best_valid_rmse = 8.569624163502223\n",
      "Epoch 23, Step 9614: Loss = 30921.08984375, best_valid_rmse = 8.569624163502223\n",
      "early stopping, step 9800, validation RMSE = 8.597169841237015\n",
      "H = 45 Weight decay = 0.1, Test RMSE = 8.851353864431783\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34462.296875, best_valid_rmse = 8.917254626555824\n",
      "Epoch 2, Step 836: Loss = 34030.7890625, best_valid_rmse = 8.698164607675224\n",
      "Epoch 3, Step 1254: Loss = 33437.859375, best_valid_rmse = 8.656580620790855\n",
      "Epoch 4, Step 1672: Loss = 33448.40625, best_valid_rmse = 8.624874768554143\n",
      "Epoch 5, Step 2090: Loss = 33207.1171875, best_valid_rmse = 8.617164932529311\n",
      "Epoch 6, Step 2508: Loss = 32972.078125, best_valid_rmse = 8.617164932529311\n",
      "Epoch 7, Step 2926: Loss = 32946.89453125, best_valid_rmse = 8.609978886739293\n",
      "Epoch 8, Step 3344: Loss = 32745.208984375, best_valid_rmse = 8.576257841995167\n",
      "Epoch 9, Step 3762: Loss = 32872.51171875, best_valid_rmse = 8.573661832149122\n",
      "Epoch 10, Step 4180: Loss = 33082.234375, best_valid_rmse = 8.573661832149122\n",
      "Epoch 11, Step 4598: Loss = 33200.15625, best_valid_rmse = 8.573661832149122\n",
      "Epoch 12, Step 5016: Loss = 33149.2265625, best_valid_rmse = 8.560034822188571\n",
      "Epoch 13, Step 5434: Loss = 33106.0, best_valid_rmse = 8.560034822188571\n",
      "Epoch 14, Step 5852: Loss = 33310.7890625, best_valid_rmse = 8.560034822188571\n",
      "Epoch 15, Step 6270: Loss = 33157.828125, best_valid_rmse = 8.560034822188571\n",
      "Epoch 16, Step 6688: Loss = 33379.140625, best_valid_rmse = 8.560034822188571\n",
      "Epoch 17, Step 7106: Loss = 32860.3515625, best_valid_rmse = 8.560034822188571\n",
      "Epoch 18, Step 7524: Loss = 33102.67578125, best_valid_rmse = 8.560034822188571\n",
      "Epoch 19, Step 7942: Loss = 32521.32421875, best_valid_rmse = 8.560034822188571\n",
      "Epoch 20, Step 8360: Loss = 32043.068359375, best_valid_rmse = 8.560034822188571\n",
      "Epoch 21, Step 8778: Loss = 32070.52734375, best_valid_rmse = 8.560034822188571\n",
      "Epoch 22, Step 9196: Loss = 31538.60546875, best_valid_rmse = 8.560034822188571\n",
      "Epoch 23, Step 9614: Loss = 31506.7265625, best_valid_rmse = 8.560034822188571\n",
      "early stopping, step 9800, validation RMSE = 8.627312889915519\n",
      "H = 45 Weight decay = 0.2, Test RMSE = 8.819504119562046\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34559.9609375, best_valid_rmse = 8.881725737713577\n",
      "Epoch 2, Step 836: Loss = 33271.82421875, best_valid_rmse = 8.691728489881214\n",
      "Epoch 3, Step 1254: Loss = 32780.53125, best_valid_rmse = 8.663582397593315\n",
      "Epoch 4, Step 1672: Loss = 32482.58984375, best_valid_rmse = 8.599778502458415\n",
      "Epoch 5, Step 2090: Loss = 32183.5234375, best_valid_rmse = 8.580497223864274\n",
      "Epoch 6, Step 2508: Loss = 32146.03515625, best_valid_rmse = 8.580497223864274\n",
      "Epoch 7, Step 2926: Loss = 32444.2890625, best_valid_rmse = 8.580497223864274\n",
      "Epoch 8, Step 3344: Loss = 32248.12890625, best_valid_rmse = 8.567530833881275\n",
      "Epoch 9, Step 3762: Loss = 32050.62890625, best_valid_rmse = 8.562462442819037\n",
      "Epoch 10, Step 4180: Loss = 32220.390625, best_valid_rmse = 8.562462442819037\n",
      "Epoch 11, Step 4598: Loss = 32469.0390625, best_valid_rmse = 8.562462442819037\n",
      "Epoch 12, Step 5016: Loss = 32691.1875, best_valid_rmse = 8.541651354044294\n",
      "Epoch 13, Step 5434: Loss = 32659.458984375, best_valid_rmse = 8.541651354044294\n",
      "Epoch 14, Step 5852: Loss = 32576.0234375, best_valid_rmse = 8.541651354044294\n",
      "Epoch 15, Step 6270: Loss = 32261.84765625, best_valid_rmse = 8.541651354044294\n",
      "Epoch 16, Step 6688: Loss = 32354.37890625, best_valid_rmse = 8.541651354044294\n",
      "Epoch 17, Step 7106: Loss = 32698.89453125, best_valid_rmse = 8.541651354044294\n",
      "Epoch 18, Step 7524: Loss = 32725.8203125, best_valid_rmse = 8.541651354044294\n",
      "Epoch 19, Step 7942: Loss = 32903.3515625, best_valid_rmse = 8.541651354044294\n",
      "Epoch 20, Step 8360: Loss = 32789.2421875, best_valid_rmse = 8.541651354044294\n",
      "Epoch 21, Step 8778: Loss = 32520.701171875, best_valid_rmse = 8.541651354044294\n",
      "Epoch 22, Step 9196: Loss = 32211.544921875, best_valid_rmse = 8.541651354044294\n",
      "Epoch 23, Step 9614: Loss = 32292.232421875, best_valid_rmse = 8.541651354044294\n",
      "early stopping, step 9800, validation RMSE = 8.59790884893433\n",
      "H = 45 Weight decay = 0.4, Test RMSE = 8.794360173977948\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34927.3046875, best_valid_rmse = 8.886551070767755\n",
      "Epoch 2, Step 836: Loss = 33805.625, best_valid_rmse = 8.668629603540655\n",
      "Epoch 3, Step 1254: Loss = 33181.87890625, best_valid_rmse = 8.648042521367552\n",
      "Epoch 4, Step 1672: Loss = 33110.4296875, best_valid_rmse = 8.582788402104507\n",
      "Epoch 5, Step 2090: Loss = 32138.080078125, best_valid_rmse = 8.569312115536963\n",
      "Epoch 6, Step 2508: Loss = 32159.541015625, best_valid_rmse = 8.569312115536963\n",
      "Epoch 7, Step 2926: Loss = 31553.390625, best_valid_rmse = 8.55445183144975\n",
      "Epoch 8, Step 3344: Loss = 31156.603515625, best_valid_rmse = 8.55445183144975\n",
      "Epoch 9, Step 3762: Loss = 31300.1875, best_valid_rmse = 8.5460807580512\n",
      "Epoch 10, Step 4180: Loss = 30399.794921875, best_valid_rmse = 8.541034986696875\n",
      "Epoch 11, Step 4598: Loss = 31513.263671875, best_valid_rmse = 8.541034986696875\n",
      "Epoch 12, Step 5016: Loss = 31618.107421875, best_valid_rmse = 8.541034986696875\n",
      "Epoch 13, Step 5434: Loss = 30669.69140625, best_valid_rmse = 8.541034986696875\n",
      "Epoch 14, Step 5852: Loss = 29964.396484375, best_valid_rmse = 8.541034986696875\n",
      "Epoch 15, Step 6270: Loss = 29810.40625, best_valid_rmse = 8.541034986696875\n",
      "Epoch 16, Step 6688: Loss = 30490.59375, best_valid_rmse = 8.541034986696875\n",
      "Epoch 17, Step 7106: Loss = 30579.041015625, best_valid_rmse = 8.541034986696875\n",
      "Epoch 18, Step 7524: Loss = 29256.966796875, best_valid_rmse = 8.541034986696875\n",
      "Epoch 19, Step 7942: Loss = 29902.59765625, best_valid_rmse = 8.541034986696875\n",
      "Epoch 20, Step 8360: Loss = 30182.98828125, best_valid_rmse = 8.541034986696875\n",
      "Epoch 21, Step 8778: Loss = 29295.8828125, best_valid_rmse = 8.541034986696875\n",
      "early stopping, step 8800, validation RMSE = 8.7525231499847\n",
      "H = 90 Weight decay = 0.1, Test RMSE = 8.885646642211364\n",
      "\n",
      "Epoch 1, Step 418: Loss = 33973.62109375, best_valid_rmse = 8.879677144594403\n",
      "Epoch 2, Step 836: Loss = 32851.8515625, best_valid_rmse = 8.677611250110472\n",
      "Epoch 3, Step 1254: Loss = 31940.30078125, best_valid_rmse = 8.649352835853822\n",
      "Epoch 4, Step 1672: Loss = 31619.556640625, best_valid_rmse = 8.57152766167714\n",
      "Epoch 5, Step 2090: Loss = 30733.6640625, best_valid_rmse = 8.557955841311509\n",
      "Epoch 6, Step 2508: Loss = 30214.091796875, best_valid_rmse = 8.557955841311509\n",
      "Epoch 7, Step 2926: Loss = 30506.55859375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 8, Step 3344: Loss = 29877.43359375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 9, Step 3762: Loss = 29759.44921875, best_valid_rmse = 8.549762870916814\n",
      "Epoch 10, Step 4180: Loss = 28960.17578125, best_valid_rmse = 8.549762870916814\n",
      "Epoch 11, Step 4598: Loss = 29228.017578125, best_valid_rmse = 8.549762870916814\n",
      "Epoch 12, Step 5016: Loss = 28993.662109375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 13, Step 5434: Loss = 28092.568359375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 14, Step 5852: Loss = 27083.5859375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 15, Step 6270: Loss = 26789.20703125, best_valid_rmse = 8.549762870916814\n",
      "Epoch 16, Step 6688: Loss = 27314.515625, best_valid_rmse = 8.549762870916814\n",
      "Epoch 17, Step 7106: Loss = 28337.52734375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 18, Step 7524: Loss = 29396.578125, best_valid_rmse = 8.549762870916814\n",
      "early stopping, step 7900, validation RMSE = 8.853353061411825\n",
      "H = 90 Weight decay = 0.2, Test RMSE = 8.806475693435283\n",
      "\n",
      "Epoch 1, Step 418: Loss = 33937.81640625, best_valid_rmse = 8.87782453509795\n",
      "Epoch 2, Step 836: Loss = 33344.8046875, best_valid_rmse = 8.672950334175994\n",
      "Epoch 3, Step 1254: Loss = 31762.38671875, best_valid_rmse = 8.658701922067856\n",
      "Epoch 4, Step 1672: Loss = 31498.1171875, best_valid_rmse = 8.574921268574053\n",
      "Epoch 5, Step 2090: Loss = 30466.859375, best_valid_rmse = 8.566666800924734\n",
      "Epoch 6, Step 2508: Loss = 30171.93359375, best_valid_rmse = 8.566666800924734\n",
      "Epoch 7, Step 2926: Loss = 29960.490234375, best_valid_rmse = 8.530335361504942\n",
      "Epoch 8, Step 3344: Loss = 29694.052734375, best_valid_rmse = 8.530335361504942\n",
      "Epoch 9, Step 3762: Loss = 30834.654296875, best_valid_rmse = 8.530335361504942\n",
      "Epoch 10, Step 4180: Loss = 30730.49609375, best_valid_rmse = 8.530335361504942\n",
      "Epoch 11, Step 4598: Loss = 31328.263671875, best_valid_rmse = 8.530335361504942\n",
      "Epoch 12, Step 5016: Loss = 31197.82421875, best_valid_rmse = 8.530335361504942\n",
      "Epoch 13, Step 5434: Loss = 31094.322265625, best_valid_rmse = 8.530335361504942\n",
      "Epoch 14, Step 5852: Loss = 32556.33203125, best_valid_rmse = 8.530335361504942\n",
      "Epoch 15, Step 6270: Loss = 32216.21875, best_valid_rmse = 8.530335361504942\n",
      "Epoch 16, Step 6688: Loss = 32157.19140625, best_valid_rmse = 8.530335361504942\n",
      "Epoch 17, Step 7106: Loss = 31649.466796875, best_valid_rmse = 8.530335361504942\n",
      "Epoch 18, Step 7524: Loss = 30651.400390625, best_valid_rmse = 8.530335361504942\n",
      "early stopping, step 7900, validation RMSE = 8.619891382246069\n",
      "H = 90 Weight decay = 0.4, Test RMSE = 8.800657846979894\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34387.76171875, best_valid_rmse = 8.863910322929424\n",
      "Epoch 2, Step 836: Loss = 33512.0234375, best_valid_rmse = 8.64502841853085\n",
      "Epoch 3, Step 1254: Loss = 32934.6640625, best_valid_rmse = 8.626262350977633\n",
      "Epoch 4, Step 1672: Loss = 31933.296875, best_valid_rmse = 8.553214909129162\n",
      "Epoch 5, Step 2090: Loss = 31030.109375, best_valid_rmse = 8.532850410765244\n",
      "Epoch 6, Step 2508: Loss = 29463.5703125, best_valid_rmse = 8.532850410765244\n",
      "Epoch 7, Step 2926: Loss = 28776.3671875, best_valid_rmse = 8.532850410765244\n",
      "Epoch 8, Step 3344: Loss = 27535.70703125, best_valid_rmse = 8.527279653298958\n",
      "Epoch 9, Step 3762: Loss = 27045.900390625, best_valid_rmse = 8.522156009014823\n",
      "Epoch 10, Step 4180: Loss = 26498.484375, best_valid_rmse = 8.522156009014823\n",
      "Epoch 11, Step 4598: Loss = 26385.076171875, best_valid_rmse = 8.522156009014823\n",
      "Epoch 12, Step 5016: Loss = 24847.142578125, best_valid_rmse = 8.522156009014823\n",
      "Epoch 13, Step 5434: Loss = 25109.69140625, best_valid_rmse = 8.522156009014823\n",
      "Epoch 14, Step 5852: Loss = 25372.22265625, best_valid_rmse = 8.522156009014823\n",
      "Epoch 15, Step 6270: Loss = 26994.755859375, best_valid_rmse = 8.522156009014823\n",
      "Epoch 16, Step 6688: Loss = 24910.41015625, best_valid_rmse = 8.522156009014823\n",
      "Epoch 17, Step 7106: Loss = 24449.90234375, best_valid_rmse = 8.522156009014823\n",
      "Epoch 18, Step 7524: Loss = 22592.55859375, best_valid_rmse = 8.522156009014823\n",
      "Epoch 19, Step 7942: Loss = 23169.455078125, best_valid_rmse = 8.522156009014823\n",
      "Epoch 20, Step 8360: Loss = 24028.515625, best_valid_rmse = 8.522156009014823\n",
      "early stopping, step 8600, validation RMSE = 8.99097882807079\n",
      "H = 180 Weight decay = 0.1, Test RMSE = 8.875524879929499\n",
      "\n",
      "Epoch 1, Step 418: Loss = 33751.3125, best_valid_rmse = 8.846688271982877\n",
      "Epoch 2, Step 836: Loss = 32691.751953125, best_valid_rmse = 8.662645482402596\n",
      "Epoch 3, Step 1254: Loss = 32101.31640625, best_valid_rmse = 8.629473873112682\n",
      "Epoch 4, Step 1672: Loss = 31696.322265625, best_valid_rmse = 8.5631084328947\n",
      "Epoch 5, Step 2090: Loss = 31237.1875, best_valid_rmse = 8.536620647992228\n",
      "Epoch 6, Step 2508: Loss = 30627.244140625, best_valid_rmse = 8.536620647992228\n",
      "Epoch 7, Step 2926: Loss = 30032.306640625, best_valid_rmse = 8.536620647992228\n",
      "Epoch 8, Step 3344: Loss = 29818.357421875, best_valid_rmse = 8.530688966614015\n",
      "Epoch 9, Step 3762: Loss = 29551.7734375, best_valid_rmse = 8.530688966614015\n",
      "Epoch 10, Step 4180: Loss = 28327.056640625, best_valid_rmse = 8.530688966614015\n",
      "Epoch 11, Step 4598: Loss = 28074.1640625, best_valid_rmse = 8.528235548961627\n",
      "Epoch 12, Step 5016: Loss = 27113.3203125, best_valid_rmse = 8.528235548961627\n",
      "Epoch 13, Step 5434: Loss = 26846.26953125, best_valid_rmse = 8.528235548961627\n",
      "Epoch 14, Step 5852: Loss = 26805.71484375, best_valid_rmse = 8.528235548961627\n",
      "Epoch 15, Step 6270: Loss = 24925.0625, best_valid_rmse = 8.528235548961627\n",
      "Epoch 16, Step 6688: Loss = 25059.82421875, best_valid_rmse = 8.528235548961627\n",
      "Epoch 17, Step 7106: Loss = 23658.76953125, best_valid_rmse = 8.528235548961627\n",
      "Epoch 18, Step 7524: Loss = 22327.98828125, best_valid_rmse = 8.528235548961627\n",
      "Epoch 19, Step 7942: Loss = 21432.818359375, best_valid_rmse = 8.528235548961627\n",
      "Epoch 20, Step 8360: Loss = 21441.65234375, best_valid_rmse = 8.528235548961627\n",
      "Epoch 21, Step 8778: Loss = 22197.560546875, best_valid_rmse = 8.528235548961627\n",
      "Epoch 22, Step 9196: Loss = 18604.216796875, best_valid_rmse = 8.528235548961627\n",
      "early stopping, step 9300, validation RMSE = 8.839641547355317\n",
      "H = 180 Weight decay = 0.2, Test RMSE = 8.883971610633367\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34862.578125, best_valid_rmse = 8.858036215194678\n",
      "Epoch 2, Step 836: Loss = 33618.02734375, best_valid_rmse = 8.645813221485273\n",
      "Epoch 3, Step 1254: Loss = 32709.123046875, best_valid_rmse = 8.630792316268295\n",
      "Epoch 4, Step 1672: Loss = 31735.15234375, best_valid_rmse = 8.561291540285554\n",
      "Epoch 5, Step 2090: Loss = 30962.7421875, best_valid_rmse = 8.544818963602657\n",
      "Epoch 6, Step 2508: Loss = 30336.07421875, best_valid_rmse = 8.544818963602657\n",
      "Epoch 7, Step 2926: Loss = 29980.84765625, best_valid_rmse = 8.544818963602657\n",
      "Epoch 8, Step 3344: Loss = 29229.0, best_valid_rmse = 8.544818963602657\n",
      "Epoch 9, Step 3762: Loss = 28731.767578125, best_valid_rmse = 8.530658315098089\n",
      "Epoch 10, Step 4180: Loss = 26437.75, best_valid_rmse = 8.530658315098089\n",
      "Epoch 11, Step 4598: Loss = 27507.423828125, best_valid_rmse = 8.530658315098089\n",
      "Epoch 12, Step 5016: Loss = 24949.95703125, best_valid_rmse = 8.530658315098089\n",
      "Epoch 13, Step 5434: Loss = 25729.892578125, best_valid_rmse = 8.530658315098089\n",
      "Epoch 14, Step 5852: Loss = 24489.5859375, best_valid_rmse = 8.530658315098089\n",
      "Epoch 15, Step 6270: Loss = 24429.373046875, best_valid_rmse = 8.530658315098089\n",
      "Epoch 16, Step 6688: Loss = 24007.67578125, best_valid_rmse = 8.530658315098089\n",
      "Epoch 17, Step 7106: Loss = 24550.10546875, best_valid_rmse = 8.530658315098089\n",
      "Epoch 18, Step 7524: Loss = 22887.37890625, best_valid_rmse = 8.530658315098089\n",
      "Epoch 19, Step 7942: Loss = 23665.78515625, best_valid_rmse = 8.530658315098089\n",
      "Epoch 20, Step 8360: Loss = 24208.125, best_valid_rmse = 8.530658315098089\n",
      "early stopping, step 8600, validation RMSE = 8.728568210177409\n",
      "H = 180 Weight decay = 0.4, Test RMSE = 8.868065281236266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d_hidden in [45, 90, 180]:\n",
    "    for weight_decay in [0.1, 0.2, 0.4]:\n",
    "    \n",
    "        d_input = subtrain_set.Xnp.shape[1]\n",
    "        d_output = 1\n",
    "\n",
    "        mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_input, d_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_hidden, d_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_hidden, d_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_hidden, d_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_hidden, d_output)\n",
    "        )\n",
    "\n",
    "        mlp = mlp.float()\n",
    "        mlp.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        lr = 0.00001\n",
    "        momentum = 0\n",
    "        sgd_optimizer_wd = torch.optim.SGD(mlp.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "        weight_path = f'./MLP{d_hidden}_{weight_decay*10}_weight'\n",
    "        all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=sgd_optimizer_wd, loss_f=l2_loss, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=False)\n",
    "        saved_mlp = torch.load(weight_path)\n",
    "        test_rmse = RMSE(saved_mlp, test_loader)\n",
    "        print(f'H = {d_hidden} Weight decay = {weight_decay}, Test RMSE = {test_rmse}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    Test RMSE     | H=45  | H=90  | H=180 |\n",
    "| ---------------- | ----- | ----- | ----- |\n",
    "| weight_decay=0.1 | 8.851 | 8.886 | 8.876 |\n",
    "| weight_decay=0.2 | 8.820 | 8.806 | 8.884 |\n",
    "| weight_decay=0.4 | 8.794 | 8.801 | 8.868 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we can see that **H=45** with **weight_decay=0.4** leads to the lowest testing RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. MLP with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 37498.9921875, best_valid_rmse = 9.186258692961909\n",
      "Epoch 2, Step 836: Loss = 36125.3359375, best_valid_rmse = 9.071526105535854\n",
      "Epoch 3, Step 1254: Loss = 35325.8828125, best_valid_rmse = 9.000254585594107\n",
      "Epoch 4, Step 1672: Loss = 34050.859375, best_valid_rmse = 8.95554586681681\n",
      "Epoch 5, Step 2090: Loss = 34787.98828125, best_valid_rmse = 8.931380310302917\n",
      "Epoch 6, Step 2508: Loss = 35317.65625, best_valid_rmse = 8.920974207522569\n",
      "Epoch 7, Step 2926: Loss = 32773.45703125, best_valid_rmse = 8.90552114791495\n",
      "Epoch 8, Step 3344: Loss = 34434.96875, best_valid_rmse = 8.904098673894705\n",
      "Epoch 9, Step 3762: Loss = 34547.0703125, best_valid_rmse = 8.867553483159396\n",
      "Epoch 10, Step 4180: Loss = 33072.8984375, best_valid_rmse = 8.865956174229254\n",
      "Epoch 11, Step 4598: Loss = 34885.29296875, best_valid_rmse = 8.854683529840045\n",
      "Epoch 12, Step 5016: Loss = 34875.23828125, best_valid_rmse = 8.854683529840045\n",
      "Epoch 13, Step 5434: Loss = 31445.935546875, best_valid_rmse = 8.854683529840045\n",
      "Epoch 14, Step 5852: Loss = 34940.26171875, best_valid_rmse = 8.843549230116075\n",
      "Epoch 15, Step 6270: Loss = 34832.55859375, best_valid_rmse = 8.843549230116075\n",
      "Epoch 16, Step 6688: Loss = 36101.1171875, best_valid_rmse = 8.843549230116075\n",
      "Epoch 17, Step 7106: Loss = 33880.984375, best_valid_rmse = 8.843549230116075\n",
      "Epoch 18, Step 7524: Loss = 33412.26171875, best_valid_rmse = 8.843549230116075\n",
      "Epoch 19, Step 7942: Loss = 33485.734375, best_valid_rmse = 8.843549230116075\n",
      "Epoch 20, Step 8360: Loss = 33591.3515625, best_valid_rmse = 8.838697066184602\n",
      "Epoch 21, Step 8778: Loss = 33695.5859375, best_valid_rmse = 8.838697066184602\n",
      "Epoch 22, Step 9196: Loss = 33030.671875, best_valid_rmse = 8.838697066184602\n",
      "Epoch 23, Step 9614: Loss = 35249.03125, best_valid_rmse = 8.823254192148125\n",
      "Epoch 24, Step 10032: Loss = 34508.0078125, best_valid_rmse = 8.823254192148125\n",
      "Epoch 25, Step 10450: Loss = 33798.85546875, best_valid_rmse = 8.821462461097738\n",
      "Epoch 26, Step 10868: Loss = 33107.2265625, best_valid_rmse = 8.814298928766751\n",
      "Epoch 27, Step 11286: Loss = 32872.40625, best_valid_rmse = 8.814298928766751\n",
      "Epoch 28, Step 11704: Loss = 33921.5625, best_valid_rmse = 8.814298928766751\n",
      "Epoch 29, Step 12122: Loss = 32726.55078125, best_valid_rmse = 8.814298928766751\n",
      "Epoch 30, Step 12540: Loss = 34868.4921875, best_valid_rmse = 8.814298928766751\n",
      "Epoch 31, Step 12958: Loss = 33140.140625, best_valid_rmse = 8.814298928766751\n",
      "Epoch 32, Step 13376: Loss = 34056.1875, best_valid_rmse = 8.814298928766751\n",
      "Epoch 33, Step 13794: Loss = 33043.1953125, best_valid_rmse = 8.814298928766751\n",
      "Epoch 34, Step 14212: Loss = 31808.677734375, best_valid_rmse = 8.814298928766751\n",
      "Epoch 35, Step 14630: Loss = 32798.51171875, best_valid_rmse = 8.814298928766751\n",
      "Epoch 36, Step 15048: Loss = 35199.6484375, best_valid_rmse = 8.814298928766751\n",
      "Epoch 37, Step 15466: Loss = 36264.6171875, best_valid_rmse = 8.814298928766751\n",
      "early stopping, step 15500, validation RMSE = 8.86281604871643\n"
     ]
    }
   ],
   "source": [
    "d_hidden = 90\n",
    "d_input = subtrain_set.Xnp.shape[1]\n",
    "d_output = 1\n",
    "\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d_input, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_output)\n",
    ")\n",
    "mlp = mlp.float()\n",
    "mlp.to(device)\n",
    "weight_path = './MLP90_drop_weight'\n",
    "\n",
    "# optimizer\n",
    "lr = 0.001\n",
    "momentum = 0\n",
    "weight_decay = 0\n",
    "adam_optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# loss\n",
    "l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# train\n",
    "all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=adam_optimizer, loss_f=l2_loss, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=True)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'H=90 with Drop Out, Training & Validation RMSE')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot subtrain / validation RMSE\n",
    "plt.plot(all_step, all_train_rmse, c='b', label='Training RMSE')\n",
    "plt.plot(all_step, all_valid_rmse, c='r', label='Validation RMSE')\n",
    "plt.legend()\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('H=90 with Drop Out, Training & Validation RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above, we can see that both training RMSE and validation RMSE drop dramastically in the beginning of the training process and converges afterwards. In the first half of the training process, training RMSE is larger than validation RMSE, while in the second half, training RMSE turns out to be smaller than validation RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 9.029719368877945\n"
     ]
    }
   ],
   "source": [
    "# test RMSE\n",
    "saved_mlp = torch.load(weight_path)\n",
    "test_rmse = RMSE(saved_mlp, test_loader)\n",
    "print(f'Test RMSE = {test_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Explore Number of Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 43384.8203125, best_valid_rmse = 9.763224669693798\n",
      "Epoch 2, Step 836: Loss = 38465.2734375, best_valid_rmse = 9.572969094941469\n",
      "Epoch 3, Step 1254: Loss = 39023.46484375, best_valid_rmse = 9.482433656928665\n",
      "Epoch 4, Step 1672: Loss = 38567.38671875, best_valid_rmse = 9.417149557373707\n",
      "Epoch 5, Step 2090: Loss = 38634.7265625, best_valid_rmse = 9.413087093072491\n",
      "Epoch 6, Step 2508: Loss = 36937.37890625, best_valid_rmse = 9.381767395648092\n",
      "Epoch 7, Step 2926: Loss = 39024.5078125, best_valid_rmse = 9.352594674772869\n",
      "Epoch 8, Step 3344: Loss = 39017.86328125, best_valid_rmse = 9.35191789831301\n",
      "Epoch 9, Step 3762: Loss = 38357.61328125, best_valid_rmse = 9.35191789831301\n",
      "Epoch 10, Step 4180: Loss = 35153.7421875, best_valid_rmse = 9.341722855055735\n",
      "Epoch 11, Step 4598: Loss = 38768.7265625, best_valid_rmse = 9.341722855055735\n",
      "Epoch 12, Step 5016: Loss = 39046.4375, best_valid_rmse = 9.331254659203784\n",
      "Epoch 13, Step 5434: Loss = 37445.828125, best_valid_rmse = 9.312020293055658\n",
      "Epoch 14, Step 5852: Loss = 41450.328125, best_valid_rmse = 9.312020293055658\n",
      "Epoch 15, Step 6270: Loss = 38891.13671875, best_valid_rmse = 9.306672338035106\n",
      "Epoch 16, Step 6688: Loss = 38966.9765625, best_valid_rmse = 9.306672338035106\n",
      "Epoch 17, Step 7106: Loss = 40096.328125, best_valid_rmse = 9.306672338035106\n",
      "Epoch 18, Step 7524: Loss = 39424.74609375, best_valid_rmse = 9.306672338035106\n",
      "Epoch 19, Step 7942: Loss = 39548.265625, best_valid_rmse = 9.306672338035106\n",
      "Epoch 20, Step 8360: Loss = 36777.41015625, best_valid_rmse = 9.306672338035106\n",
      "Epoch 21, Step 8778: Loss = 35394.83984375, best_valid_rmse = 9.306672338035106\n",
      "Epoch 22, Step 9196: Loss = 35826.54296875, best_valid_rmse = 9.306672338035106\n",
      "Epoch 23, Step 9614: Loss = 39954.1953125, best_valid_rmse = 9.295074014780239\n",
      "Epoch 24, Step 10032: Loss = 36614.515625, best_valid_rmse = 9.295074014780239\n",
      "Epoch 25, Step 10450: Loss = 40547.4453125, best_valid_rmse = 9.295074014780239\n",
      "Epoch 26, Step 10868: Loss = 37179.30859375, best_valid_rmse = 9.295074014780239\n",
      "Epoch 27, Step 11286: Loss = 39730.640625, best_valid_rmse = 9.295074014780239\n",
      "Epoch 28, Step 11704: Loss = 39129.390625, best_valid_rmse = 9.295074014780239\n",
      "Epoch 29, Step 12122: Loss = 37045.859375, best_valid_rmse = 9.295074014780239\n",
      "Epoch 30, Step 12540: Loss = 41133.8984375, best_valid_rmse = 9.292288936096801\n",
      "Epoch 31, Step 12958: Loss = 37965.98046875, best_valid_rmse = 9.292288936096801\n",
      "Epoch 32, Step 13376: Loss = 37179.859375, best_valid_rmse = 9.292288936096801\n",
      "Epoch 33, Step 13794: Loss = 36986.71875, best_valid_rmse = 9.292288936096801\n",
      "Epoch 34, Step 14212: Loss = 36490.25390625, best_valid_rmse = 9.292288936096801\n",
      "Epoch 35, Step 14630: Loss = 35744.08984375, best_valid_rmse = 9.292288936096801\n",
      "Epoch 36, Step 15048: Loss = 36182.171875, best_valid_rmse = 9.292288936096801\n",
      "Epoch 37, Step 15466: Loss = 36457.6171875, best_valid_rmse = 9.292288936096801\n",
      "Epoch 38, Step 15884: Loss = 38000.11328125, best_valid_rmse = 9.292288936096801\n",
      "Epoch 39, Step 16302: Loss = 38757.0625, best_valid_rmse = 9.292288936096801\n",
      "Epoch 40, Step 16720: Loss = 38789.7734375, best_valid_rmse = 9.292288936096801\n",
      "Epoch 41, Step 17138: Loss = 35469.46484375, best_valid_rmse = 9.292288936096801\n",
      "early stopping, step 17200, validation RMSE = 9.301678931340964\n",
      "H = 20: Test RMSE = 9.384141162892735\n",
      "\n",
      "Epoch 1, Step 418: Loss = 37766.75390625, best_valid_rmse = 9.389627138400803\n",
      "Epoch 2, Step 836: Loss = 38355.73828125, best_valid_rmse = 9.245848714137475\n",
      "Epoch 3, Step 1254: Loss = 37743.75, best_valid_rmse = 9.19694263626668\n",
      "Epoch 4, Step 1672: Loss = 37656.03125, best_valid_rmse = 9.13900539650094\n",
      "Epoch 5, Step 2090: Loss = 35381.45703125, best_valid_rmse = 9.102826427321821\n",
      "Epoch 6, Step 2508: Loss = 38385.203125, best_valid_rmse = 9.095179718193048\n",
      "Epoch 7, Step 2926: Loss = 36625.3828125, best_valid_rmse = 9.073470480170618\n",
      "Epoch 8, Step 3344: Loss = 37271.01171875, best_valid_rmse = 9.073470480170618\n",
      "Epoch 9, Step 3762: Loss = 35580.6875, best_valid_rmse = 9.05076091285827\n",
      "Epoch 10, Step 4180: Loss = 34036.875, best_valid_rmse = 9.043666040489267\n",
      "Epoch 11, Step 4598: Loss = 38042.27734375, best_valid_rmse = 9.043666040489267\n",
      "Epoch 12, Step 5016: Loss = 37104.515625, best_valid_rmse = 9.022634468917543\n",
      "Epoch 13, Step 5434: Loss = 35556.03125, best_valid_rmse = 9.021431559541645\n",
      "Epoch 14, Step 5852: Loss = 34710.640625, best_valid_rmse = 9.021431559541645\n",
      "Epoch 15, Step 6270: Loss = 35942.890625, best_valid_rmse = 9.014329751229539\n",
      "Epoch 16, Step 6688: Loss = 34809.296875, best_valid_rmse = 9.010344154550493\n",
      "Epoch 17, Step 7106: Loss = 35887.4609375, best_valid_rmse = 9.010344154550493\n",
      "Epoch 18, Step 7524: Loss = 34336.109375, best_valid_rmse = 9.010344154550493\n",
      "Epoch 19, Step 7942: Loss = 33998.4140625, best_valid_rmse = 9.005431593923527\n",
      "Epoch 20, Step 8360: Loss = 35967.84765625, best_valid_rmse = 9.005431593923527\n",
      "Epoch 21, Step 8778: Loss = 35525.5546875, best_valid_rmse = 8.998182349117807\n",
      "Epoch 22, Step 9196: Loss = 36512.49609375, best_valid_rmse = 8.998182349117807\n",
      "Epoch 23, Step 9614: Loss = 39137.11328125, best_valid_rmse = 8.998182349117807\n",
      "Epoch 24, Step 10032: Loss = 35922.65625, best_valid_rmse = 8.989295196730556\n",
      "Epoch 25, Step 10450: Loss = 37552.203125, best_valid_rmse = 8.989295196730556\n",
      "Epoch 26, Step 10868: Loss = 36805.6328125, best_valid_rmse = 8.987893779204981\n",
      "Epoch 27, Step 11286: Loss = 35973.4765625, best_valid_rmse = 8.987893779204981\n",
      "Epoch 28, Step 11704: Loss = 36582.64453125, best_valid_rmse = 8.987893779204981\n",
      "Epoch 29, Step 12122: Loss = 35513.48828125, best_valid_rmse = 8.987893779204981\n",
      "Epoch 30, Step 12540: Loss = 36949.0234375, best_valid_rmse = 8.976019895779967\n",
      "Epoch 31, Step 12958: Loss = 34467.6953125, best_valid_rmse = 8.976019895779967\n",
      "Epoch 32, Step 13376: Loss = 33859.26171875, best_valid_rmse = 8.976019895779967\n",
      "Epoch 33, Step 13794: Loss = 36745.328125, best_valid_rmse = 8.976019895779967\n",
      "Epoch 34, Step 14212: Loss = 35183.1640625, best_valid_rmse = 8.976019895779967\n",
      "Epoch 35, Step 14630: Loss = 36752.73828125, best_valid_rmse = 8.976019895779967\n",
      "Epoch 36, Step 15048: Loss = 36903.7265625, best_valid_rmse = 8.976019895779967\n",
      "Epoch 37, Step 15466: Loss = 36479.15234375, best_valid_rmse = 8.976019895779967\n",
      "Epoch 38, Step 15884: Loss = 36431.25, best_valid_rmse = 8.976019895779967\n",
      "Epoch 39, Step 16302: Loss = 33615.421875, best_valid_rmse = 8.976019895779967\n",
      "Epoch 40, Step 16720: Loss = 35799.4140625, best_valid_rmse = 8.976019895779967\n",
      "Epoch 41, Step 17138: Loss = 37229.68359375, best_valid_rmse = 8.976019895779967\n",
      "early stopping, step 17200, validation RMSE = 9.018814842065643\n",
      "H = 45: Test RMSE = 9.123405036795587\n",
      "\n",
      "Epoch 1, Step 418: Loss = 36516.75, best_valid_rmse = 9.061649533549339\n",
      "Epoch 2, Step 836: Loss = 36989.2734375, best_valid_rmse = 8.94613908334211\n",
      "Epoch 3, Step 1254: Loss = 34375.98828125, best_valid_rmse = 8.886116676642734\n",
      "Epoch 4, Step 1672: Loss = 34164.84765625, best_valid_rmse = 8.860590891668336\n",
      "Epoch 5, Step 2090: Loss = 36468.98828125, best_valid_rmse = 8.821709365164963\n",
      "Epoch 6, Step 2508: Loss = 34918.10546875, best_valid_rmse = 8.807156504412388\n",
      "Epoch 7, Step 2926: Loss = 36318.7578125, best_valid_rmse = 8.803870482498105\n",
      "Epoch 8, Step 3344: Loss = 33731.1953125, best_valid_rmse = 8.787329911831131\n",
      "Epoch 9, Step 3762: Loss = 34989.50390625, best_valid_rmse = 8.774025847732332\n",
      "Epoch 10, Step 4180: Loss = 35390.015625, best_valid_rmse = 8.773700177122642\n",
      "Epoch 11, Step 4598: Loss = 33561.50390625, best_valid_rmse = 8.773700177122642\n",
      "Epoch 12, Step 5016: Loss = 34468.4921875, best_valid_rmse = 8.766428429440898\n",
      "Epoch 13, Step 5434: Loss = 35798.4609375, best_valid_rmse = 8.766428429440898\n",
      "Epoch 14, Step 5852: Loss = 32876.03515625, best_valid_rmse = 8.755958670059082\n",
      "Epoch 15, Step 6270: Loss = 31079.58984375, best_valid_rmse = 8.755958670059082\n",
      "Epoch 16, Step 6688: Loss = 31750.421875, best_valid_rmse = 8.755958670059082\n",
      "Epoch 17, Step 7106: Loss = 31217.921875, best_valid_rmse = 8.745686281291066\n",
      "Epoch 18, Step 7524: Loss = 34395.65625, best_valid_rmse = 8.745686281291066\n",
      "Epoch 19, Step 7942: Loss = 32321.634765625, best_valid_rmse = 8.745686281291066\n",
      "Epoch 20, Step 8360: Loss = 30931.74609375, best_valid_rmse = 8.745686281291066\n",
      "Epoch 21, Step 8778: Loss = 32426.37109375, best_valid_rmse = 8.745686281291066\n",
      "Epoch 22, Step 9196: Loss = 32358.263671875, best_valid_rmse = 8.733287412501104\n",
      "Epoch 23, Step 9614: Loss = 31151.6875, best_valid_rmse = 8.733287412501104\n",
      "Epoch 24, Step 10032: Loss = 33581.796875, best_valid_rmse = 8.733287412501104\n",
      "Epoch 25, Step 10450: Loss = 31040.943359375, best_valid_rmse = 8.733287412501104\n",
      "Epoch 26, Step 10868: Loss = 32125.068359375, best_valid_rmse = 8.733287412501104\n",
      "Epoch 27, Step 11286: Loss = 31535.75, best_valid_rmse = 8.733287412501104\n",
      "Epoch 28, Step 11704: Loss = 30701.021484375, best_valid_rmse = 8.733287412501104\n",
      "Epoch 29, Step 12122: Loss = 32268.3046875, best_valid_rmse = 8.733287412501104\n",
      "Epoch 30, Step 12540: Loss = 31177.8359375, best_valid_rmse = 8.733287412501104\n",
      "Epoch 31, Step 12958: Loss = 32122.1484375, best_valid_rmse = 8.733287412501104\n",
      "Epoch 32, Step 13376: Loss = 30802.775390625, best_valid_rmse = 8.733276917923188\n",
      "Epoch 33, Step 13794: Loss = 30314.57421875, best_valid_rmse = 8.733276917923188\n",
      "Epoch 34, Step 14212: Loss = 29242.515625, best_valid_rmse = 8.731131746645385\n",
      "Epoch 35, Step 14630: Loss = 30225.517578125, best_valid_rmse = 8.731131746645385\n",
      "Epoch 36, Step 15048: Loss = 29822.728515625, best_valid_rmse = 8.721132104087317\n",
      "Epoch 37, Step 15466: Loss = 31687.328125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 38, Step 15884: Loss = 28296.55078125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 39, Step 16302: Loss = 28884.92578125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 40, Step 16720: Loss = 34396.125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 41, Step 17138: Loss = 30390.72265625, best_valid_rmse = 8.721132104087317\n",
      "Epoch 42, Step 17556: Loss = 31533.015625, best_valid_rmse = 8.721132104087317\n",
      "Epoch 43, Step 17974: Loss = 28280.232421875, best_valid_rmse = 8.721132104087317\n",
      "Epoch 44, Step 18392: Loss = 31004.517578125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 45, Step 18810: Loss = 31844.267578125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 46, Step 19228: Loss = 32471.375, best_valid_rmse = 8.721132104087317\n",
      "Epoch 47, Step 19646: Loss = 31636.248046875, best_valid_rmse = 8.721132104087317\n",
      "early stopping, step 19800, validation RMSE = 8.749034518834238\n",
      "H = 180: Test RMSE = 9.00075103931562\n",
      "\n",
      "Epoch 1, Step 418: Loss = 35608.92578125, best_valid_rmse = 9.001491170115884\n",
      "Epoch 2, Step 836: Loss = 35315.859375, best_valid_rmse = 8.878200128113022\n",
      "Epoch 3, Step 1254: Loss = 34476.1484375, best_valid_rmse = 8.819138535186656\n",
      "Epoch 4, Step 1672: Loss = 33937.52734375, best_valid_rmse = 8.783886100209712\n",
      "Epoch 5, Step 2090: Loss = 34343.15625, best_valid_rmse = 8.762482970279985\n",
      "Epoch 6, Step 2508: Loss = 32163.216796875, best_valid_rmse = 8.752194523277046\n",
      "Epoch 7, Step 2926: Loss = 33314.55078125, best_valid_rmse = 8.744276957212167\n",
      "Epoch 8, Step 3344: Loss = 33160.7734375, best_valid_rmse = 8.730190964942219\n",
      "Epoch 9, Step 3762: Loss = 32076.58203125, best_valid_rmse = 8.719205926167838\n",
      "Epoch 10, Step 4180: Loss = 31774.453125, best_valid_rmse = 8.691122455727196\n",
      "Epoch 11, Step 4598: Loss = 31893.72265625, best_valid_rmse = 8.686228282374476\n",
      "Epoch 12, Step 5016: Loss = 33366.546875, best_valid_rmse = 8.686228282374476\n",
      "Epoch 13, Step 5434: Loss = 32553.830078125, best_valid_rmse = 8.686228282374476\n",
      "Epoch 14, Step 5852: Loss = 32460.298828125, best_valid_rmse = 8.686228282374476\n",
      "Epoch 15, Step 6270: Loss = 29339.984375, best_valid_rmse = 8.684844383416356\n",
      "Epoch 16, Step 6688: Loss = 30111.5859375, best_valid_rmse = 8.654968981091756\n",
      "Epoch 17, Step 7106: Loss = 30227.763671875, best_valid_rmse = 8.654968981091756\n",
      "Epoch 18, Step 7524: Loss = 30722.46484375, best_valid_rmse = 8.654968981091756\n",
      "Epoch 19, Step 7942: Loss = 30613.650390625, best_valid_rmse = 8.654968981091756\n",
      "Epoch 20, Step 8360: Loss = 29621.103515625, best_valid_rmse = 8.654968981091756\n",
      "Epoch 21, Step 8778: Loss = 30992.7890625, best_valid_rmse = 8.654968981091756\n",
      "Epoch 22, Step 9196: Loss = 32969.6328125, best_valid_rmse = 8.654968981091756\n",
      "Epoch 23, Step 9614: Loss = 29754.5859375, best_valid_rmse = 8.654968981091756\n",
      "Epoch 24, Step 10032: Loss = 30970.873046875, best_valid_rmse = 8.654968981091756\n",
      "Epoch 25, Step 10450: Loss = 28240.841796875, best_valid_rmse = 8.654968981091756\n",
      "Epoch 26, Step 10868: Loss = 33438.4375, best_valid_rmse = 8.654968981091756\n",
      "Epoch 27, Step 11286: Loss = 30044.19140625, best_valid_rmse = 8.654968981091756\n",
      "early stopping, step 11500, validation RMSE = 8.697770392914155\n",
      "H = 360: Test RMSE = 8.936875654569297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d_hidden in [20, 45, 180, 360]:\n",
    "    \n",
    "    d_input = subtrain_set.Xnp.shape[1]\n",
    "    d_output = 1\n",
    "\n",
    "    mlp = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d_input, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_output)\n",
    "    )\n",
    "    mlp = mlp.float()\n",
    "    mlp.to(device)\n",
    "    weight_path = f'./MLP{d_hidden}_drop_weight'\n",
    "\n",
    "    # optimizer\n",
    "    lr = 0.001\n",
    "    momentum = 0\n",
    "    weight_decay = 0\n",
    "    adam_optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # loss\n",
    "    l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    # train\n",
    "    all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=adam_optimizer, loss_f=l2_loss, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=False)        \n",
    "    \n",
    "    # test RMSE\n",
    "    saved_mlp = torch.load(weight_path)\n",
    "    test_rmse = RMSE(saved_mlp, test_loader)\n",
    "    print(f'H = {d_hidden}: Test RMSE = {test_rmse}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H = 20, Test RMSE = 9.384  \n",
    "H = 45, Test RMSE = 9.123  \n",
    "H = 180, Test RMSE = 9.001  \n",
    "H = 360, Test RMSE = 8.937  \n",
    "The test RMSE is the lowest when H = 360."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. L2 + L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, loss_f1, loss_f2, loss_w, max_epoch, max_step, valid_interval, weight_path, train_rmse):\n",
    "    \n",
    "    step_count = 0\n",
    "    best_step_count = 0\n",
    "    best_valid_rmse = np.inf\n",
    "    all_train_rmse = []\n",
    "    all_valid_rmse = []\n",
    "    all_step = []\n",
    "\n",
    "    for epoch in range(1, max_epoch+1):\n",
    "        for batch, (inputs, targets) in enumerate(subtrain_loader):\n",
    "\n",
    "            targets = targets.reshape((-1, 1))\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            model.to(device)\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_w * loss_f1(outputs, targets) + (1-loss_w) * loss_f2(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            step_count += 1\n",
    "\n",
    "            # check train/validation RMSE\n",
    "            if (step_count % valid_interval == 0):\n",
    "\n",
    "                # subtrain, validation RMSE\n",
    "                if train_rmse:\n",
    "                    train_rmse = RMSE(model, subtrain_loader)\n",
    "                    all_train_rmse.append(train_rmse)\n",
    "                valid_rmse = RMSE(model, valid_loader)\n",
    "                all_valid_rmse.append(valid_rmse)\n",
    "                all_step.append(step_count)\n",
    "\n",
    "                # update weight\n",
    "                if valid_rmse < best_valid_rmse:\n",
    "                    best_step_count = step_count\n",
    "                    best_valid_rmse = valid_rmse\n",
    "                    torch.save(model, weight_path)\n",
    "\n",
    "                # early stopping\n",
    "                elif (step_count - best_step_count >= max_step):\n",
    "                    print(f'early stopping, step {step_count}, validation RMSE = {valid_rmse}')\n",
    "                    return all_step, all_train_rmse, all_valid_rmse\n",
    "                \n",
    "        print(f'Epoch {epoch}, Step {step_count}: Loss = {loss.item()}, best_valid_rmse = {best_valid_rmse}')\n",
    "    return all_step, all_train_rmse, all_valid_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 19039.677734375, best_valid_rmse = 9.207003751155755\n",
      "Epoch 2, Step 836: Loss = 20158.171875, best_valid_rmse = 9.070728506168356\n",
      "Epoch 3, Step 1254: Loss = 19221.349609375, best_valid_rmse = 8.995735369384574\n",
      "Epoch 4, Step 1672: Loss = 18910.095703125, best_valid_rmse = 8.948240543267026\n",
      "Epoch 5, Step 2090: Loss = 18798.01171875, best_valid_rmse = 8.934639047125376\n",
      "Epoch 6, Step 2508: Loss = 19771.625, best_valid_rmse = 8.92031152421668\n",
      "Epoch 7, Step 2926: Loss = 19387.59765625, best_valid_rmse = 8.907486930292979\n",
      "Epoch 8, Step 3344: Loss = 19379.306640625, best_valid_rmse = 8.893747638748312\n",
      "Epoch 9, Step 3762: Loss = 19221.673828125, best_valid_rmse = 8.886163999938404\n",
      "Epoch 10, Step 4180: Loss = 18663.40234375, best_valid_rmse = 8.863302071561145\n",
      "Epoch 11, Step 4598: Loss = 18827.24609375, best_valid_rmse = 8.863302071561145\n",
      "Epoch 12, Step 5016: Loss = 18568.0625, best_valid_rmse = 8.863302071561145\n",
      "Epoch 13, Step 5434: Loss = 18220.98046875, best_valid_rmse = 8.854825698510362\n",
      "Epoch 14, Step 5852: Loss = 19443.931640625, best_valid_rmse = 8.846347907406319\n",
      "Epoch 15, Step 6270: Loss = 18819.970703125, best_valid_rmse = 8.846347907406319\n",
      "Epoch 16, Step 6688: Loss = 18226.94140625, best_valid_rmse = 8.828747808330697\n",
      "Epoch 17, Step 7106: Loss = 17780.767578125, best_valid_rmse = 8.828747808330697\n",
      "Epoch 18, Step 7524: Loss = 19022.35546875, best_valid_rmse = 8.828747808330697\n",
      "Epoch 19, Step 7942: Loss = 18054.1875, best_valid_rmse = 8.828747808330697\n",
      "Epoch 20, Step 8360: Loss = 18207.703125, best_valid_rmse = 8.828747808330697\n",
      "Epoch 21, Step 8778: Loss = 18921.2890625, best_valid_rmse = 8.825249600705998\n",
      "Epoch 22, Step 9196: Loss = 19076.74609375, best_valid_rmse = 8.825249600705998\n",
      "Epoch 23, Step 9614: Loss = 18563.951171875, best_valid_rmse = 8.822418870322712\n",
      "Epoch 24, Step 10032: Loss = 18918.578125, best_valid_rmse = 8.822418870322712\n",
      "Epoch 25, Step 10450: Loss = 17972.791015625, best_valid_rmse = 8.822418870322712\n",
      "Epoch 26, Step 10868: Loss = 18411.86328125, best_valid_rmse = 8.822418870322712\n",
      "Epoch 27, Step 11286: Loss = 18833.1953125, best_valid_rmse = 8.822418870322712\n",
      "Epoch 28, Step 11704: Loss = 17751.052734375, best_valid_rmse = 8.822418870322712\n",
      "Epoch 29, Step 12122: Loss = 18253.560546875, best_valid_rmse = 8.822418870322712\n",
      "Epoch 30, Step 12540: Loss = 17461.73046875, best_valid_rmse = 8.822418870322712\n",
      "Epoch 31, Step 12958: Loss = 17746.06640625, best_valid_rmse = 8.822418870322712\n",
      "Epoch 32, Step 13376: Loss = 18308.419921875, best_valid_rmse = 8.822418870322712\n",
      "Epoch 33, Step 13794: Loss = 17884.845703125, best_valid_rmse = 8.822418870322712\n",
      "Epoch 34, Step 14212: Loss = 18072.5078125, best_valid_rmse = 8.822418870322712\n",
      "Epoch 35, Step 14630: Loss = 18165.3671875, best_valid_rmse = 8.815252751465602\n",
      "Epoch 36, Step 15048: Loss = 18256.578125, best_valid_rmse = 8.815252751465602\n",
      "Epoch 37, Step 15466: Loss = 18069.42578125, best_valid_rmse = 8.815252751465602\n",
      "Epoch 38, Step 15884: Loss = 18353.015625, best_valid_rmse = 8.815252751465602\n",
      "Epoch 39, Step 16302: Loss = 19064.296875, best_valid_rmse = 8.815252751465602\n",
      "Epoch 40, Step 16720: Loss = 17251.640625, best_valid_rmse = 8.815252751465602\n",
      "Epoch 41, Step 17138: Loss = 19031.525390625, best_valid_rmse = 8.809645766560216\n",
      "Epoch 42, Step 17556: Loss = 17697.2265625, best_valid_rmse = 8.809645766560216\n",
      "Epoch 43, Step 17974: Loss = 19089.560546875, best_valid_rmse = 8.809645766560216\n",
      "Epoch 44, Step 18392: Loss = 17228.7578125, best_valid_rmse = 8.809645766560216\n",
      "Epoch 45, Step 18810: Loss = 19136.0546875, best_valid_rmse = 8.809645766560216\n",
      "Epoch 46, Step 19228: Loss = 18228.689453125, best_valid_rmse = 8.809645766560216\n",
      "Epoch 47, Step 19646: Loss = 18023.1484375, best_valid_rmse = 8.809645766560216\n",
      "Epoch 48, Step 20064: Loss = 17265.810546875, best_valid_rmse = 8.809645766560216\n",
      "Epoch 49, Step 20482: Loss = 18496.814453125, best_valid_rmse = 8.809645766560216\n",
      "Epoch 50, Step 20900: Loss = 17992.771484375, best_valid_rmse = 8.80709926820413\n",
      "Epoch 51, Step 21318: Loss = 17518.921875, best_valid_rmse = 8.80709926820413\n",
      "Epoch 52, Step 21736: Loss = 17693.376953125, best_valid_rmse = 8.80709926820413\n",
      "Epoch 53, Step 22154: Loss = 18972.94140625, best_valid_rmse = 8.80709926820413\n",
      "Epoch 54, Step 22572: Loss = 16790.125, best_valid_rmse = 8.80709926820413\n",
      "Epoch 55, Step 22990: Loss = 17566.26953125, best_valid_rmse = 8.80709926820413\n",
      "Epoch 56, Step 23408: Loss = 18328.3203125, best_valid_rmse = 8.802004061318112\n",
      "Epoch 57, Step 23826: Loss = 17535.677734375, best_valid_rmse = 8.802004061318112\n",
      "Epoch 58, Step 24244: Loss = 18165.791015625, best_valid_rmse = 8.802004061318112\n",
      "Epoch 59, Step 24662: Loss = 17926.611328125, best_valid_rmse = 8.802004061318112\n",
      "Epoch 60, Step 25080: Loss = 18196.50390625, best_valid_rmse = 8.802004061318112\n",
      "Epoch 61, Step 25498: Loss = 18317.080078125, best_valid_rmse = 8.802004061318112\n",
      "Epoch 62, Step 25916: Loss = 17229.466796875, best_valid_rmse = 8.802004061318112\n",
      "Epoch 63, Step 26334: Loss = 17846.37109375, best_valid_rmse = 8.802004061318112\n",
      "Epoch 64, Step 26752: Loss = 16815.78515625, best_valid_rmse = 8.802004061318112\n",
      "Epoch 65, Step 27170: Loss = 17621.16015625, best_valid_rmse = 8.802004061318112\n",
      "Epoch 66, Step 27588: Loss = 18589.990234375, best_valid_rmse = 8.802004061318112\n",
      "Epoch 67, Step 28006: Loss = 18241.994140625, best_valid_rmse = 8.802004061318112\n",
      "early stopping, step 28300, validation RMSE = 8.82764093053397\n"
     ]
    }
   ],
   "source": [
    "d_hidden = 90\n",
    "d_input = subtrain_set.Xnp.shape[1]\n",
    "d_output = 1\n",
    "\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d_input, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_output)\n",
    ")\n",
    "mlp = mlp.float()\n",
    "mlp.to(device)\n",
    "weight_path = './MLP90_l1_l2_weight'\n",
    "\n",
    "# optimizer\n",
    "lr = 0.001\n",
    "momentum = 0\n",
    "weight_decay = 0\n",
    "adam_optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# loss\n",
    "z = 0.5\n",
    "l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "l1_loss = torch.nn.L1Loss(reduction='sum')\n",
    "\n",
    "# train\n",
    "all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=adam_optimizer, loss_f1=l2_loss, loss_f2=l1_loss, loss_w=z, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'H=90 with L1+L2 Loss, Training & Validation RMSE')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot subtrain / validation RMSE\n",
    "plt.plot(all_step, all_train_rmse, c='b', label='Training RMSE')\n",
    "plt.plot(all_step, all_valid_rmse, c='r', label='Validation RMSE')\n",
    "plt.legend()\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('H=90 with L1+L2 Loss, Training & Validation RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 9.012171112552256\n"
     ]
    }
   ],
   "source": [
    "# test RMSE\n",
    "saved_mlp = torch.load(weight_path)\n",
    "test_rmse = RMSE(saved_mlp, test_loader)\n",
    "print(f'Test RMSE = {test_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 2286.55322265625, best_valid_rmse = 9.529282127906578\n",
      "Epoch 2, Step 836: Loss = 2276.855224609375, best_valid_rmse = 9.408890512526758\n",
      "Epoch 3, Step 1254: Loss = 2220.9931640625, best_valid_rmse = 9.348120354665896\n",
      "Epoch 4, Step 1672: Loss = 2178.87646484375, best_valid_rmse = 9.271568688171815\n",
      "Epoch 5, Step 2090: Loss = 2183.24755859375, best_valid_rmse = 9.262593446738155\n",
      "Epoch 6, Step 2508: Loss = 2250.6748046875, best_valid_rmse = 9.221690893159767\n",
      "Epoch 7, Step 2926: Loss = 2269.218017578125, best_valid_rmse = 9.220542310999868\n",
      "Epoch 8, Step 3344: Loss = 2246.974609375, best_valid_rmse = 9.207401634082654\n",
      "Epoch 9, Step 3762: Loss = 2278.02490234375, best_valid_rmse = 9.159364819672334\n",
      "Epoch 10, Step 4180: Loss = 2280.98095703125, best_valid_rmse = 9.159364819672334\n",
      "Epoch 11, Step 4598: Loss = 2203.843994140625, best_valid_rmse = 9.159364819672334\n",
      "Epoch 12, Step 5016: Loss = 2196.048828125, best_valid_rmse = 9.159364819672334\n",
      "Epoch 13, Step 5434: Loss = 2267.88232421875, best_valid_rmse = 9.155437060893416\n",
      "Epoch 14, Step 5852: Loss = 2156.24853515625, best_valid_rmse = 9.155437060893416\n",
      "Epoch 15, Step 6270: Loss = 2212.17138671875, best_valid_rmse = 9.155437060893416\n",
      "Epoch 16, Step 6688: Loss = 2180.509765625, best_valid_rmse = 9.155437060893416\n",
      "Epoch 17, Step 7106: Loss = 2176.21142578125, best_valid_rmse = 9.155437060893416\n",
      "Epoch 18, Step 7524: Loss = 2202.470703125, best_valid_rmse = 9.155437060893416\n",
      "Epoch 19, Step 7942: Loss = 2272.157958984375, best_valid_rmse = 9.155437060893416\n",
      "Epoch 20, Step 8360: Loss = 2257.271728515625, best_valid_rmse = 9.155437060893416\n",
      "Epoch 21, Step 8778: Loss = 2216.92138671875, best_valid_rmse = 9.155437060893416\n",
      "Epoch 22, Step 9196: Loss = 2106.673828125, best_valid_rmse = 9.155437060893416\n",
      "Epoch 23, Step 9614: Loss = 2202.194091796875, best_valid_rmse = 9.13022402273886\n",
      "Epoch 24, Step 10032: Loss = 2169.219970703125, best_valid_rmse = 9.13022402273886\n",
      "Epoch 25, Step 10450: Loss = 2189.486328125, best_valid_rmse = 9.13022402273886\n",
      "Epoch 26, Step 10868: Loss = 2196.253173828125, best_valid_rmse = 9.13022402273886\n",
      "Epoch 27, Step 11286: Loss = 2157.5009765625, best_valid_rmse = 9.13022402273886\n",
      "Epoch 28, Step 11704: Loss = 2182.21630859375, best_valid_rmse = 9.13022402273886\n",
      "Epoch 29, Step 12122: Loss = 2169.667724609375, best_valid_rmse = 9.13022402273886\n",
      "Epoch 30, Step 12540: Loss = 2194.81884765625, best_valid_rmse = 9.121731788055587\n",
      "Epoch 31, Step 12958: Loss = 2170.310546875, best_valid_rmse = 9.121731788055587\n",
      "Epoch 32, Step 13376: Loss = 2163.028564453125, best_valid_rmse = 9.121731788055587\n",
      "Epoch 33, Step 13794: Loss = 2192.0615234375, best_valid_rmse = 9.121731788055587\n",
      "Epoch 34, Step 14212: Loss = 2191.240966796875, best_valid_rmse = 9.121731788055587\n",
      "Epoch 35, Step 14630: Loss = 2160.033935546875, best_valid_rmse = 9.119799180827265\n",
      "Epoch 36, Step 15048: Loss = 2178.677001953125, best_valid_rmse = 9.11345022986307\n",
      "Epoch 37, Step 15466: Loss = 2183.201171875, best_valid_rmse = 9.11345022986307\n",
      "Epoch 38, Step 15884: Loss = 2142.225341796875, best_valid_rmse = 9.11345022986307\n",
      "Epoch 39, Step 16302: Loss = 2151.593994140625, best_valid_rmse = 9.11345022986307\n",
      "Epoch 40, Step 16720: Loss = 2186.1181640625, best_valid_rmse = 9.11345022986307\n",
      "Epoch 41, Step 17138: Loss = 2122.73583984375, best_valid_rmse = 9.11345022986307\n",
      "Epoch 42, Step 17556: Loss = 2151.857421875, best_valid_rmse = 9.11345022986307\n",
      "Epoch 43, Step 17974: Loss = 2127.096435546875, best_valid_rmse = 9.11345022986307\n",
      "Epoch 44, Step 18392: Loss = 2186.198974609375, best_valid_rmse = 9.11345022986307\n",
      "Epoch 45, Step 18810: Loss = 2126.22607421875, best_valid_rmse = 9.11345022986307\n",
      "Epoch 46, Step 19228: Loss = 2167.611328125, best_valid_rmse = 9.11345022986307\n",
      "Epoch 47, Step 19646: Loss = 2192.97900390625, best_valid_rmse = 9.11345022986307\n",
      "early stopping, step 20000, validation RMSE = 9.169387791325335\n",
      "z = 0.0: Test RMSE = 9.2479751069796\n",
      "\n",
      "Epoch 1, Step 418: Loss = 5685.50048828125, best_valid_rmse = 9.17283410223962\n",
      "Epoch 2, Step 836: Loss = 5759.861328125, best_valid_rmse = 9.069133394228425\n",
      "Epoch 3, Step 1254: Loss = 5708.005859375, best_valid_rmse = 9.001021293999752\n",
      "Epoch 4, Step 1672: Loss = 5432.935546875, best_valid_rmse = 8.96141742255185\n",
      "Epoch 5, Step 2090: Loss = 5741.74755859375, best_valid_rmse = 8.929801059516457\n",
      "Epoch 6, Step 2508: Loss = 5431.64697265625, best_valid_rmse = 8.929801059516457\n",
      "Epoch 7, Step 2926: Loss = 5561.5380859375, best_valid_rmse = 8.915377775547308\n",
      "Epoch 8, Step 3344: Loss = 5444.00146484375, best_valid_rmse = 8.915377775547308\n",
      "Epoch 9, Step 3762: Loss = 5420.12255859375, best_valid_rmse = 8.899858661995102\n",
      "Epoch 10, Step 4180: Loss = 5439.36572265625, best_valid_rmse = 8.879577571350806\n",
      "Epoch 11, Step 4598: Loss = 5448.76953125, best_valid_rmse = 8.87666425614964\n",
      "Epoch 12, Step 5016: Loss = 5506.94677734375, best_valid_rmse = 8.863150306338149\n",
      "Epoch 13, Step 5434: Loss = 5528.8193359375, best_valid_rmse = 8.863150306338149\n",
      "Epoch 14, Step 5852: Loss = 5332.26513671875, best_valid_rmse = 8.863150306338149\n",
      "Epoch 15, Step 6270: Loss = 5536.919921875, best_valid_rmse = 8.84201739989155\n",
      "Epoch 16, Step 6688: Loss = 5425.37939453125, best_valid_rmse = 8.84201739989155\n",
      "Epoch 17, Step 7106: Loss = 5290.6171875, best_valid_rmse = 8.836903890592476\n",
      "Epoch 18, Step 7524: Loss = 5576.619140625, best_valid_rmse = 8.831150705887987\n",
      "Epoch 19, Step 7942: Loss = 5656.44140625, best_valid_rmse = 8.831150705887987\n",
      "Epoch 20, Step 8360: Loss = 5507.68017578125, best_valid_rmse = 8.831150705887987\n",
      "Epoch 21, Step 8778: Loss = 5439.96533203125, best_valid_rmse = 8.831150705887987\n",
      "Epoch 22, Step 9196: Loss = 5622.46484375, best_valid_rmse = 8.831150705887987\n",
      "Epoch 23, Step 9614: Loss = 5473.39013671875, best_valid_rmse = 8.831150705887987\n",
      "Epoch 24, Step 10032: Loss = 5518.40478515625, best_valid_rmse = 8.831150705887987\n",
      "Epoch 25, Step 10450: Loss = 5469.548828125, best_valid_rmse = 8.831150705887987\n",
      "Epoch 26, Step 10868: Loss = 5522.810546875, best_valid_rmse = 8.830696797315465\n",
      "Epoch 27, Step 11286: Loss = 5565.2900390625, best_valid_rmse = 8.830696797315465\n",
      "Epoch 28, Step 11704: Loss = 5347.5703125, best_valid_rmse = 8.830696797315465\n",
      "Epoch 29, Step 12122: Loss = 5431.724609375, best_valid_rmse = 8.830696797315465\n",
      "Epoch 30, Step 12540: Loss = 5450.7587890625, best_valid_rmse = 8.828206753267935\n",
      "Epoch 31, Step 12958: Loss = 5442.7158203125, best_valid_rmse = 8.828206753267935\n",
      "Epoch 32, Step 13376: Loss = 5555.5234375, best_valid_rmse = 8.828206753267935\n",
      "Epoch 33, Step 13794: Loss = 5363.55322265625, best_valid_rmse = 8.828206753267935\n",
      "Epoch 34, Step 14212: Loss = 5449.02734375, best_valid_rmse = 8.828206753267935\n",
      "Epoch 35, Step 14630: Loss = 5596.14111328125, best_valid_rmse = 8.828206753267935\n",
      "Epoch 36, Step 15048: Loss = 5348.75341796875, best_valid_rmse = 8.828206753267935\n",
      "Epoch 37, Step 15466: Loss = 5384.8720703125, best_valid_rmse = 8.828206753267935\n",
      "Epoch 38, Step 15884: Loss = 5194.1689453125, best_valid_rmse = 8.828206753267935\n",
      "Epoch 39, Step 16302: Loss = 5283.50830078125, best_valid_rmse = 8.828206753267935\n",
      "Epoch 40, Step 16720: Loss = 5331.0478515625, best_valid_rmse = 8.828206753267935\n",
      "Epoch 41, Step 17138: Loss = 5432.10205078125, best_valid_rmse = 8.823119458314956\n",
      "Epoch 42, Step 17556: Loss = 5070.6787109375, best_valid_rmse = 8.822945003968625\n",
      "Epoch 43, Step 17974: Loss = 5110.16650390625, best_valid_rmse = 8.822945003968625\n",
      "Epoch 44, Step 18392: Loss = 5172.4423828125, best_valid_rmse = 8.822945003968625\n",
      "Epoch 45, Step 18810: Loss = 5462.41015625, best_valid_rmse = 8.822086430565978\n",
      "Epoch 46, Step 19228: Loss = 5303.5361328125, best_valid_rmse = 8.822086430565978\n",
      "Epoch 47, Step 19646: Loss = 5256.537109375, best_valid_rmse = 8.822086430565978\n",
      "Epoch 48, Step 20064: Loss = 5243.2568359375, best_valid_rmse = 8.822086430565978\n",
      "Epoch 49, Step 20482: Loss = 5489.48193359375, best_valid_rmse = 8.822086430565978\n",
      "Epoch 50, Step 20900: Loss = 5455.31640625, best_valid_rmse = 8.822086430565978\n",
      "Epoch 51, Step 21318: Loss = 5654.02685546875, best_valid_rmse = 8.822086430565978\n",
      "Epoch 52, Step 21736: Loss = 5013.884765625, best_valid_rmse = 8.822086430565978\n",
      "Epoch 53, Step 22154: Loss = 5415.02099609375, best_valid_rmse = 8.822086430565978\n",
      "Epoch 54, Step 22572: Loss = 5317.0859375, best_valid_rmse = 8.822086430565978\n",
      "Epoch 55, Step 22990: Loss = 5379.2275390625, best_valid_rmse = 8.822086430565978\n",
      "Epoch 56, Step 23408: Loss = 5132.7607421875, best_valid_rmse = 8.822086430565978\n",
      "early stopping, step 23600, validation RMSE = 8.855289633496463\n",
      "z = 0.1: Test RMSE = 9.000391936286942\n",
      "\n",
      "Epoch 1, Step 418: Loss = 33089.671875, best_valid_rmse = 9.19811467996941\n",
      "Epoch 2, Step 836: Loss = 34511.28125, best_valid_rmse = 9.085322035666808\n",
      "Epoch 3, Step 1254: Loss = 32174.1171875, best_valid_rmse = 8.987598952878594\n",
      "Epoch 4, Step 1672: Loss = 32156.8046875, best_valid_rmse = 8.936254848000749\n",
      "Epoch 5, Step 2090: Loss = 33094.6328125, best_valid_rmse = 8.931575886149862\n",
      "Epoch 6, Step 2508: Loss = 33009.2421875, best_valid_rmse = 8.914277420960662\n",
      "Epoch 7, Step 2926: Loss = 30725.798828125, best_valid_rmse = 8.90428395045997\n",
      "Epoch 8, Step 3344: Loss = 30212.451171875, best_valid_rmse = 8.898720643280415\n",
      "Epoch 9, Step 3762: Loss = 30414.52734375, best_valid_rmse = 8.882549412740973\n",
      "Epoch 10, Step 4180: Loss = 30751.31640625, best_valid_rmse = 8.882549412740973\n",
      "Epoch 11, Step 4598: Loss = 31659.037109375, best_valid_rmse = 8.8539394664161\n",
      "Epoch 12, Step 5016: Loss = 27956.5625, best_valid_rmse = 8.8539394664161\n",
      "Epoch 13, Step 5434: Loss = 31079.7578125, best_valid_rmse = 8.8539394664161\n",
      "Epoch 14, Step 5852: Loss = 30715.474609375, best_valid_rmse = 8.8539394664161\n",
      "Epoch 15, Step 6270: Loss = 29111.06640625, best_valid_rmse = 8.8539394664161\n",
      "Epoch 16, Step 6688: Loss = 30318.158203125, best_valid_rmse = 8.845727477763257\n",
      "Epoch 17, Step 7106: Loss = 31007.861328125, best_valid_rmse = 8.845727477763257\n",
      "Epoch 18, Step 7524: Loss = 31542.03125, best_valid_rmse = 8.845727477763257\n",
      "Epoch 19, Step 7942: Loss = 31401.490234375, best_valid_rmse = 8.845727477763257\n",
      "Epoch 20, Step 8360: Loss = 30682.83984375, best_valid_rmse = 8.845727477763257\n",
      "Epoch 21, Step 8778: Loss = 30482.2265625, best_valid_rmse = 8.836414280357712\n",
      "Epoch 22, Step 9196: Loss = 31265.078125, best_valid_rmse = 8.832626434333907\n",
      "Epoch 23, Step 9614: Loss = 31057.888671875, best_valid_rmse = 8.832626434333907\n",
      "Epoch 24, Step 10032: Loss = 28649.291015625, best_valid_rmse = 8.832626434333907\n",
      "Epoch 25, Step 10450: Loss = 31444.45703125, best_valid_rmse = 8.832626434333907\n",
      "Epoch 26, Step 10868: Loss = 29690.509765625, best_valid_rmse = 8.832626434333907\n",
      "Epoch 27, Step 11286: Loss = 31210.07421875, best_valid_rmse = 8.832626434333907\n",
      "Epoch 28, Step 11704: Loss = 32969.51171875, best_valid_rmse = 8.832626434333907\n",
      "Epoch 29, Step 12122: Loss = 30915.474609375, best_valid_rmse = 8.832626434333907\n",
      "Epoch 30, Step 12540: Loss = 32098.318359375, best_valid_rmse = 8.832626434333907\n",
      "Epoch 31, Step 12958: Loss = 31427.75390625, best_valid_rmse = 8.829533378358173\n",
      "Epoch 32, Step 13376: Loss = 31134.05859375, best_valid_rmse = 8.829533378358173\n",
      "Epoch 33, Step 13794: Loss = 31113.447265625, best_valid_rmse = 8.829533378358173\n",
      "Epoch 34, Step 14212: Loss = 31837.330078125, best_valid_rmse = 8.829533378358173\n",
      "Epoch 35, Step 14630: Loss = 32092.326171875, best_valid_rmse = 8.829533378358173\n",
      "Epoch 36, Step 15048: Loss = 31896.10546875, best_valid_rmse = 8.829533378358173\n",
      "Epoch 37, Step 15466: Loss = 30928.6484375, best_valid_rmse = 8.825628347460713\n",
      "Epoch 38, Step 15884: Loss = 31331.09765625, best_valid_rmse = 8.825628347460713\n",
      "Epoch 39, Step 16302: Loss = 28524.39453125, best_valid_rmse = 8.825628347460713\n",
      "Epoch 40, Step 16720: Loss = 31612.921875, best_valid_rmse = 8.825628347460713\n",
      "Epoch 41, Step 17138: Loss = 30771.916015625, best_valid_rmse = 8.825628347460713\n",
      "Epoch 42, Step 17556: Loss = 31102.896484375, best_valid_rmse = 8.823692903026332\n",
      "Epoch 43, Step 17974: Loss = 30943.080078125, best_valid_rmse = 8.823692903026332\n",
      "Epoch 44, Step 18392: Loss = 31233.7578125, best_valid_rmse = 8.823692903026332\n",
      "Epoch 45, Step 18810: Loss = 32034.505859375, best_valid_rmse = 8.823692903026332\n",
      "Epoch 46, Step 19228: Loss = 30749.177734375, best_valid_rmse = 8.806601572980943\n",
      "Epoch 47, Step 19646: Loss = 31498.98046875, best_valid_rmse = 8.806601572980943\n",
      "Epoch 48, Step 20064: Loss = 28261.857421875, best_valid_rmse = 8.806601572980943\n",
      "Epoch 49, Step 20482: Loss = 29652.396484375, best_valid_rmse = 8.806601572980943\n",
      "Epoch 50, Step 20900: Loss = 29600.3359375, best_valid_rmse = 8.806601572980943\n",
      "Epoch 51, Step 21318: Loss = 31070.9765625, best_valid_rmse = 8.806601572980943\n",
      "Epoch 52, Step 21736: Loss = 29227.486328125, best_valid_rmse = 8.806601572980943\n",
      "Epoch 53, Step 22154: Loss = 28905.11328125, best_valid_rmse = 8.804439057532893\n",
      "Epoch 54, Step 22572: Loss = 29367.134765625, best_valid_rmse = 8.804439057532893\n",
      "Epoch 55, Step 22990: Loss = 30145.994140625, best_valid_rmse = 8.804439057532893\n",
      "Epoch 56, Step 23408: Loss = 32469.96875, best_valid_rmse = 8.804439057532893\n",
      "Epoch 57, Step 23826: Loss = 29748.228515625, best_valid_rmse = 8.804439057532893\n",
      "Epoch 58, Step 24244: Loss = 28355.25390625, best_valid_rmse = 8.804439057532893\n",
      "Epoch 59, Step 24662: Loss = 31046.65234375, best_valid_rmse = 8.804439057532893\n",
      "Epoch 60, Step 25080: Loss = 30975.794921875, best_valid_rmse = 8.804439057532893\n",
      "Epoch 61, Step 25498: Loss = 30925.24609375, best_valid_rmse = 8.804439057532893\n",
      "Epoch 62, Step 25916: Loss = 32378.65625, best_valid_rmse = 8.804439057532893\n",
      "Epoch 63, Step 26334: Loss = 29237.1875, best_valid_rmse = 8.804439057532893\n",
      "Epoch 64, Step 26752: Loss = 30280.443359375, best_valid_rmse = 8.803752905049755\n",
      "Epoch 65, Step 27170: Loss = 30736.578125, best_valid_rmse = 8.803752905049755\n",
      "Epoch 66, Step 27588: Loss = 30879.07421875, best_valid_rmse = 8.803752905049755\n",
      "Epoch 67, Step 28006: Loss = 31417.375, best_valid_rmse = 8.803752905049755\n",
      "Epoch 68, Step 28424: Loss = 28927.046875, best_valid_rmse = 8.803752905049755\n",
      "Epoch 69, Step 28842: Loss = 31171.708984375, best_valid_rmse = 8.803752905049755\n",
      "Epoch 70, Step 29260: Loss = 27963.046875, best_valid_rmse = 8.803752905049755\n",
      "Epoch 71, Step 29678: Loss = 31250.154296875, best_valid_rmse = 8.803752905049755\n",
      "Epoch 72, Step 30096: Loss = 30960.7265625, best_valid_rmse = 8.803752905049755\n",
      "Epoch 73, Step 30514: Loss = 30279.296875, best_valid_rmse = 8.803752905049755\n",
      "Epoch 74, Step 30932: Loss = 31290.337890625, best_valid_rmse = 8.803752905049755\n",
      "Epoch 75, Step 31350: Loss = 27185.634765625, best_valid_rmse = 8.803752905049755\n",
      "early stopping, step 31600, validation RMSE = 8.848297284794064\n",
      "z = 0.9: Test RMSE = 8.998227600027366\n",
      "\n",
      "Epoch 1, Step 418: Loss = 37403.625, best_valid_rmse = 9.163182047875111\n",
      "Epoch 2, Step 836: Loss = 37538.56640625, best_valid_rmse = 9.062994931118343\n",
      "Epoch 3, Step 1254: Loss = 34139.8671875, best_valid_rmse = 8.997117888354738\n",
      "Epoch 4, Step 1672: Loss = 36580.4296875, best_valid_rmse = 8.960675603655272\n",
      "Epoch 5, Step 2090: Loss = 37041.07421875, best_valid_rmse = 8.946884819169165\n",
      "Epoch 6, Step 2508: Loss = 35662.0390625, best_valid_rmse = 8.929084690281737\n",
      "Epoch 7, Step 2926: Loss = 34892.375, best_valid_rmse = 8.915722761206993\n",
      "Epoch 8, Step 3344: Loss = 35713.578125, best_valid_rmse = 8.894849017724358\n",
      "Epoch 9, Step 3762: Loss = 34867.4765625, best_valid_rmse = 8.871973008862183\n",
      "Epoch 10, Step 4180: Loss = 33852.34375, best_valid_rmse = 8.871973008862183\n",
      "Epoch 11, Step 4598: Loss = 34031.59375, best_valid_rmse = 8.871973008862183\n",
      "Epoch 12, Step 5016: Loss = 35579.9453125, best_valid_rmse = 8.871973008862183\n",
      "Epoch 13, Step 5434: Loss = 33820.5234375, best_valid_rmse = 8.865791380267757\n",
      "Epoch 14, Step 5852: Loss = 33819.5078125, best_valid_rmse = 8.850939745178254\n",
      "Epoch 15, Step 6270: Loss = 34453.34375, best_valid_rmse = 8.850633656166984\n",
      "Epoch 16, Step 6688: Loss = 32096.98046875, best_valid_rmse = 8.840235875280396\n",
      "Epoch 17, Step 7106: Loss = 33021.15625, best_valid_rmse = 8.840235875280396\n",
      "Epoch 18, Step 7524: Loss = 37288.84375, best_valid_rmse = 8.840235875280396\n",
      "Epoch 19, Step 7942: Loss = 34855.43359375, best_valid_rmse = 8.840235875280396\n",
      "Epoch 20, Step 8360: Loss = 35170.0390625, best_valid_rmse = 8.840235875280396\n",
      "Epoch 21, Step 8778: Loss = 36130.8203125, best_valid_rmse = 8.84004346255818\n",
      "Epoch 22, Step 9196: Loss = 34332.0703125, best_valid_rmse = 8.84004346255818\n",
      "Epoch 23, Step 9614: Loss = 36806.9609375, best_valid_rmse = 8.84004346255818\n",
      "Epoch 24, Step 10032: Loss = 35128.8125, best_valid_rmse = 8.84004346255818\n",
      "Epoch 25, Step 10450: Loss = 35647.9296875, best_valid_rmse = 8.84004346255818\n",
      "Epoch 26, Step 10868: Loss = 33872.1484375, best_valid_rmse = 8.839367087930713\n",
      "Epoch 27, Step 11286: Loss = 33957.11328125, best_valid_rmse = 8.839367087930713\n",
      "Epoch 28, Step 11704: Loss = 35296.01953125, best_valid_rmse = 8.839367087930713\n",
      "Epoch 29, Step 12122: Loss = 33498.76953125, best_valid_rmse = 8.839367087930713\n",
      "Epoch 30, Step 12540: Loss = 34989.890625, best_valid_rmse = 8.83635845391643\n",
      "Epoch 31, Step 12958: Loss = 34254.3984375, best_valid_rmse = 8.833717429719474\n",
      "Epoch 32, Step 13376: Loss = 33449.4375, best_valid_rmse = 8.833717429719474\n",
      "Epoch 33, Step 13794: Loss = 32260.08984375, best_valid_rmse = 8.833717429719474\n",
      "Epoch 34, Step 14212: Loss = 35426.27734375, best_valid_rmse = 8.833717429719474\n",
      "Epoch 35, Step 14630: Loss = 31772.5, best_valid_rmse = 8.833717429719474\n",
      "Epoch 36, Step 15048: Loss = 31877.15625, best_valid_rmse = 8.833717429719474\n",
      "Epoch 37, Step 15466: Loss = 35054.78125, best_valid_rmse = 8.831200460418914\n",
      "Epoch 38, Step 15884: Loss = 34160.6171875, best_valid_rmse = 8.830132965811687\n",
      "Epoch 39, Step 16302: Loss = 35021.16796875, best_valid_rmse = 8.830132965811687\n",
      "Epoch 40, Step 16720: Loss = 36044.1328125, best_valid_rmse = 8.827705056896958\n",
      "Epoch 41, Step 17138: Loss = 33317.3984375, best_valid_rmse = 8.827705056896958\n",
      "Epoch 42, Step 17556: Loss = 32936.546875, best_valid_rmse = 8.825414846248563\n",
      "Epoch 43, Step 17974: Loss = 34930.1328125, best_valid_rmse = 8.825414846248563\n",
      "Epoch 44, Step 18392: Loss = 32872.546875, best_valid_rmse = 8.825414846248563\n",
      "Epoch 45, Step 18810: Loss = 34591.23828125, best_valid_rmse = 8.803333716419834\n",
      "Epoch 46, Step 19228: Loss = 34227.91796875, best_valid_rmse = 8.803333716419834\n",
      "Epoch 47, Step 19646: Loss = 35980.21875, best_valid_rmse = 8.803333716419834\n",
      "Epoch 48, Step 20064: Loss = 31362.890625, best_valid_rmse = 8.803333716419834\n",
      "Epoch 49, Step 20482: Loss = 34895.25, best_valid_rmse = 8.803333716419834\n",
      "Epoch 50, Step 20900: Loss = 33531.67578125, best_valid_rmse = 8.803333716419834\n",
      "Epoch 51, Step 21318: Loss = 32882.1484375, best_valid_rmse = 8.803333716419834\n",
      "Epoch 52, Step 21736: Loss = 33492.6640625, best_valid_rmse = 8.803333716419834\n",
      "Epoch 53, Step 22154: Loss = 34440.625, best_valid_rmse = 8.803333716419834\n",
      "Epoch 54, Step 22572: Loss = 32253.4375, best_valid_rmse = 8.803333716419834\n",
      "Epoch 55, Step 22990: Loss = 32156.3203125, best_valid_rmse = 8.803333716419834\n",
      "Epoch 56, Step 23408: Loss = 34846.640625, best_valid_rmse = 8.803333716419834\n",
      "early stopping, step 23700, validation RMSE = 8.838902012228765\n",
      "z = 1.0: Test RMSE = 9.02904742831244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for z in [0.0, 0.1, 0.9, 1.0]:\n",
    "    \n",
    "    d_hidden = 90\n",
    "    d_input = subtrain_set.Xnp.shape[1]\n",
    "    d_output = 1\n",
    "\n",
    "    mlp = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d_input, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_output)\n",
    "    )\n",
    "    mlp = mlp.float()\n",
    "    mlp.to(device)\n",
    "    weight_path = f'./MLP90_l1_l2_{z*10}_weight'\n",
    "\n",
    "    # optimizer\n",
    "    lr = 0.001\n",
    "    momentum = 0\n",
    "    weight_decay = 0\n",
    "    adam_optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # loss\n",
    "    l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "    l1_loss = torch.nn.L1Loss(reduction='sum')\n",
    "\n",
    "    # train\n",
    "    all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=adam_optimizer, loss_f1=l2_loss, loss_f2=l1_loss, loss_w=z, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=False)        \n",
    "    \n",
    "    # test RMSE\n",
    "    saved_mlp = torch.load(weight_path)\n",
    "    test_rmse = RMSE(saved_mlp, test_loader)\n",
    "    print(f'z = {z}: Test RMSE = {test_rmse}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = 0.0: Test RMSE = 9.248  \n",
    "z = 0.1: Test RMSE = 9.000  \n",
    "z = 0.9: Test RMSE = 8.998  \n",
    "z = 1.0: Test RMSE = 9.029  \n",
    "  \n",
    "The lowest test RMSE appears when z = 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. L2 + Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, y_pred, y):\n",
    "        zeros = torch.zeros_like(y)\n",
    "        return torch.sum(0.5 * torch.max((y - y_pred), zeros) + 0.5 * torch.max((y_pred - y), zeros), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 1136.8642578125, best_valid_rmse = 9.592942821831075\n",
      "Epoch 2, Step 836: Loss = 1150.536865234375, best_valid_rmse = 9.387847023385566\n",
      "Epoch 3, Step 1254: Loss = 1117.765625, best_valid_rmse = 9.305436621631438\n",
      "Epoch 4, Step 1672: Loss = 1089.094970703125, best_valid_rmse = 9.285557444938389\n",
      "Epoch 5, Step 2090: Loss = 1072.47216796875, best_valid_rmse = 9.250074967916674\n",
      "Epoch 6, Step 2508: Loss = 1094.2357177734375, best_valid_rmse = 9.21623056252559\n",
      "Epoch 7, Step 2926: Loss = 1107.087646484375, best_valid_rmse = 9.21623056252559\n",
      "Epoch 8, Step 3344: Loss = 1110.1632080078125, best_valid_rmse = 9.21623056252559\n",
      "Epoch 9, Step 3762: Loss = 1100.4564208984375, best_valid_rmse = 9.177753983792192\n",
      "Epoch 10, Step 4180: Loss = 1085.0706787109375, best_valid_rmse = 9.177753983792192\n",
      "Epoch 11, Step 4598: Loss = 1090.9288330078125, best_valid_rmse = 9.177753983792192\n",
      "Epoch 12, Step 5016: Loss = 1053.58740234375, best_valid_rmse = 9.177753983792192\n",
      "Epoch 13, Step 5434: Loss = 1123.3240966796875, best_valid_rmse = 9.177753983792192\n",
      "Epoch 14, Step 5852: Loss = 1100.345703125, best_valid_rmse = 9.15817928089667\n",
      "Epoch 15, Step 6270: Loss = 1099.9569091796875, best_valid_rmse = 9.15817928089667\n",
      "Epoch 16, Step 6688: Loss = 1070.3546142578125, best_valid_rmse = 9.15817928089667\n",
      "Epoch 17, Step 7106: Loss = 1078.498046875, best_valid_rmse = 9.153711528908925\n",
      "Epoch 18, Step 7524: Loss = 1100.83740234375, best_valid_rmse = 9.153711528908925\n",
      "Epoch 19, Step 7942: Loss = 1070.348388671875, best_valid_rmse = 9.153711528908925\n",
      "Epoch 20, Step 8360: Loss = 1077.673583984375, best_valid_rmse = 9.147952788416786\n",
      "Epoch 21, Step 8778: Loss = 1076.98974609375, best_valid_rmse = 9.147952788416786\n",
      "Epoch 22, Step 9196: Loss = 1074.9222412109375, best_valid_rmse = 9.147952788416786\n",
      "Epoch 23, Step 9614: Loss = 1079.192626953125, best_valid_rmse = 9.147952788416786\n",
      "Epoch 24, Step 10032: Loss = 1092.3267822265625, best_valid_rmse = 9.147952788416786\n",
      "Epoch 25, Step 10450: Loss = 1050.5709228515625, best_valid_rmse = 9.147952788416786\n",
      "Epoch 26, Step 10868: Loss = 1097.7552490234375, best_valid_rmse = 9.147952788416786\n",
      "Epoch 27, Step 11286: Loss = 1130.77685546875, best_valid_rmse = 9.147952788416786\n",
      "Epoch 28, Step 11704: Loss = 1071.80908203125, best_valid_rmse = 9.147952788416786\n",
      "Epoch 29, Step 12122: Loss = 1077.331298828125, best_valid_rmse = 9.147952788416786\n",
      "Epoch 30, Step 12540: Loss = 1064.26123046875, best_valid_rmse = 9.147952788416786\n",
      "Epoch 31, Step 12958: Loss = 1083.3837890625, best_valid_rmse = 9.147952788416786\n",
      "early stopping, step 13200, validation RMSE = 9.176203327692777\n"
     ]
    }
   ],
   "source": [
    "d_hidden = 90\n",
    "d_input = subtrain_set.Xnp.shape[1]\n",
    "d_output = 1\n",
    "\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d_input, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_output)\n",
    ")\n",
    "mlp = mlp.float()\n",
    "mlp.to(device)\n",
    "weight_path = './MLP90_l2_cus_weight'\n",
    "\n",
    "# optimizer\n",
    "lr = 0.001\n",
    "momentum = 0\n",
    "weight_decay = 0\n",
    "adam_optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# loss\n",
    "z = 0.0\n",
    "l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "cus_loss = QLoss()\n",
    "\n",
    "# train\n",
    "all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=adam_optimizer, loss_f1=l2_loss, loss_f2=cus_loss, loss_w=z, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'H=90 with Drop Out, Training & Validation RMSE')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot subtrain / validation RMSE\n",
    "plt.plot(all_step, all_train_rmse, c='b', label='Training RMSE')\n",
    "plt.plot(all_step, all_valid_rmse, c='r', label='Validation RMSE')\n",
    "plt.legend()\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('H=90 with Drop Out, Training & Validation RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 9.261542886662433\n"
     ]
    }
   ],
   "source": [
    "# test RMSE\n",
    "saved_mlp = torch.load(weight_path)\n",
    "test_rmse = RMSE(saved_mlp, test_loader)\n",
    "print(f'Test RMSE = {test_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 4808.353515625, best_valid_rmse = 9.2045358488281\n",
      "Epoch 2, Step 836: Loss = 4757.80126953125, best_valid_rmse = 9.068606086662355\n",
      "Epoch 3, Step 1254: Loss = 4768.10791015625, best_valid_rmse = 8.99842230725774\n",
      "Epoch 4, Step 1672: Loss = 4574.27978515625, best_valid_rmse = 8.95501608505581\n",
      "Epoch 5, Step 2090: Loss = 4481.84912109375, best_valid_rmse = 8.943940382702856\n",
      "Epoch 6, Step 2508: Loss = 4529.0537109375, best_valid_rmse = 8.91652515649958\n",
      "Epoch 7, Step 2926: Loss = 4516.794921875, best_valid_rmse = 8.912463157038324\n",
      "Epoch 8, Step 3344: Loss = 4510.6689453125, best_valid_rmse = 8.894702942831989\n",
      "Epoch 9, Step 3762: Loss = 4518.48681640625, best_valid_rmse = 8.890495441140091\n",
      "Epoch 10, Step 4180: Loss = 4739.11669921875, best_valid_rmse = 8.879081813552615\n",
      "Epoch 11, Step 4598: Loss = 4527.60595703125, best_valid_rmse = 8.861743236868163\n",
      "Epoch 12, Step 5016: Loss = 4281.25390625, best_valid_rmse = 8.861340480660234\n",
      "Epoch 13, Step 5434: Loss = 4559.3330078125, best_valid_rmse = 8.847027099986958\n",
      "Epoch 14, Step 5852: Loss = 4497.4501953125, best_valid_rmse = 8.847027099986958\n",
      "Epoch 15, Step 6270: Loss = 4387.22900390625, best_valid_rmse = 8.847027099986958\n",
      "Epoch 16, Step 6688: Loss = 4369.826171875, best_valid_rmse = 8.842479263215148\n",
      "Epoch 17, Step 7106: Loss = 4529.89208984375, best_valid_rmse = 8.842479263215148\n",
      "Epoch 18, Step 7524: Loss = 4406.20947265625, best_valid_rmse = 8.83898588014745\n",
      "Epoch 19, Step 7942: Loss = 4476.06005859375, best_valid_rmse = 8.83898588014745\n",
      "Epoch 20, Step 8360: Loss = 4498.0, best_valid_rmse = 8.83898588014745\n",
      "Epoch 21, Step 8778: Loss = 4585.78466796875, best_valid_rmse = 8.838539388686995\n",
      "Epoch 22, Step 9196: Loss = 4298.7275390625, best_valid_rmse = 8.838539388686995\n",
      "Epoch 23, Step 9614: Loss = 4429.353515625, best_valid_rmse = 8.838539388686995\n",
      "Epoch 24, Step 10032: Loss = 4432.169921875, best_valid_rmse = 8.838539388686995\n",
      "Epoch 25, Step 10450: Loss = 4458.234375, best_valid_rmse = 8.838539388686995\n",
      "Epoch 26, Step 10868: Loss = 4290.5556640625, best_valid_rmse = 8.829616724671812\n",
      "Epoch 27, Step 11286: Loss = 4308.564453125, best_valid_rmse = 8.829616724671812\n",
      "Epoch 28, Step 11704: Loss = 4487.14501953125, best_valid_rmse = 8.829616724671812\n",
      "Epoch 29, Step 12122: Loss = 4401.6181640625, best_valid_rmse = 8.829616724671812\n",
      "Epoch 30, Step 12540: Loss = 4455.84326171875, best_valid_rmse = 8.829616724671812\n",
      "Epoch 31, Step 12958: Loss = 4534.220703125, best_valid_rmse = 8.829616724671812\n",
      "Epoch 32, Step 13376: Loss = 4167.47119140625, best_valid_rmse = 8.82771879819985\n",
      "Epoch 33, Step 13794: Loss = 4168.791015625, best_valid_rmse = 8.82771879819985\n",
      "Epoch 34, Step 14212: Loss = 4444.15283203125, best_valid_rmse = 8.82484884450593\n",
      "Epoch 35, Step 14630: Loss = 4451.52978515625, best_valid_rmse = 8.82484884450593\n",
      "Epoch 36, Step 15048: Loss = 4252.578125, best_valid_rmse = 8.82484884450593\n",
      "Epoch 37, Step 15466: Loss = 4182.07275390625, best_valid_rmse = 8.811227585096226\n",
      "Epoch 38, Step 15884: Loss = 4478.671875, best_valid_rmse = 8.811227585096226\n",
      "Epoch 39, Step 16302: Loss = 4108.0908203125, best_valid_rmse = 8.811227585096226\n",
      "Epoch 40, Step 16720: Loss = 4398.29638671875, best_valid_rmse = 8.811227585096226\n",
      "Epoch 41, Step 17138: Loss = 4392.900390625, best_valid_rmse = 8.811227585096226\n",
      "Epoch 42, Step 17556: Loss = 4273.04296875, best_valid_rmse = 8.811227585096226\n",
      "Epoch 43, Step 17974: Loss = 4571.9326171875, best_valid_rmse = 8.811227585096226\n",
      "Epoch 44, Step 18392: Loss = 4723.23828125, best_valid_rmse = 8.811227585096226\n",
      "Epoch 45, Step 18810: Loss = 4320.18701171875, best_valid_rmse = 8.811227585096226\n",
      "Epoch 46, Step 19228: Loss = 4499.5869140625, best_valid_rmse = 8.811227585096226\n",
      "Epoch 47, Step 19646: Loss = 4368.40087890625, best_valid_rmse = 8.811227585096226\n",
      "Epoch 48, Step 20064: Loss = 4296.2177734375, best_valid_rmse = 8.811227585096226\n",
      "early stopping, step 20400, validation RMSE = 8.832029763074074\n",
      "z = 0.1: Test RMSE = 9.050065194847479\n",
      "\n",
      "Epoch 1, Step 418: Loss = 19289.7265625, best_valid_rmse = 9.195648496035643\n",
      "Epoch 2, Step 836: Loss = 18966.166015625, best_valid_rmse = 9.053329689162759\n",
      "Epoch 3, Step 1254: Loss = 19281.13671875, best_valid_rmse = 8.985050986528147\n",
      "Epoch 4, Step 1672: Loss = 19081.65625, best_valid_rmse = 8.96896027352118\n",
      "Epoch 5, Step 2090: Loss = 18787.564453125, best_valid_rmse = 8.916664827577955\n",
      "Epoch 6, Step 2508: Loss = 17909.369140625, best_valid_rmse = 8.910897494286063\n",
      "Epoch 7, Step 2926: Loss = 19231.935546875, best_valid_rmse = 8.903564317109138\n",
      "Epoch 8, Step 3344: Loss = 18690.734375, best_valid_rmse = 8.874537038461261\n",
      "Epoch 9, Step 3762: Loss = 18177.93359375, best_valid_rmse = 8.874537038461261\n",
      "Epoch 10, Step 4180: Loss = 18337.310546875, best_valid_rmse = 8.868156883387782\n",
      "Epoch 11, Step 4598: Loss = 18280.970703125, best_valid_rmse = 8.868156883387782\n",
      "Epoch 12, Step 5016: Loss = 17518.162109375, best_valid_rmse = 8.857195347634033\n",
      "Epoch 13, Step 5434: Loss = 17510.421875, best_valid_rmse = 8.857195347634033\n",
      "Epoch 14, Step 5852: Loss = 17466.451171875, best_valid_rmse = 8.852149378470111\n",
      "Epoch 15, Step 6270: Loss = 18280.541015625, best_valid_rmse = 8.840115122302311\n",
      "Epoch 16, Step 6688: Loss = 18010.451171875, best_valid_rmse = 8.837464849908427\n",
      "Epoch 17, Step 7106: Loss = 18825.72265625, best_valid_rmse = 8.837464849908427\n",
      "Epoch 18, Step 7524: Loss = 17475.90234375, best_valid_rmse = 8.837464849908427\n",
      "Epoch 19, Step 7942: Loss = 18804.060546875, best_valid_rmse = 8.833839185601468\n",
      "Epoch 20, Step 8360: Loss = 18413.169921875, best_valid_rmse = 8.833839185601468\n",
      "Epoch 21, Step 8778: Loss = 18136.796875, best_valid_rmse = 8.833839185601468\n",
      "Epoch 22, Step 9196: Loss = 16698.216796875, best_valid_rmse = 8.819393451225116\n",
      "Epoch 23, Step 9614: Loss = 16938.689453125, best_valid_rmse = 8.819393451225116\n",
      "Epoch 24, Step 10032: Loss = 18211.734375, best_valid_rmse = 8.819393451225116\n",
      "Epoch 25, Step 10450: Loss = 17576.693359375, best_valid_rmse = 8.819393451225116\n",
      "Epoch 26, Step 10868: Loss = 16905.197265625, best_valid_rmse = 8.819393451225116\n",
      "Epoch 27, Step 11286: Loss = 17094.08203125, best_valid_rmse = 8.819381530855162\n",
      "Epoch 28, Step 11704: Loss = 17904.64453125, best_valid_rmse = 8.819381530855162\n",
      "Epoch 29, Step 12122: Loss = 17434.310546875, best_valid_rmse = 8.819381530855162\n",
      "Epoch 30, Step 12540: Loss = 17525.560546875, best_valid_rmse = 8.819381530855162\n",
      "Epoch 31, Step 12958: Loss = 16175.4775390625, best_valid_rmse = 8.819381530855162\n",
      "Epoch 32, Step 13376: Loss = 16929.298828125, best_valid_rmse = 8.819381530855162\n",
      "Epoch 33, Step 13794: Loss = 17439.931640625, best_valid_rmse = 8.819381530855162\n",
      "Epoch 34, Step 14212: Loss = 17038.59375, best_valid_rmse = 8.819381530855162\n",
      "Epoch 35, Step 14630: Loss = 18352.623046875, best_valid_rmse = 8.819381530855162\n",
      "Epoch 36, Step 15048: Loss = 16198.37109375, best_valid_rmse = 8.818435185458911\n",
      "Epoch 37, Step 15466: Loss = 17620.298828125, best_valid_rmse = 8.818435185458911\n",
      "Epoch 38, Step 15884: Loss = 18336.474609375, best_valid_rmse = 8.818435185458911\n",
      "Epoch 39, Step 16302: Loss = 17563.84765625, best_valid_rmse = 8.816495104436282\n",
      "Epoch 40, Step 16720: Loss = 17530.61328125, best_valid_rmse = 8.810248237317069\n",
      "Epoch 41, Step 17138: Loss = 17945.54296875, best_valid_rmse = 8.810248237317069\n",
      "Epoch 42, Step 17556: Loss = 16623.56640625, best_valid_rmse = 8.810248237317069\n",
      "Epoch 43, Step 17974: Loss = 17005.2578125, best_valid_rmse = 8.810248237317069\n",
      "Epoch 44, Step 18392: Loss = 16882.41796875, best_valid_rmse = 8.810248237317069\n",
      "Epoch 45, Step 18810: Loss = 17835.744140625, best_valid_rmse = 8.810248237317069\n",
      "Epoch 46, Step 19228: Loss = 16042.4892578125, best_valid_rmse = 8.810248237317069\n",
      "Epoch 47, Step 19646: Loss = 17014.5625, best_valid_rmse = 8.810248237317069\n",
      "Epoch 48, Step 20064: Loss = 17984.09375, best_valid_rmse = 8.80372351044231\n",
      "Epoch 49, Step 20482: Loss = 17001.201171875, best_valid_rmse = 8.80372351044231\n",
      "Epoch 50, Step 20900: Loss = 17642.0703125, best_valid_rmse = 8.80372351044231\n",
      "Epoch 51, Step 21318: Loss = 17810.35546875, best_valid_rmse = 8.80372351044231\n",
      "Epoch 52, Step 21736: Loss = 17118.88671875, best_valid_rmse = 8.80372351044231\n",
      "Epoch 53, Step 22154: Loss = 17033.798828125, best_valid_rmse = 8.80372351044231\n",
      "Epoch 54, Step 22572: Loss = 16117.60546875, best_valid_rmse = 8.80372351044231\n",
      "Epoch 55, Step 22990: Loss = 17814.564453125, best_valid_rmse = 8.80372351044231\n",
      "Epoch 56, Step 23408: Loss = 16733.919921875, best_valid_rmse = 8.80372351044231\n",
      "Epoch 57, Step 23826: Loss = 18329.80859375, best_valid_rmse = 8.80372351044231\n",
      "Epoch 58, Step 24244: Loss = 17255.12109375, best_valid_rmse = 8.80372351044231\n",
      "Epoch 59, Step 24662: Loss = 17659.630859375, best_valid_rmse = 8.80372351044231\n",
      "early stopping, step 24900, validation RMSE = 8.823003970534652\n",
      "z = 0.5: Test RMSE = 9.020450971821628\n",
      "\n",
      "Epoch 1, Step 418: Loss = 32199.435546875, best_valid_rmse = 9.175041999898346\n",
      "Epoch 2, Step 836: Loss = 30931.650390625, best_valid_rmse = 9.050265000884142\n",
      "Epoch 3, Step 1254: Loss = 32644.9609375, best_valid_rmse = 9.004676936127698\n",
      "Epoch 4, Step 1672: Loss = 32952.04296875, best_valid_rmse = 8.960052259842296\n",
      "Epoch 5, Step 2090: Loss = 32293.671875, best_valid_rmse = 8.942440821658876\n",
      "Epoch 6, Step 2508: Loss = 31199.939453125, best_valid_rmse = 8.90761312505709\n",
      "Epoch 7, Step 2926: Loss = 29908.98828125, best_valid_rmse = 8.894351686538581\n",
      "Epoch 8, Step 3344: Loss = 32314.0859375, best_valid_rmse = 8.88206353262172\n",
      "Epoch 9, Step 3762: Loss = 32883.35546875, best_valid_rmse = 8.88088589900866\n",
      "Epoch 10, Step 4180: Loss = 31540.900390625, best_valid_rmse = 8.861809854144324\n",
      "Epoch 11, Step 4598: Loss = 30677.068359375, best_valid_rmse = 8.861809854144324\n",
      "Epoch 12, Step 5016: Loss = 31748.763671875, best_valid_rmse = 8.858685907993117\n",
      "Epoch 13, Step 5434: Loss = 31981.71875, best_valid_rmse = 8.858685907993117\n",
      "Epoch 14, Step 5852: Loss = 31418.892578125, best_valid_rmse = 8.858685907993117\n",
      "Epoch 15, Step 6270: Loss = 31468.45703125, best_valid_rmse = 8.858685907993117\n",
      "Epoch 16, Step 6688: Loss = 30711.466796875, best_valid_rmse = 8.851968796685545\n",
      "Epoch 17, Step 7106: Loss = 30181.0, best_valid_rmse = 8.83419956097175\n",
      "Epoch 18, Step 7524: Loss = 30900.23828125, best_valid_rmse = 8.83419956097175\n",
      "Epoch 19, Step 7942: Loss = 31393.333984375, best_valid_rmse = 8.83419956097175\n",
      "Epoch 20, Step 8360: Loss = 29734.064453125, best_valid_rmse = 8.83419956097175\n",
      "Epoch 21, Step 8778: Loss = 30693.11328125, best_valid_rmse = 8.83419956097175\n",
      "Epoch 22, Step 9196: Loss = 31265.9765625, best_valid_rmse = 8.83419956097175\n",
      "Epoch 23, Step 9614: Loss = 29199.451171875, best_valid_rmse = 8.83419956097175\n",
      "Epoch 24, Step 10032: Loss = 31066.375, best_valid_rmse = 8.826909857068248\n",
      "Epoch 25, Step 10450: Loss = 31222.4296875, best_valid_rmse = 8.826288671882061\n",
      "Epoch 26, Step 10868: Loss = 30532.134765625, best_valid_rmse = 8.824292887324766\n",
      "Epoch 27, Step 11286: Loss = 28511.8828125, best_valid_rmse = 8.824292887324766\n",
      "Epoch 28, Step 11704: Loss = 32282.02734375, best_valid_rmse = 8.824292887324766\n",
      "Epoch 29, Step 12122: Loss = 31579.478515625, best_valid_rmse = 8.824292887324766\n",
      "Epoch 30, Step 12540: Loss = 31155.0390625, best_valid_rmse = 8.815756073814946\n",
      "Epoch 31, Step 12958: Loss = 31043.185546875, best_valid_rmse = 8.814228282454495\n",
      "Epoch 32, Step 13376: Loss = 30525.845703125, best_valid_rmse = 8.814228282454495\n",
      "Epoch 33, Step 13794: Loss = 30669.4609375, best_valid_rmse = 8.814228282454495\n",
      "Epoch 34, Step 14212: Loss = 32122.208984375, best_valid_rmse = 8.814228282454495\n",
      "Epoch 35, Step 14630: Loss = 29822.455078125, best_valid_rmse = 8.814228282454495\n",
      "Epoch 36, Step 15048: Loss = 29586.681640625, best_valid_rmse = 8.814228282454495\n",
      "Epoch 37, Step 15466: Loss = 30887.537109375, best_valid_rmse = 8.814228282454495\n",
      "Epoch 38, Step 15884: Loss = 28764.71484375, best_valid_rmse = 8.811070333838767\n",
      "Epoch 39, Step 16302: Loss = 30746.0, best_valid_rmse = 8.808924828153827\n",
      "Epoch 40, Step 16720: Loss = 32485.865234375, best_valid_rmse = 8.808924828153827\n",
      "Epoch 41, Step 17138: Loss = 28987.783203125, best_valid_rmse = 8.808924828153827\n",
      "Epoch 42, Step 17556: Loss = 29470.759765625, best_valid_rmse = 8.808924828153827\n",
      "Epoch 43, Step 17974: Loss = 30296.380859375, best_valid_rmse = 8.808924828153827\n",
      "Epoch 44, Step 18392: Loss = 31364.96484375, best_valid_rmse = 8.808924828153827\n",
      "Epoch 45, Step 18810: Loss = 29894.498046875, best_valid_rmse = 8.808924828153827\n",
      "Epoch 46, Step 19228: Loss = 30965.185546875, best_valid_rmse = 8.808924828153827\n",
      "Epoch 47, Step 19646: Loss = 32640.46484375, best_valid_rmse = 8.808924828153827\n",
      "Epoch 48, Step 20064: Loss = 30070.591796875, best_valid_rmse = 8.807274036422553\n",
      "Epoch 49, Step 20482: Loss = 31667.3046875, best_valid_rmse = 8.807274036422553\n",
      "Epoch 50, Step 20900: Loss = 29134.19921875, best_valid_rmse = 8.80458754846588\n",
      "Epoch 51, Step 21318: Loss = 30989.095703125, best_valid_rmse = 8.78488219518033\n",
      "Epoch 52, Step 21736: Loss = 31121.349609375, best_valid_rmse = 8.78488219518033\n",
      "Epoch 53, Step 22154: Loss = 33082.71484375, best_valid_rmse = 8.78488219518033\n",
      "Epoch 54, Step 22572: Loss = 29477.982421875, best_valid_rmse = 8.78488219518033\n",
      "Epoch 55, Step 22990: Loss = 31354.4375, best_valid_rmse = 8.78488219518033\n",
      "Epoch 56, Step 23408: Loss = 30891.998046875, best_valid_rmse = 8.78488219518033\n",
      "Epoch 57, Step 23826: Loss = 30767.353515625, best_valid_rmse = 8.78488219518033\n",
      "Epoch 58, Step 24244: Loss = 31030.29296875, best_valid_rmse = 8.78488219518033\n",
      "Epoch 59, Step 24662: Loss = 29321.607421875, best_valid_rmse = 8.78488219518033\n",
      "Epoch 60, Step 25080: Loss = 31398.220703125, best_valid_rmse = 8.78488219518033\n",
      "Epoch 61, Step 25498: Loss = 29888.314453125, best_valid_rmse = 8.78488219518033\n",
      "Epoch 62, Step 25916: Loss = 29038.65234375, best_valid_rmse = 8.78488219518033\n",
      "early stopping, step 26300, validation RMSE = 8.845334353945324\n",
      "z = 0.9: Test RMSE = 9.006283450144693\n",
      "\n",
      "Epoch 1, Step 418: Loss = 36752.51953125, best_valid_rmse = 9.207267837254163\n",
      "Epoch 2, Step 836: Loss = 36364.0, best_valid_rmse = 9.049523316050466\n",
      "Epoch 3, Step 1254: Loss = 34931.37890625, best_valid_rmse = 8.975511745233643\n",
      "Epoch 4, Step 1672: Loss = 34779.15625, best_valid_rmse = 8.943425586872326\n",
      "Epoch 5, Step 2090: Loss = 35431.55078125, best_valid_rmse = 8.932779730192419\n",
      "Epoch 6, Step 2508: Loss = 35222.84765625, best_valid_rmse = 8.926596120957882\n",
      "Epoch 7, Step 2926: Loss = 34896.484375, best_valid_rmse = 8.911716355696623\n",
      "Epoch 8, Step 3344: Loss = 35559.0625, best_valid_rmse = 8.900689441893128\n",
      "Epoch 9, Step 3762: Loss = 36196.0390625, best_valid_rmse = 8.883393645901707\n",
      "Epoch 10, Step 4180: Loss = 33984.1875, best_valid_rmse = 8.86848698825379\n",
      "Epoch 11, Step 4598: Loss = 36955.87890625, best_valid_rmse = 8.865339549376683\n",
      "Epoch 12, Step 5016: Loss = 34887.36328125, best_valid_rmse = 8.862135632789043\n",
      "Epoch 13, Step 5434: Loss = 35774.4921875, best_valid_rmse = 8.862135632789043\n",
      "Epoch 14, Step 5852: Loss = 33091.453125, best_valid_rmse = 8.862131678498208\n",
      "Epoch 15, Step 6270: Loss = 35661.9609375, best_valid_rmse = 8.857067521471148\n",
      "Epoch 16, Step 6688: Loss = 32928.7265625, best_valid_rmse = 8.85158356353477\n",
      "Epoch 17, Step 7106: Loss = 34110.15625, best_valid_rmse = 8.85158356353477\n",
      "Epoch 18, Step 7524: Loss = 34830.5390625, best_valid_rmse = 8.834454652686427\n",
      "Epoch 19, Step 7942: Loss = 33396.125, best_valid_rmse = 8.834454652686427\n",
      "Epoch 20, Step 8360: Loss = 35575.66015625, best_valid_rmse = 8.834454652686427\n",
      "Epoch 21, Step 8778: Loss = 35833.109375, best_valid_rmse = 8.82595271280877\n",
      "Epoch 22, Step 9196: Loss = 32830.390625, best_valid_rmse = 8.82595271280877\n",
      "Epoch 23, Step 9614: Loss = 32182.01953125, best_valid_rmse = 8.82595271280877\n",
      "Epoch 24, Step 10032: Loss = 33716.44921875, best_valid_rmse = 8.82595271280877\n",
      "Epoch 25, Step 10450: Loss = 35135.671875, best_valid_rmse = 8.82595271280877\n",
      "Epoch 26, Step 10868: Loss = 33843.953125, best_valid_rmse = 8.82595271280877\n",
      "Epoch 27, Step 11286: Loss = 34228.74609375, best_valid_rmse = 8.82595271280877\n",
      "Epoch 28, Step 11704: Loss = 34909.296875, best_valid_rmse = 8.82595271280877\n",
      "Epoch 29, Step 12122: Loss = 33057.80078125, best_valid_rmse = 8.82595271280877\n",
      "Epoch 30, Step 12540: Loss = 34987.87109375, best_valid_rmse = 8.82595271280877\n",
      "Epoch 31, Step 12958: Loss = 34468.0078125, best_valid_rmse = 8.822351344510954\n",
      "Epoch 32, Step 13376: Loss = 35133.34375, best_valid_rmse = 8.818043596176793\n",
      "Epoch 33, Step 13794: Loss = 34953.04296875, best_valid_rmse = 8.818043596176793\n",
      "Epoch 34, Step 14212: Loss = 32020.73046875, best_valid_rmse = 8.818043596176793\n",
      "Epoch 35, Step 14630: Loss = 35487.65625, best_valid_rmse = 8.815094654614494\n",
      "Epoch 36, Step 15048: Loss = 34268.34765625, best_valid_rmse = 8.815094654614494\n",
      "Epoch 37, Step 15466: Loss = 34391.0234375, best_valid_rmse = 8.815094654614494\n",
      "Epoch 38, Step 15884: Loss = 34117.4765625, best_valid_rmse = 8.815094654614494\n",
      "Epoch 39, Step 16302: Loss = 33262.921875, best_valid_rmse = 8.815094654614494\n",
      "Epoch 40, Step 16720: Loss = 35155.421875, best_valid_rmse = 8.815094654614494\n",
      "Epoch 41, Step 17138: Loss = 34680.828125, best_valid_rmse = 8.815094654614494\n",
      "Epoch 42, Step 17556: Loss = 30968.515625, best_valid_rmse = 8.815094654614494\n",
      "Epoch 43, Step 17974: Loss = 34212.5078125, best_valid_rmse = 8.815094654614494\n",
      "Epoch 44, Step 18392: Loss = 33316.0703125, best_valid_rmse = 8.815094654614494\n",
      "Epoch 45, Step 18810: Loss = 34163.3515625, best_valid_rmse = 8.812333159652981\n",
      "Epoch 46, Step 19228: Loss = 32887.1640625, best_valid_rmse = 8.812333159652981\n",
      "Epoch 47, Step 19646: Loss = 32614.150390625, best_valid_rmse = 8.812333159652981\n",
      "Epoch 48, Step 20064: Loss = 33533.3125, best_valid_rmse = 8.812333159652981\n",
      "Epoch 49, Step 20482: Loss = 33595.36328125, best_valid_rmse = 8.812333159652981\n",
      "Epoch 50, Step 20900: Loss = 33784.125, best_valid_rmse = 8.812333159652981\n",
      "Epoch 51, Step 21318: Loss = 34803.9453125, best_valid_rmse = 8.812333159652981\n",
      "Epoch 52, Step 21736: Loss = 35191.1328125, best_valid_rmse = 8.812333159652981\n",
      "Epoch 53, Step 22154: Loss = 33677.625, best_valid_rmse = 8.812333159652981\n",
      "Epoch 54, Step 22572: Loss = 34947.546875, best_valid_rmse = 8.812333159652981\n",
      "Epoch 55, Step 22990: Loss = 34548.42578125, best_valid_rmse = 8.812333159652981\n",
      "early stopping, step 23400, validation RMSE = 8.837760719489209\n",
      "z = 1.0: Test RMSE = 9.015639455619583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for z in [0.1, 0.5, 0.9, 1.0]:\n",
    "    \n",
    "    d_hidden = 90\n",
    "    d_input = subtrain_set.Xnp.shape[1]\n",
    "    d_output = 1\n",
    "\n",
    "    mlp = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d_input, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_output)\n",
    "    )\n",
    "    mlp = mlp.float()\n",
    "    mlp.to(device)\n",
    "    weight_path = f'./MLP90_l1_l2_{z*10}_weight'\n",
    "\n",
    "    # optimizer\n",
    "    lr = 0.001\n",
    "    momentum = 0\n",
    "    weight_decay = 0\n",
    "    adam_optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # loss\n",
    "    l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "    cus_loss = QLoss()\n",
    "\n",
    "    # train\n",
    "    all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=adam_optimizer, loss_f1=l2_loss, loss_f2=cus_loss, loss_w=z, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=False)        \n",
    "    \n",
    "    # test RMSE\n",
    "    saved_mlp = torch.load(weight_path)\n",
    "    test_rmse = RMSE(saved_mlp, test_loader)\n",
    "    print(f'z = {z}: Test RMSE = {test_rmse}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(z = 0.0: Test RMSE = 9.262)  \n",
    "z = 0.1: Test RMSE = 9.050  \n",
    "z = 0.5: Test RMSE = 9.020  \n",
    "z = 0.9: Test RMSE = 9.006  \n",
    "z = 1.0: Test RMSE = 9.016  \n",
    "\n",
    "From the results above, we can see that the lowest test RMSE appears at z = 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
