{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning and Deep Learning HW4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (463715, 90)\n",
      "X_subtrain shape =  (417344, 90)\n",
      "X_valid shape =  (46371, 90)\n",
      "\n",
      "Y_train shape =  (463715,)\n",
      "Y_subtrain shape =  (417344,)\n",
      "Y_valid shape =  (46371,)\n",
      "\n",
      "X_test shape =  (51630, 90)\n",
      "Y_test shape =  (51630,)\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "# load data\n",
    "with open('msd_full.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "doscaling = 1\n",
    "if doscaling == 1:\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    # standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test'].astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_mean = Y_train.mean()\n",
    "Y_train_keep = Y_train.copy()\n",
    "Y_test_keep = Y_test.copy()\n",
    "Y_train = Y_train - y_mean\n",
    "Y_test = Y_test - y_mean\n",
    "\n",
    "\n",
    "# validation is the last 10% of training, subtraining is the first 90% of training\n",
    "nvalid = int(X_train.shape[0] * 0.1)\n",
    "nsubtrain = X_train.shape[0] - nvalid\n",
    "\n",
    "X_subtrain = X_train[0:nsubtrain, :].astype('float32')\n",
    "X_valid = X_train[nsubtrain:, :].astype('float32')\n",
    "Y_subtrain = Y_train[0:nsubtrain].astype('float32')\n",
    "Y_valid = Y_train[nsubtrain:].astype('float32')\n",
    "\n",
    "Y_subtrain_keep = Y_train_keep[0:nsubtrain].astype('float32')\n",
    "Y_valid_keep = Y_train_keep[nsubtrain:].astype('float32')\n",
    "\n",
    "print(\"X_train shape = \", X_train.shape)\n",
    "print(\"X_subtrain shape = \", X_subtrain.shape)\n",
    "print(\"X_valid shape = \", X_valid.shape)\n",
    "print()\n",
    "print(\"Y_train shape = \", Y_train.shape)\n",
    "print(\"Y_subtrain shape = \", Y_subtrain.shape)\n",
    "print(\"Y_valid shape = \", Y_valid.shape)\n",
    "print()\n",
    "print(\"X_test shape = \", X_test.shape)\n",
    "print(\"Y_test shape = \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Oridinary Least Square (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "ols = sm.OLS(Y_train, X_train, hasconst=True)\n",
    "ols_result = ols.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 parameters: [ 5.30975265 -2.88088114 -1.53234348  0.05737583 -0.33952889]\n"
     ]
    }
   ],
   "source": [
    "print(f'The first 5 parameters: {ols_result.params[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted Y is [-5.81070695  0.03250657  5.13960445 ... -1.39829429 -0.26047668\n",
      "  0.05193056]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "Y_predict = ols_result.predict(X_test)\n",
    "print(f'The predicted Y is {Y_predict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 9.510160684544402\n"
     ]
    }
   ],
   "source": [
    "# RMSE\n",
    "print(f'RMSE = {mean_squared_error(Y_test, Y_predict, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. MLP with Four Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.7.1rc2\n",
      "using cuda\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('torch version:', torch.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'using {device}')\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "class Dataset(data.Dataset):\n",
    "    \n",
    "  def __init__(self, Xnp, Ynp):\n",
    "        self.labels = Ynp\n",
    "        self.nobs = Xnp.shape[0]        \n",
    "        self.Xnp = Xnp\n",
    "        self.Ynp = Ynp\n",
    "        \n",
    "  def __len__(self):\n",
    "        return self.nobs\n",
    "    \n",
    "  def __getitem__(self, index):     \n",
    "        X = self.Xnp[index]\n",
    "        y = self.Ynp[index]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtrain length 417344\n",
      "valid length 46371\n",
      "test length 51630\n"
     ]
    }
   ],
   "source": [
    "# create dataloader\n",
    "subtrain_set = Dataset(X_subtrain, Y_subtrain)    \n",
    "valid_set = Dataset(X_valid, Y_valid)\n",
    "test_set = Dataset(X_test, Y_test)\n",
    "print('subtrain length', len(subtrain_set))\n",
    "print('valid length', len(valid_set))\n",
    "print('test length', len(test_set))\n",
    "\n",
    "batch_size = 1000\n",
    "subtrain_loader = data.DataLoader(subtrain_set, batch_size=batch_size)\n",
    "valid_loader = data.DataLoader(valid_set, batch_size=batch_size)\n",
    "test_loader = data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "def RMSE(model, data_loader):\n",
    "    n_obs = 0\n",
    "    sse = 0\n",
    "    with torch.no_grad():\n",
    "        for _batch, (_inputs, _targets) in enumerate(data_loader):\n",
    "            _targets = _targets.reshape((-1, 1))\n",
    "            _inputs, _targets = _inputs.to(device), _targets.to(device)\n",
    "            _outputs = model(_inputs)\n",
    "            n_obs += _targets.shape[0]\n",
    "            sse += (_targets - _outputs).pow(2).sum(0)\n",
    "    return np.sqrt(sse.cpu().numpy()[0] / n_obs)\n",
    "\n",
    "    \n",
    "def train(model, optim, loss_f, max_epoch, max_step, valid_interval, weight_path, train_rmse):\n",
    "    \n",
    "    step_count = 0\n",
    "    best_step_count = 0\n",
    "    best_valid_rmse = np.inf\n",
    "    all_train_rmse = []\n",
    "    all_valid_rmse = []\n",
    "    all_step = []\n",
    "\n",
    "    for epoch in range(1, max_epoch+1):\n",
    "        for batch, (inputs, targets) in enumerate(subtrain_loader):\n",
    "\n",
    "            targets = targets.reshape((-1, 1))\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            model.to(device)\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_f(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            step_count += 1\n",
    "\n",
    "            # check train/validation RMSE\n",
    "            if (step_count % valid_interval == 0):\n",
    "\n",
    "                # subtrain, validation RMSE\n",
    "                if train_rmse:\n",
    "                    train_rmse = RMSE(model, subtrain_loader)\n",
    "                    all_train_rmse.append(train_rmse)\n",
    "                valid_rmse = RMSE(model, valid_loader)\n",
    "                all_valid_rmse.append(valid_rmse)\n",
    "                all_step.append(step_count)\n",
    "\n",
    "                # update weight\n",
    "                if valid_rmse < best_valid_rmse:\n",
    "                    best_step_count = step_count\n",
    "                    best_valid_rmse = valid_rmse\n",
    "                    torch.save(model, weight_path)\n",
    "\n",
    "                # early stopping\n",
    "                elif (step_count - best_step_count >= max_step):\n",
    "                    print(f'early stopping, step {step_count}, validation RMSE = {valid_rmse}')\n",
    "                    return all_step, all_train_rmse, all_valid_rmse\n",
    "                \n",
    "        print(f'Epoch {epoch}, Step {step_count}: Loss = {loss.item()}, best_valid_rmse = {best_valid_rmse}')\n",
    "    return all_step, all_train_rmse, all_valid_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 34811.2265625, best_valid_rmse = 8.94553009496888\n",
      "Epoch 2, Step 836: Loss = 34226.890625, best_valid_rmse = 8.71017680179246\n",
      "Epoch 3, Step 1254: Loss = 34025.21875, best_valid_rmse = 8.688642361852033\n",
      "Epoch 4, Step 1672: Loss = 33730.0390625, best_valid_rmse = 8.613268462556709\n",
      "Epoch 5, Step 2090: Loss = 33249.12890625, best_valid_rmse = 8.603137778883292\n",
      "Epoch 6, Step 2508: Loss = 33170.2578125, best_valid_rmse = 8.603137778883292\n",
      "Epoch 7, Step 2926: Loss = 33348.11328125, best_valid_rmse = 8.603116472186596\n",
      "Epoch 8, Step 3344: Loss = 33474.8828125, best_valid_rmse = 8.597652382522488\n",
      "Epoch 9, Step 3762: Loss = 33425.3046875, best_valid_rmse = 8.596214084982677\n",
      "Epoch 10, Step 4180: Loss = 33322.63671875, best_valid_rmse = 8.596214084982677\n",
      "Epoch 11, Step 4598: Loss = 32925.5390625, best_valid_rmse = 8.596214084982677\n",
      "Epoch 12, Step 5016: Loss = 33085.90625, best_valid_rmse = 8.58240239366959\n",
      "Epoch 13, Step 5434: Loss = 32851.0703125, best_valid_rmse = 8.58240239366959\n",
      "Epoch 14, Step 5852: Loss = 32281.1875, best_valid_rmse = 8.58240239366959\n",
      "Epoch 15, Step 6270: Loss = 32069.12109375, best_valid_rmse = 8.576209122962839\n",
      "Epoch 16, Step 6688: Loss = 32177.71875, best_valid_rmse = 8.576209122962839\n",
      "Epoch 17, Step 7106: Loss = 32468.765625, best_valid_rmse = 8.576209122962839\n",
      "Epoch 18, Step 7524: Loss = 32280.25, best_valid_rmse = 8.576209122962839\n",
      "Epoch 19, Step 7942: Loss = 32291.38671875, best_valid_rmse = 8.576209122962839\n",
      "Epoch 20, Step 8360: Loss = 32116.17578125, best_valid_rmse = 8.576209122962839\n",
      "Epoch 21, Step 8778: Loss = 32028.76171875, best_valid_rmse = 8.576209122962839\n",
      "Epoch 22, Step 9196: Loss = 31936.49609375, best_valid_rmse = 8.576209122962839\n",
      "Epoch 23, Step 9614: Loss = 31729.8359375, best_valid_rmse = 8.576209122962839\n",
      "Epoch 24, Step 10032: Loss = 31334.64453125, best_valid_rmse = 8.576209122962839\n",
      "Epoch 25, Step 10450: Loss = 30936.8828125, best_valid_rmse = 8.576209122962839\n",
      "Epoch 26, Step 10868: Loss = 30678.36328125, best_valid_rmse = 8.576209122962839\n",
      "early stopping, step 11100, validation RMSE = 8.643195686989584\n"
     ]
    }
   ],
   "source": [
    "# create MLP model\n",
    "d_hidden = 45\n",
    "d_input = subtrain_set.Xnp.shape[1]\n",
    "d_output = 1\n",
    "\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d_input, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_hidden, d_output)\n",
    ")\n",
    "mlp = mlp.float()\n",
    "mlp.to(device)\n",
    "weight_path = './MLP45_weight'\n",
    "\n",
    "# optimizer\n",
    "lr = 0.00001\n",
    "momentum = 0\n",
    "weight_decay = 0\n",
    "sgd_optimizer = torch.optim.SGD(mlp.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# loss\n",
    "l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# train\n",
    "all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=sgd_optimizer, loss_f=l2_loss, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=True)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'H=45, Training & Validation RMSE')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot subtrain / validation RMSE\n",
    "plt.plot(all_step, all_train_rmse, c='b', label='Training RMSE')\n",
    "plt.plot(all_step, all_valid_rmse, c='r', label='Validation RMSE')\n",
    "plt.legend()\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('H=45, Training & Validation RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above, we can see that both training RMSE and validation RMSE reduce but fluctuate through the training process. While the two curves are quite close in the beginning, training RMSE turns out to be lower than training RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 8.841294436539107\n"
     ]
    }
   ],
   "source": [
    "# test RMSE\n",
    "saved_mlp = torch.load(weight_path)\n",
    "test_rmse = RMSE(saved_mlp, test_loader)\n",
    "print(f'Test RMSE = {test_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. H = 90, 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 34200.109375, best_valid_rmse = 8.901226091145327\n",
      "Epoch 2, Step 836: Loss = 33780.6875, best_valid_rmse = 8.674241658673425\n",
      "Epoch 3, Step 1254: Loss = 33153.56640625, best_valid_rmse = 8.638080569142724\n",
      "Epoch 4, Step 1672: Loss = 32139.08203125, best_valid_rmse = 8.562022940231628\n",
      "Epoch 5, Step 2090: Loss = 31482.390625, best_valid_rmse = 8.562022940231628\n",
      "Epoch 6, Step 2508: Loss = 30781.51171875, best_valid_rmse = 8.562022940231628\n",
      "Epoch 7, Step 2926: Loss = 30700.87109375, best_valid_rmse = 8.544342587750187\n",
      "Epoch 8, Step 3344: Loss = 29986.6953125, best_valid_rmse = 8.544342587750187\n",
      "Epoch 9, Step 3762: Loss = 29589.697265625, best_valid_rmse = 8.524440732241779\n",
      "Epoch 10, Step 4180: Loss = 29720.140625, best_valid_rmse = 8.524440732241779\n",
      "Epoch 11, Step 4598: Loss = 28300.787109375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 12, Step 5016: Loss = 27845.724609375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 13, Step 5434: Loss = 28651.138671875, best_valid_rmse = 8.524440732241779\n",
      "Epoch 14, Step 5852: Loss = 27804.57421875, best_valid_rmse = 8.524440732241779\n",
      "Epoch 15, Step 6270: Loss = 27145.83203125, best_valid_rmse = 8.524440732241779\n",
      "Epoch 16, Step 6688: Loss = 27365.318359375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 17, Step 7106: Loss = 25914.62109375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 18, Step 7524: Loss = 25514.162109375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 19, Step 7942: Loss = 26703.62109375, best_valid_rmse = 8.524440732241779\n",
      "Epoch 20, Step 8360: Loss = 25954.685546875, best_valid_rmse = 8.524440732241779\n",
      "early stopping, step 8500, validation RMSE = 8.666315084529058\n",
      "H = 90, Test RMSE = 8.82367738460801\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34003.7265625, best_valid_rmse = 8.821839537051991\n",
      "Epoch 2, Step 836: Loss = 33052.890625, best_valid_rmse = 8.644408196072064\n",
      "Epoch 3, Step 1254: Loss = 31986.494140625, best_valid_rmse = 8.618648213139085\n",
      "Epoch 4, Step 1672: Loss = 31146.46484375, best_valid_rmse = 8.547684545341538\n",
      "Epoch 5, Step 2090: Loss = 30769.58203125, best_valid_rmse = 8.547684545341538\n",
      "Epoch 6, Step 2508: Loss = 30652.318359375, best_valid_rmse = 8.547684545341538\n",
      "Epoch 7, Step 2926: Loss = 29517.716796875, best_valid_rmse = 8.536382234634074\n",
      "Epoch 8, Step 3344: Loss = 28933.384765625, best_valid_rmse = 8.530303760692343\n",
      "Epoch 9, Step 3762: Loss = 28975.279296875, best_valid_rmse = 8.530303760692343\n",
      "Epoch 10, Step 4180: Loss = 28712.99609375, best_valid_rmse = 8.530303760692343\n",
      "Epoch 11, Step 4598: Loss = 27377.40234375, best_valid_rmse = 8.530303760692343\n",
      "Epoch 12, Step 5016: Loss = 27813.775390625, best_valid_rmse = 8.530303760692343\n",
      "Epoch 13, Step 5434: Loss = 25784.716796875, best_valid_rmse = 8.530303760692343\n",
      "Epoch 14, Step 5852: Loss = 23894.630859375, best_valid_rmse = 8.530303760692343\n",
      "Epoch 15, Step 6270: Loss = 24050.05078125, best_valid_rmse = 8.530303760692343\n",
      "Epoch 16, Step 6688: Loss = 24439.98828125, best_valid_rmse = 8.530303760692343\n",
      "Epoch 17, Step 7106: Loss = 24895.626953125, best_valid_rmse = 8.530303760692343\n",
      "Epoch 18, Step 7524: Loss = 21453.94140625, best_valid_rmse = 8.530303760692343\n",
      "Epoch 19, Step 7942: Loss = 21510.59765625, best_valid_rmse = 8.530303760692343\n",
      "early stopping, step 8000, validation RMSE = 8.731816505287078\n",
      "H = 180, Test RMSE = 8.900774927254158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d_hidden in [90, 180]:\n",
    "    \n",
    "    d_input = subtrain_set.Xnp.shape[1]\n",
    "    d_output = 1\n",
    "\n",
    "    mlp = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d_input, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(d_hidden, d_output)\n",
    "    )\n",
    "    \n",
    "    # optimizer\n",
    "    lr = 0.00001\n",
    "    momentum = 0\n",
    "    weight_decay = 0\n",
    "    sgd_optimizer = torch.optim.SGD(mlp.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # loss\n",
    "    l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    mlp = mlp.float()\n",
    "    mlp.to(device)\n",
    "    weight_path = f'./MLP{d_hidden}_weight'\n",
    "    all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=sgd_optimizer, loss_f=l2_loss, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=False)\n",
    "    saved_mlp = torch.load(weight_path)\n",
    "    test_rmse = RMSE(saved_mlp, test_loader)\n",
    "    print(f'H = {d_hidden}, Test RMSE = {test_rmse}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(hidden dimension = 45: test RMSE = 8.841)  \n",
    "**hidden dimension = 90: test RMSE = 8.824**  \n",
    "**hidden dimension = 180: test RMSE = 8.901**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With test RMSE shown above, we suggest that providing 90 hidden nodes in the linear hidden layers performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 MLP with Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 34912.4140625, best_valid_rmse = 8.939762386907391\n",
      "Epoch 2, Step 836: Loss = 34052.6328125, best_valid_rmse = 8.719163570821374\n",
      "Epoch 3, Step 1254: Loss = 33766.41796875, best_valid_rmse = 8.70004401025287\n",
      "Epoch 4, Step 1672: Loss = 33187.953125, best_valid_rmse = 8.622452527263516\n",
      "Epoch 5, Step 2090: Loss = 32928.0703125, best_valid_rmse = 8.612548300323168\n",
      "Epoch 6, Step 2508: Loss = 32510.919921875, best_valid_rmse = 8.612548300323168\n",
      "Epoch 7, Step 2926: Loss = 32025.77734375, best_valid_rmse = 8.609259075554823\n",
      "Epoch 8, Step 3344: Loss = 31523.86328125, best_valid_rmse = 8.579808557178483\n",
      "Epoch 9, Step 3762: Loss = 31441.578125, best_valid_rmse = 8.579808557178483\n",
      "Epoch 10, Step 4180: Loss = 31203.77734375, best_valid_rmse = 8.579808557178483\n",
      "Epoch 11, Step 4598: Loss = 31422.44921875, best_valid_rmse = 8.579808557178483\n",
      "Epoch 12, Step 5016: Loss = 31260.291015625, best_valid_rmse = 8.569624163502223\n",
      "Epoch 13, Step 5434: Loss = 32024.271484375, best_valid_rmse = 8.569624163502223\n",
      "Epoch 14, Step 5852: Loss = 31735.267578125, best_valid_rmse = 8.569624163502223\n",
      "Epoch 15, Step 6270: Loss = 31538.45703125, best_valid_rmse = 8.569624163502223\n",
      "Epoch 16, Step 6688: Loss = 31671.77734375, best_valid_rmse = 8.569624163502223\n",
      "Epoch 17, Step 7106: Loss = 31534.8046875, best_valid_rmse = 8.569624163502223\n",
      "Epoch 18, Step 7524: Loss = 31360.6875, best_valid_rmse = 8.569624163502223\n",
      "Epoch 19, Step 7942: Loss = 31117.5859375, best_valid_rmse = 8.569624163502223\n",
      "Epoch 20, Step 8360: Loss = 31096.337890625, best_valid_rmse = 8.569624163502223\n",
      "Epoch 21, Step 8778: Loss = 31077.04296875, best_valid_rmse = 8.569624163502223\n",
      "Epoch 22, Step 9196: Loss = 31229.375, best_valid_rmse = 8.569624163502223\n",
      "Epoch 23, Step 9614: Loss = 30921.08984375, best_valid_rmse = 8.569624163502223\n",
      "early stopping, step 9800, validation RMSE = 8.597169841237015\n",
      "H = 45 Weight decay = 0.1, Test RMSE = 8.851353864431783\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34462.296875, best_valid_rmse = 8.917254626555824\n",
      "Epoch 2, Step 836: Loss = 34030.7890625, best_valid_rmse = 8.698164607675224\n",
      "Epoch 3, Step 1254: Loss = 33437.859375, best_valid_rmse = 8.656580620790855\n",
      "Epoch 4, Step 1672: Loss = 33448.40625, best_valid_rmse = 8.624874768554143\n",
      "Epoch 5, Step 2090: Loss = 33207.1171875, best_valid_rmse = 8.617164932529311\n",
      "Epoch 6, Step 2508: Loss = 32972.078125, best_valid_rmse = 8.617164932529311\n",
      "Epoch 7, Step 2926: Loss = 32946.89453125, best_valid_rmse = 8.609978886739293\n",
      "Epoch 8, Step 3344: Loss = 32745.208984375, best_valid_rmse = 8.576257841995167\n",
      "Epoch 9, Step 3762: Loss = 32872.51171875, best_valid_rmse = 8.573661832149122\n",
      "Epoch 10, Step 4180: Loss = 33082.234375, best_valid_rmse = 8.573661832149122\n",
      "Epoch 11, Step 4598: Loss = 33200.15625, best_valid_rmse = 8.573661832149122\n",
      "Epoch 12, Step 5016: Loss = 33149.2265625, best_valid_rmse = 8.560034822188571\n",
      "Epoch 13, Step 5434: Loss = 33106.0, best_valid_rmse = 8.560034822188571\n",
      "Epoch 14, Step 5852: Loss = 33310.7890625, best_valid_rmse = 8.560034822188571\n",
      "Epoch 15, Step 6270: Loss = 33157.828125, best_valid_rmse = 8.560034822188571\n",
      "Epoch 16, Step 6688: Loss = 33379.140625, best_valid_rmse = 8.560034822188571\n",
      "Epoch 17, Step 7106: Loss = 32860.3515625, best_valid_rmse = 8.560034822188571\n",
      "Epoch 18, Step 7524: Loss = 33102.67578125, best_valid_rmse = 8.560034822188571\n",
      "Epoch 19, Step 7942: Loss = 32521.32421875, best_valid_rmse = 8.560034822188571\n",
      "Epoch 20, Step 8360: Loss = 32043.068359375, best_valid_rmse = 8.560034822188571\n",
      "Epoch 21, Step 8778: Loss = 32070.52734375, best_valid_rmse = 8.560034822188571\n",
      "Epoch 22, Step 9196: Loss = 31538.60546875, best_valid_rmse = 8.560034822188571\n",
      "Epoch 23, Step 9614: Loss = 31506.7265625, best_valid_rmse = 8.560034822188571\n",
      "early stopping, step 9800, validation RMSE = 8.627312889915519\n",
      "H = 45 Weight decay = 0.2, Test RMSE = 8.819504119562046\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34559.9609375, best_valid_rmse = 8.881725737713577\n",
      "Epoch 2, Step 836: Loss = 33271.82421875, best_valid_rmse = 8.691728489881214\n",
      "Epoch 3, Step 1254: Loss = 32780.53125, best_valid_rmse = 8.663582397593315\n",
      "Epoch 4, Step 1672: Loss = 32482.58984375, best_valid_rmse = 8.599778502458415\n",
      "Epoch 5, Step 2090: Loss = 32183.5234375, best_valid_rmse = 8.580497223864274\n",
      "Epoch 6, Step 2508: Loss = 32146.03515625, best_valid_rmse = 8.580497223864274\n",
      "Epoch 7, Step 2926: Loss = 32444.2890625, best_valid_rmse = 8.580497223864274\n",
      "Epoch 8, Step 3344: Loss = 32248.12890625, best_valid_rmse = 8.567530833881275\n",
      "Epoch 9, Step 3762: Loss = 32050.62890625, best_valid_rmse = 8.562462442819037\n",
      "Epoch 10, Step 4180: Loss = 32220.390625, best_valid_rmse = 8.562462442819037\n",
      "Epoch 11, Step 4598: Loss = 32469.0390625, best_valid_rmse = 8.562462442819037\n",
      "Epoch 12, Step 5016: Loss = 32691.1875, best_valid_rmse = 8.541651354044294\n",
      "Epoch 13, Step 5434: Loss = 32659.458984375, best_valid_rmse = 8.541651354044294\n",
      "Epoch 14, Step 5852: Loss = 32576.0234375, best_valid_rmse = 8.541651354044294\n",
      "Epoch 15, Step 6270: Loss = 32261.84765625, best_valid_rmse = 8.541651354044294\n",
      "Epoch 16, Step 6688: Loss = 32354.37890625, best_valid_rmse = 8.541651354044294\n",
      "Epoch 17, Step 7106: Loss = 32698.89453125, best_valid_rmse = 8.541651354044294\n",
      "Epoch 18, Step 7524: Loss = 32725.8203125, best_valid_rmse = 8.541651354044294\n",
      "Epoch 19, Step 7942: Loss = 32903.3515625, best_valid_rmse = 8.541651354044294\n",
      "Epoch 20, Step 8360: Loss = 32789.2421875, best_valid_rmse = 8.541651354044294\n",
      "Epoch 21, Step 8778: Loss = 32520.701171875, best_valid_rmse = 8.541651354044294\n",
      "Epoch 22, Step 9196: Loss = 32211.544921875, best_valid_rmse = 8.541651354044294\n",
      "Epoch 23, Step 9614: Loss = 32292.232421875, best_valid_rmse = 8.541651354044294\n",
      "early stopping, step 9800, validation RMSE = 8.59790884893433\n",
      "H = 45 Weight decay = 0.4, Test RMSE = 8.794360173977948\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34927.3046875, best_valid_rmse = 8.886551070767755\n",
      "Epoch 2, Step 836: Loss = 33805.625, best_valid_rmse = 8.668629603540655\n",
      "Epoch 3, Step 1254: Loss = 33181.87890625, best_valid_rmse = 8.648042521367552\n",
      "Epoch 4, Step 1672: Loss = 33110.4296875, best_valid_rmse = 8.582788402104507\n",
      "Epoch 5, Step 2090: Loss = 32138.080078125, best_valid_rmse = 8.569312115536963\n",
      "Epoch 6, Step 2508: Loss = 32159.541015625, best_valid_rmse = 8.569312115536963\n",
      "Epoch 7, Step 2926: Loss = 31553.390625, best_valid_rmse = 8.55445183144975\n",
      "Epoch 8, Step 3344: Loss = 31156.603515625, best_valid_rmse = 8.55445183144975\n",
      "Epoch 9, Step 3762: Loss = 31300.1875, best_valid_rmse = 8.5460807580512\n",
      "Epoch 10, Step 4180: Loss = 30399.794921875, best_valid_rmse = 8.541034986696875\n",
      "Epoch 11, Step 4598: Loss = 31513.263671875, best_valid_rmse = 8.541034986696875\n",
      "Epoch 12, Step 5016: Loss = 31618.107421875, best_valid_rmse = 8.541034986696875\n",
      "Epoch 13, Step 5434: Loss = 30669.69140625, best_valid_rmse = 8.541034986696875\n",
      "Epoch 14, Step 5852: Loss = 29964.396484375, best_valid_rmse = 8.541034986696875\n",
      "Epoch 15, Step 6270: Loss = 29810.40625, best_valid_rmse = 8.541034986696875\n",
      "Epoch 16, Step 6688: Loss = 30490.59375, best_valid_rmse = 8.541034986696875\n",
      "Epoch 17, Step 7106: Loss = 30579.041015625, best_valid_rmse = 8.541034986696875\n",
      "Epoch 18, Step 7524: Loss = 29256.966796875, best_valid_rmse = 8.541034986696875\n",
      "Epoch 19, Step 7942: Loss = 29902.59765625, best_valid_rmse = 8.541034986696875\n",
      "Epoch 20, Step 8360: Loss = 30182.98828125, best_valid_rmse = 8.541034986696875\n",
      "Epoch 21, Step 8778: Loss = 29295.8828125, best_valid_rmse = 8.541034986696875\n",
      "early stopping, step 8800, validation RMSE = 8.7525231499847\n",
      "H = 90 Weight decay = 0.1, Test RMSE = 8.885646642211364\n",
      "\n",
      "Epoch 1, Step 418: Loss = 33973.62109375, best_valid_rmse = 8.879677144594403\n",
      "Epoch 2, Step 836: Loss = 32851.8515625, best_valid_rmse = 8.677611250110472\n",
      "Epoch 3, Step 1254: Loss = 31940.30078125, best_valid_rmse = 8.649352835853822\n",
      "Epoch 4, Step 1672: Loss = 31619.556640625, best_valid_rmse = 8.57152766167714\n",
      "Epoch 5, Step 2090: Loss = 30733.6640625, best_valid_rmse = 8.557955841311509\n",
      "Epoch 6, Step 2508: Loss = 30214.091796875, best_valid_rmse = 8.557955841311509\n",
      "Epoch 7, Step 2926: Loss = 30506.55859375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 8, Step 3344: Loss = 29877.43359375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 9, Step 3762: Loss = 29759.44921875, best_valid_rmse = 8.549762870916814\n",
      "Epoch 10, Step 4180: Loss = 28960.17578125, best_valid_rmse = 8.549762870916814\n",
      "Epoch 11, Step 4598: Loss = 29228.017578125, best_valid_rmse = 8.549762870916814\n",
      "Epoch 12, Step 5016: Loss = 28993.662109375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 13, Step 5434: Loss = 28092.568359375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 14, Step 5852: Loss = 27083.5859375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 15, Step 6270: Loss = 26789.20703125, best_valid_rmse = 8.549762870916814\n",
      "Epoch 16, Step 6688: Loss = 27314.515625, best_valid_rmse = 8.549762870916814\n",
      "Epoch 17, Step 7106: Loss = 28337.52734375, best_valid_rmse = 8.549762870916814\n",
      "Epoch 18, Step 7524: Loss = 29396.578125, best_valid_rmse = 8.549762870916814\n",
      "early stopping, step 7900, validation RMSE = 8.853353061411825\n",
      "H = 90 Weight decay = 0.2, Test RMSE = 8.806475693435283\n",
      "\n",
      "Epoch 1, Step 418: Loss = 33937.81640625, best_valid_rmse = 8.87782453509795\n",
      "Epoch 2, Step 836: Loss = 33344.8046875, best_valid_rmse = 8.672950334175994\n",
      "Epoch 3, Step 1254: Loss = 31762.38671875, best_valid_rmse = 8.658701922067856\n",
      "Epoch 4, Step 1672: Loss = 31498.1171875, best_valid_rmse = 8.574921268574053\n",
      "Epoch 5, Step 2090: Loss = 30466.859375, best_valid_rmse = 8.566666800924734\n",
      "Epoch 6, Step 2508: Loss = 30171.93359375, best_valid_rmse = 8.566666800924734\n",
      "Epoch 7, Step 2926: Loss = 29960.490234375, best_valid_rmse = 8.530335361504942\n",
      "Epoch 8, Step 3344: Loss = 29694.052734375, best_valid_rmse = 8.530335361504942\n",
      "Epoch 9, Step 3762: Loss = 30834.654296875, best_valid_rmse = 8.530335361504942\n",
      "Epoch 10, Step 4180: Loss = 30730.49609375, best_valid_rmse = 8.530335361504942\n",
      "Epoch 11, Step 4598: Loss = 31328.263671875, best_valid_rmse = 8.530335361504942\n",
      "Epoch 12, Step 5016: Loss = 31197.82421875, best_valid_rmse = 8.530335361504942\n",
      "Epoch 13, Step 5434: Loss = 31094.322265625, best_valid_rmse = 8.530335361504942\n",
      "Epoch 14, Step 5852: Loss = 32556.33203125, best_valid_rmse = 8.530335361504942\n",
      "Epoch 15, Step 6270: Loss = 32216.21875, best_valid_rmse = 8.530335361504942\n",
      "Epoch 16, Step 6688: Loss = 32157.19140625, best_valid_rmse = 8.530335361504942\n",
      "Epoch 17, Step 7106: Loss = 31649.466796875, best_valid_rmse = 8.530335361504942\n",
      "Epoch 18, Step 7524: Loss = 30651.400390625, best_valid_rmse = 8.530335361504942\n",
      "early stopping, step 7900, validation RMSE = 8.619891382246069\n",
      "H = 90 Weight decay = 0.4, Test RMSE = 8.800657846979894\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34387.76171875, best_valid_rmse = 8.863910322929424\n",
      "Epoch 2, Step 836: Loss = 33512.0234375, best_valid_rmse = 8.64502841853085\n",
      "Epoch 3, Step 1254: Loss = 32934.6640625, best_valid_rmse = 8.626262350977633\n",
      "Epoch 4, Step 1672: Loss = 31933.296875, best_valid_rmse = 8.553214909129162\n",
      "Epoch 5, Step 2090: Loss = 31030.109375, best_valid_rmse = 8.532850410765244\n",
      "Epoch 6, Step 2508: Loss = 29463.5703125, best_valid_rmse = 8.532850410765244\n",
      "Epoch 7, Step 2926: Loss = 28776.3671875, best_valid_rmse = 8.532850410765244\n",
      "Epoch 8, Step 3344: Loss = 27535.70703125, best_valid_rmse = 8.527279653298958\n",
      "Epoch 9, Step 3762: Loss = 27045.900390625, best_valid_rmse = 8.522156009014823\n",
      "Epoch 10, Step 4180: Loss = 26498.484375, best_valid_rmse = 8.522156009014823\n",
      "Epoch 11, Step 4598: Loss = 26385.076171875, best_valid_rmse = 8.522156009014823\n",
      "Epoch 12, Step 5016: Loss = 24847.142578125, best_valid_rmse = 8.522156009014823\n",
      "Epoch 13, Step 5434: Loss = 25109.69140625, best_valid_rmse = 8.522156009014823\n",
      "Epoch 14, Step 5852: Loss = 25372.22265625, best_valid_rmse = 8.522156009014823\n",
      "Epoch 15, Step 6270: Loss = 26994.755859375, best_valid_rmse = 8.522156009014823\n",
      "Epoch 16, Step 6688: Loss = 24910.41015625, best_valid_rmse = 8.522156009014823\n",
      "Epoch 17, Step 7106: Loss = 24449.90234375, best_valid_rmse = 8.522156009014823\n",
      "Epoch 18, Step 7524: Loss = 22592.55859375, best_valid_rmse = 8.522156009014823\n",
      "Epoch 19, Step 7942: Loss = 23169.455078125, best_valid_rmse = 8.522156009014823\n",
      "Epoch 20, Step 8360: Loss = 24028.515625, best_valid_rmse = 8.522156009014823\n",
      "early stopping, step 8600, validation RMSE = 8.99097882807079\n",
      "H = 180 Weight decay = 0.1, Test RMSE = 8.875524879929499\n",
      "\n",
      "Epoch 1, Step 418: Loss = 33751.3125, best_valid_rmse = 8.846688271982877\n",
      "Epoch 2, Step 836: Loss = 32691.751953125, best_valid_rmse = 8.662645482402596\n",
      "Epoch 3, Step 1254: Loss = 32101.31640625, best_valid_rmse = 8.629473873112682\n",
      "Epoch 4, Step 1672: Loss = 31696.322265625, best_valid_rmse = 8.5631084328947\n",
      "Epoch 5, Step 2090: Loss = 31237.1875, best_valid_rmse = 8.536620647992228\n",
      "Epoch 6, Step 2508: Loss = 30627.244140625, best_valid_rmse = 8.536620647992228\n",
      "Epoch 7, Step 2926: Loss = 30032.306640625, best_valid_rmse = 8.536620647992228\n",
      "Epoch 8, Step 3344: Loss = 29818.357421875, best_valid_rmse = 8.530688966614015\n",
      "Epoch 9, Step 3762: Loss = 29551.7734375, best_valid_rmse = 8.530688966614015\n",
      "Epoch 10, Step 4180: Loss = 28327.056640625, best_valid_rmse = 8.530688966614015\n",
      "Epoch 11, Step 4598: Loss = 28074.1640625, best_valid_rmse = 8.528235548961627\n",
      "Epoch 12, Step 5016: Loss = 27113.3203125, best_valid_rmse = 8.528235548961627\n",
      "Epoch 13, Step 5434: Loss = 26846.26953125, best_valid_rmse = 8.528235548961627\n",
      "Epoch 14, Step 5852: Loss = 26805.71484375, best_valid_rmse = 8.528235548961627\n",
      "Epoch 15, Step 6270: Loss = 24925.0625, best_valid_rmse = 8.528235548961627\n",
      "Epoch 16, Step 6688: Loss = 25059.82421875, best_valid_rmse = 8.528235548961627\n",
      "Epoch 17, Step 7106: Loss = 23658.76953125, best_valid_rmse = 8.528235548961627\n",
      "Epoch 18, Step 7524: Loss = 22327.98828125, best_valid_rmse = 8.528235548961627\n",
      "Epoch 19, Step 7942: Loss = 21432.818359375, best_valid_rmse = 8.528235548961627\n",
      "Epoch 20, Step 8360: Loss = 21441.65234375, best_valid_rmse = 8.528235548961627\n",
      "Epoch 21, Step 8778: Loss = 22197.560546875, best_valid_rmse = 8.528235548961627\n",
      "Epoch 22, Step 9196: Loss = 18604.216796875, best_valid_rmse = 8.528235548961627\n",
      "early stopping, step 9300, validation RMSE = 8.839641547355317\n",
      "H = 180 Weight decay = 0.2, Test RMSE = 8.883971610633367\n",
      "\n",
      "Epoch 1, Step 418: Loss = 34862.578125, best_valid_rmse = 8.858036215194678\n",
      "Epoch 2, Step 836: Loss = 33618.02734375, best_valid_rmse = 8.645813221485273\n",
      "Epoch 3, Step 1254: Loss = 32709.123046875, best_valid_rmse = 8.630792316268295\n",
      "Epoch 4, Step 1672: Loss = 31735.15234375, best_valid_rmse = 8.561291540285554\n",
      "Epoch 5, Step 2090: Loss = 30962.7421875, best_valid_rmse = 8.544818963602657\n",
      "Epoch 6, Step 2508: Loss = 30336.07421875, best_valid_rmse = 8.544818963602657\n",
      "Epoch 7, Step 2926: Loss = 29980.84765625, best_valid_rmse = 8.544818963602657\n",
      "Epoch 8, Step 3344: Loss = 29229.0, best_valid_rmse = 8.544818963602657\n",
      "Epoch 9, Step 3762: Loss = 28731.767578125, best_valid_rmse = 8.530658315098089\n",
      "Epoch 10, Step 4180: Loss = 26437.75, best_valid_rmse = 8.530658315098089\n",
      "Epoch 11, Step 4598: Loss = 27507.423828125, best_valid_rmse = 8.530658315098089\n",
      "Epoch 12, Step 5016: Loss = 24949.95703125, best_valid_rmse = 8.530658315098089\n",
      "Epoch 13, Step 5434: Loss = 25729.892578125, best_valid_rmse = 8.530658315098089\n",
      "Epoch 14, Step 5852: Loss = 24489.5859375, best_valid_rmse = 8.530658315098089\n",
      "Epoch 15, Step 6270: Loss = 24429.373046875, best_valid_rmse = 8.530658315098089\n",
      "Epoch 16, Step 6688: Loss = 24007.67578125, best_valid_rmse = 8.530658315098089\n",
      "Epoch 17, Step 7106: Loss = 24550.10546875, best_valid_rmse = 8.530658315098089\n",
      "Epoch 18, Step 7524: Loss = 22887.37890625, best_valid_rmse = 8.530658315098089\n",
      "Epoch 19, Step 7942: Loss = 23665.78515625, best_valid_rmse = 8.530658315098089\n",
      "Epoch 20, Step 8360: Loss = 24208.125, best_valid_rmse = 8.530658315098089\n",
      "early stopping, step 8600, validation RMSE = 8.728568210177409\n",
      "H = 180 Weight decay = 0.4, Test RMSE = 8.868065281236266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d_hidden in [45, 90, 180]:\n",
    "    for weight_decay in [0.1, 0.2, 0.4]:\n",
    "    \n",
    "        d_input = subtrain_set.Xnp.shape[1]\n",
    "        d_output = 1\n",
    "\n",
    "        mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_input, d_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_hidden, d_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_hidden, d_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_hidden, d_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_hidden, d_output)\n",
    "        )\n",
    "\n",
    "        mlp = mlp.float()\n",
    "        mlp.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        lr = 0.00001\n",
    "        momentum = 0\n",
    "        sgd_optimizer_wd = torch.optim.SGD(mlp.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "        weight_path = f'./MLP{d_hidden}_{weight_decay*10}_weight'\n",
    "        all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=sgd_optimizer_wd, loss_f=l2_loss, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=False)\n",
    "        saved_mlp = torch.load(weight_path)\n",
    "        test_rmse = RMSE(saved_mlp, test_loader)\n",
    "        print(f'H = {d_hidden} Weight decay = {weight_decay}, Test RMSE = {test_rmse}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    Test RMSE     | H=45  | H=90  | H=180 |\n",
    "| ---------------- | ----- | ----- | ----- |\n",
    "| weight_decay=0.1 | 8.851 | 8.886 | 8.876 |\n",
    "| weight_decay=0.2 | 8.820 | 8.806 | 8.884 |\n",
    "| weight_decay=0.4 | 8.794 | 8.801 | 8.868 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we can see that **H=45** with **weight_decay=0.4** leads to the lowest testing RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. MLP with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 37498.9921875, best_valid_rmse = 9.186258692961909\n",
      "Epoch 2, Step 836: Loss = 36125.3359375, best_valid_rmse = 9.071526105535854\n",
      "Epoch 3, Step 1254: Loss = 35325.8828125, best_valid_rmse = 9.000254585594107\n",
      "Epoch 4, Step 1672: Loss = 34050.859375, best_valid_rmse = 8.95554586681681\n",
      "Epoch 5, Step 2090: Loss = 34787.98828125, best_valid_rmse = 8.931380310302917\n",
      "Epoch 6, Step 2508: Loss = 35317.65625, best_valid_rmse = 8.920974207522569\n",
      "Epoch 7, Step 2926: Loss = 32773.45703125, best_valid_rmse = 8.90552114791495\n",
      "Epoch 8, Step 3344: Loss = 34434.96875, best_valid_rmse = 8.904098673894705\n",
      "Epoch 9, Step 3762: Loss = 34547.0703125, best_valid_rmse = 8.867553483159396\n",
      "Epoch 10, Step 4180: Loss = 33072.8984375, best_valid_rmse = 8.865956174229254\n",
      "Epoch 11, Step 4598: Loss = 34885.29296875, best_valid_rmse = 8.854683529840045\n",
      "Epoch 12, Step 5016: Loss = 34875.23828125, best_valid_rmse = 8.854683529840045\n",
      "Epoch 13, Step 5434: Loss = 31445.935546875, best_valid_rmse = 8.854683529840045\n",
      "Epoch 14, Step 5852: Loss = 34940.26171875, best_valid_rmse = 8.843549230116075\n",
      "Epoch 15, Step 6270: Loss = 34832.55859375, best_valid_rmse = 8.843549230116075\n",
      "Epoch 16, Step 6688: Loss = 36101.1171875, best_valid_rmse = 8.843549230116075\n",
      "Epoch 17, Step 7106: Loss = 33880.984375, best_valid_rmse = 8.843549230116075\n",
      "Epoch 18, Step 7524: Loss = 33412.26171875, best_valid_rmse = 8.843549230116075\n",
      "Epoch 19, Step 7942: Loss = 33485.734375, best_valid_rmse = 8.843549230116075\n",
      "Epoch 20, Step 8360: Loss = 33591.3515625, best_valid_rmse = 8.838697066184602\n",
      "Epoch 21, Step 8778: Loss = 33695.5859375, best_valid_rmse = 8.838697066184602\n",
      "Epoch 22, Step 9196: Loss = 33030.671875, best_valid_rmse = 8.838697066184602\n",
      "Epoch 23, Step 9614: Loss = 35249.03125, best_valid_rmse = 8.823254192148125\n",
      "Epoch 24, Step 10032: Loss = 34508.0078125, best_valid_rmse = 8.823254192148125\n",
      "Epoch 25, Step 10450: Loss = 33798.85546875, best_valid_rmse = 8.821462461097738\n",
      "Epoch 26, Step 10868: Loss = 33107.2265625, best_valid_rmse = 8.814298928766751\n",
      "Epoch 27, Step 11286: Loss = 32872.40625, best_valid_rmse = 8.814298928766751\n",
      "Epoch 28, Step 11704: Loss = 33921.5625, best_valid_rmse = 8.814298928766751\n",
      "Epoch 29, Step 12122: Loss = 32726.55078125, best_valid_rmse = 8.814298928766751\n",
      "Epoch 30, Step 12540: Loss = 34868.4921875, best_valid_rmse = 8.814298928766751\n",
      "Epoch 31, Step 12958: Loss = 33140.140625, best_valid_rmse = 8.814298928766751\n",
      "Epoch 32, Step 13376: Loss = 34056.1875, best_valid_rmse = 8.814298928766751\n",
      "Epoch 33, Step 13794: Loss = 33043.1953125, best_valid_rmse = 8.814298928766751\n",
      "Epoch 34, Step 14212: Loss = 31808.677734375, best_valid_rmse = 8.814298928766751\n",
      "Epoch 35, Step 14630: Loss = 32798.51171875, best_valid_rmse = 8.814298928766751\n",
      "Epoch 36, Step 15048: Loss = 35199.6484375, best_valid_rmse = 8.814298928766751\n",
      "Epoch 37, Step 15466: Loss = 36264.6171875, best_valid_rmse = 8.814298928766751\n",
      "early stopping, step 15500, validation RMSE = 8.86281604871643\n"
     ]
    }
   ],
   "source": [
    "d_hidden = 90\n",
    "d_input = subtrain_set.Xnp.shape[1]\n",
    "d_output = 1\n",
    "\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d_input, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_output)\n",
    ")\n",
    "mlp = mlp.float()\n",
    "mlp.to(device)\n",
    "weight_path = './MLP90_drop_weight'\n",
    "\n",
    "# optimizer\n",
    "lr = 0.001\n",
    "momentum = 0\n",
    "weight_decay = 0\n",
    "adam_optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# loss\n",
    "l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# train\n",
    "all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=adam_optimizer, loss_f=l2_loss, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=True)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'H=90 with Drop Out, Training & Validation RMSE')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot subtrain / validation RMSE\n",
    "plt.plot(all_step, all_train_rmse, c='b', label='Training RMSE')\n",
    "plt.plot(all_step, all_valid_rmse, c='r', label='Validation RMSE')\n",
    "plt.legend()\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('H=90 with Drop Out, Training & Validation RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above, we can see that both training RMSE and validation RMSE drop dramastically in the beginning of the training process and converges afterwards. In the first half of the training process, training RMSE is larger than validation RMSE, while in the second half, training RMSE turns out to be smaller than validation RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 9.029719368877945\n"
     ]
    }
   ],
   "source": [
    "# test RMSE\n",
    "saved_mlp = torch.load(weight_path)\n",
    "test_rmse = RMSE(saved_mlp, test_loader)\n",
    "print(f'Test RMSE = {test_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Explore Number of Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 43384.8203125, best_valid_rmse = 9.763224669693798\n",
      "Epoch 2, Step 836: Loss = 38465.2734375, best_valid_rmse = 9.572969094941469\n",
      "Epoch 3, Step 1254: Loss = 39023.46484375, best_valid_rmse = 9.482433656928665\n",
      "Epoch 4, Step 1672: Loss = 38567.38671875, best_valid_rmse = 9.417149557373707\n",
      "Epoch 5, Step 2090: Loss = 38634.7265625, best_valid_rmse = 9.413087093072491\n",
      "Epoch 6, Step 2508: Loss = 36937.37890625, best_valid_rmse = 9.381767395648092\n",
      "Epoch 7, Step 2926: Loss = 39024.5078125, best_valid_rmse = 9.352594674772869\n",
      "Epoch 8, Step 3344: Loss = 39017.86328125, best_valid_rmse = 9.35191789831301\n",
      "Epoch 9, Step 3762: Loss = 38357.61328125, best_valid_rmse = 9.35191789831301\n",
      "Epoch 10, Step 4180: Loss = 35153.7421875, best_valid_rmse = 9.341722855055735\n",
      "Epoch 11, Step 4598: Loss = 38768.7265625, best_valid_rmse = 9.341722855055735\n",
      "Epoch 12, Step 5016: Loss = 39046.4375, best_valid_rmse = 9.331254659203784\n",
      "Epoch 13, Step 5434: Loss = 37445.828125, best_valid_rmse = 9.312020293055658\n",
      "Epoch 14, Step 5852: Loss = 41450.328125, best_valid_rmse = 9.312020293055658\n",
      "Epoch 15, Step 6270: Loss = 38891.13671875, best_valid_rmse = 9.306672338035106\n",
      "Epoch 16, Step 6688: Loss = 38966.9765625, best_valid_rmse = 9.306672338035106\n",
      "Epoch 17, Step 7106: Loss = 40096.328125, best_valid_rmse = 9.306672338035106\n",
      "Epoch 18, Step 7524: Loss = 39424.74609375, best_valid_rmse = 9.306672338035106\n",
      "Epoch 19, Step 7942: Loss = 39548.265625, best_valid_rmse = 9.306672338035106\n",
      "Epoch 20, Step 8360: Loss = 36777.41015625, best_valid_rmse = 9.306672338035106\n",
      "Epoch 21, Step 8778: Loss = 35394.83984375, best_valid_rmse = 9.306672338035106\n",
      "Epoch 22, Step 9196: Loss = 35826.54296875, best_valid_rmse = 9.306672338035106\n",
      "Epoch 23, Step 9614: Loss = 39954.1953125, best_valid_rmse = 9.295074014780239\n",
      "Epoch 24, Step 10032: Loss = 36614.515625, best_valid_rmse = 9.295074014780239\n",
      "Epoch 25, Step 10450: Loss = 40547.4453125, best_valid_rmse = 9.295074014780239\n",
      "Epoch 26, Step 10868: Loss = 37179.30859375, best_valid_rmse = 9.295074014780239\n",
      "Epoch 27, Step 11286: Loss = 39730.640625, best_valid_rmse = 9.295074014780239\n",
      "Epoch 28, Step 11704: Loss = 39129.390625, best_valid_rmse = 9.295074014780239\n",
      "Epoch 29, Step 12122: Loss = 37045.859375, best_valid_rmse = 9.295074014780239\n",
      "Epoch 30, Step 12540: Loss = 41133.8984375, best_valid_rmse = 9.292288936096801\n",
      "Epoch 31, Step 12958: Loss = 37965.98046875, best_valid_rmse = 9.292288936096801\n",
      "Epoch 32, Step 13376: Loss = 37179.859375, best_valid_rmse = 9.292288936096801\n",
      "Epoch 33, Step 13794: Loss = 36986.71875, best_valid_rmse = 9.292288936096801\n",
      "Epoch 34, Step 14212: Loss = 36490.25390625, best_valid_rmse = 9.292288936096801\n",
      "Epoch 35, Step 14630: Loss = 35744.08984375, best_valid_rmse = 9.292288936096801\n",
      "Epoch 36, Step 15048: Loss = 36182.171875, best_valid_rmse = 9.292288936096801\n",
      "Epoch 37, Step 15466: Loss = 36457.6171875, best_valid_rmse = 9.292288936096801\n",
      "Epoch 38, Step 15884: Loss = 38000.11328125, best_valid_rmse = 9.292288936096801\n",
      "Epoch 39, Step 16302: Loss = 38757.0625, best_valid_rmse = 9.292288936096801\n",
      "Epoch 40, Step 16720: Loss = 38789.7734375, best_valid_rmse = 9.292288936096801\n",
      "Epoch 41, Step 17138: Loss = 35469.46484375, best_valid_rmse = 9.292288936096801\n",
      "early stopping, step 17200, validation RMSE = 9.301678931340964\n",
      "H = 20: Test RMSE = 9.384141162892735\n",
      "\n",
      "Epoch 1, Step 418: Loss = 37766.75390625, best_valid_rmse = 9.389627138400803\n",
      "Epoch 2, Step 836: Loss = 38355.73828125, best_valid_rmse = 9.245848714137475\n",
      "Epoch 3, Step 1254: Loss = 37743.75, best_valid_rmse = 9.19694263626668\n",
      "Epoch 4, Step 1672: Loss = 37656.03125, best_valid_rmse = 9.13900539650094\n",
      "Epoch 5, Step 2090: Loss = 35381.45703125, best_valid_rmse = 9.102826427321821\n",
      "Epoch 6, Step 2508: Loss = 38385.203125, best_valid_rmse = 9.095179718193048\n",
      "Epoch 7, Step 2926: Loss = 36625.3828125, best_valid_rmse = 9.073470480170618\n",
      "Epoch 8, Step 3344: Loss = 37271.01171875, best_valid_rmse = 9.073470480170618\n",
      "Epoch 9, Step 3762: Loss = 35580.6875, best_valid_rmse = 9.05076091285827\n",
      "Epoch 10, Step 4180: Loss = 34036.875, best_valid_rmse = 9.043666040489267\n",
      "Epoch 11, Step 4598: Loss = 38042.27734375, best_valid_rmse = 9.043666040489267\n",
      "Epoch 12, Step 5016: Loss = 37104.515625, best_valid_rmse = 9.022634468917543\n",
      "Epoch 13, Step 5434: Loss = 35556.03125, best_valid_rmse = 9.021431559541645\n",
      "Epoch 14, Step 5852: Loss = 34710.640625, best_valid_rmse = 9.021431559541645\n",
      "Epoch 15, Step 6270: Loss = 35942.890625, best_valid_rmse = 9.014329751229539\n",
      "Epoch 16, Step 6688: Loss = 34809.296875, best_valid_rmse = 9.010344154550493\n",
      "Epoch 17, Step 7106: Loss = 35887.4609375, best_valid_rmse = 9.010344154550493\n",
      "Epoch 18, Step 7524: Loss = 34336.109375, best_valid_rmse = 9.010344154550493\n",
      "Epoch 19, Step 7942: Loss = 33998.4140625, best_valid_rmse = 9.005431593923527\n",
      "Epoch 20, Step 8360: Loss = 35967.84765625, best_valid_rmse = 9.005431593923527\n",
      "Epoch 21, Step 8778: Loss = 35525.5546875, best_valid_rmse = 8.998182349117807\n",
      "Epoch 22, Step 9196: Loss = 36512.49609375, best_valid_rmse = 8.998182349117807\n",
      "Epoch 23, Step 9614: Loss = 39137.11328125, best_valid_rmse = 8.998182349117807\n",
      "Epoch 24, Step 10032: Loss = 35922.65625, best_valid_rmse = 8.989295196730556\n",
      "Epoch 25, Step 10450: Loss = 37552.203125, best_valid_rmse = 8.989295196730556\n",
      "Epoch 26, Step 10868: Loss = 36805.6328125, best_valid_rmse = 8.987893779204981\n",
      "Epoch 27, Step 11286: Loss = 35973.4765625, best_valid_rmse = 8.987893779204981\n",
      "Epoch 28, Step 11704: Loss = 36582.64453125, best_valid_rmse = 8.987893779204981\n",
      "Epoch 29, Step 12122: Loss = 35513.48828125, best_valid_rmse = 8.987893779204981\n",
      "Epoch 30, Step 12540: Loss = 36949.0234375, best_valid_rmse = 8.976019895779967\n",
      "Epoch 31, Step 12958: Loss = 34467.6953125, best_valid_rmse = 8.976019895779967\n",
      "Epoch 32, Step 13376: Loss = 33859.26171875, best_valid_rmse = 8.976019895779967\n",
      "Epoch 33, Step 13794: Loss = 36745.328125, best_valid_rmse = 8.976019895779967\n",
      "Epoch 34, Step 14212: Loss = 35183.1640625, best_valid_rmse = 8.976019895779967\n",
      "Epoch 35, Step 14630: Loss = 36752.73828125, best_valid_rmse = 8.976019895779967\n",
      "Epoch 36, Step 15048: Loss = 36903.7265625, best_valid_rmse = 8.976019895779967\n",
      "Epoch 37, Step 15466: Loss = 36479.15234375, best_valid_rmse = 8.976019895779967\n",
      "Epoch 38, Step 15884: Loss = 36431.25, best_valid_rmse = 8.976019895779967\n",
      "Epoch 39, Step 16302: Loss = 33615.421875, best_valid_rmse = 8.976019895779967\n",
      "Epoch 40, Step 16720: Loss = 35799.4140625, best_valid_rmse = 8.976019895779967\n",
      "Epoch 41, Step 17138: Loss = 37229.68359375, best_valid_rmse = 8.976019895779967\n",
      "early stopping, step 17200, validation RMSE = 9.018814842065643\n",
      "H = 45: Test RMSE = 9.123405036795587\n",
      "\n",
      "Epoch 1, Step 418: Loss = 36516.75, best_valid_rmse = 9.061649533549339\n",
      "Epoch 2, Step 836: Loss = 36989.2734375, best_valid_rmse = 8.94613908334211\n",
      "Epoch 3, Step 1254: Loss = 34375.98828125, best_valid_rmse = 8.886116676642734\n",
      "Epoch 4, Step 1672: Loss = 34164.84765625, best_valid_rmse = 8.860590891668336\n",
      "Epoch 5, Step 2090: Loss = 36468.98828125, best_valid_rmse = 8.821709365164963\n",
      "Epoch 6, Step 2508: Loss = 34918.10546875, best_valid_rmse = 8.807156504412388\n",
      "Epoch 7, Step 2926: Loss = 36318.7578125, best_valid_rmse = 8.803870482498105\n",
      "Epoch 8, Step 3344: Loss = 33731.1953125, best_valid_rmse = 8.787329911831131\n",
      "Epoch 9, Step 3762: Loss = 34989.50390625, best_valid_rmse = 8.774025847732332\n",
      "Epoch 10, Step 4180: Loss = 35390.015625, best_valid_rmse = 8.773700177122642\n",
      "Epoch 11, Step 4598: Loss = 33561.50390625, best_valid_rmse = 8.773700177122642\n",
      "Epoch 12, Step 5016: Loss = 34468.4921875, best_valid_rmse = 8.766428429440898\n",
      "Epoch 13, Step 5434: Loss = 35798.4609375, best_valid_rmse = 8.766428429440898\n",
      "Epoch 14, Step 5852: Loss = 32876.03515625, best_valid_rmse = 8.755958670059082\n",
      "Epoch 15, Step 6270: Loss = 31079.58984375, best_valid_rmse = 8.755958670059082\n",
      "Epoch 16, Step 6688: Loss = 31750.421875, best_valid_rmse = 8.755958670059082\n",
      "Epoch 17, Step 7106: Loss = 31217.921875, best_valid_rmse = 8.745686281291066\n",
      "Epoch 18, Step 7524: Loss = 34395.65625, best_valid_rmse = 8.745686281291066\n",
      "Epoch 19, Step 7942: Loss = 32321.634765625, best_valid_rmse = 8.745686281291066\n",
      "Epoch 20, Step 8360: Loss = 30931.74609375, best_valid_rmse = 8.745686281291066\n",
      "Epoch 21, Step 8778: Loss = 32426.37109375, best_valid_rmse = 8.745686281291066\n",
      "Epoch 22, Step 9196: Loss = 32358.263671875, best_valid_rmse = 8.733287412501104\n",
      "Epoch 23, Step 9614: Loss = 31151.6875, best_valid_rmse = 8.733287412501104\n",
      "Epoch 24, Step 10032: Loss = 33581.796875, best_valid_rmse = 8.733287412501104\n",
      "Epoch 25, Step 10450: Loss = 31040.943359375, best_valid_rmse = 8.733287412501104\n",
      "Epoch 26, Step 10868: Loss = 32125.068359375, best_valid_rmse = 8.733287412501104\n",
      "Epoch 27, Step 11286: Loss = 31535.75, best_valid_rmse = 8.733287412501104\n",
      "Epoch 28, Step 11704: Loss = 30701.021484375, best_valid_rmse = 8.733287412501104\n",
      "Epoch 29, Step 12122: Loss = 32268.3046875, best_valid_rmse = 8.733287412501104\n",
      "Epoch 30, Step 12540: Loss = 31177.8359375, best_valid_rmse = 8.733287412501104\n",
      "Epoch 31, Step 12958: Loss = 32122.1484375, best_valid_rmse = 8.733287412501104\n",
      "Epoch 32, Step 13376: Loss = 30802.775390625, best_valid_rmse = 8.733276917923188\n",
      "Epoch 33, Step 13794: Loss = 30314.57421875, best_valid_rmse = 8.733276917923188\n",
      "Epoch 34, Step 14212: Loss = 29242.515625, best_valid_rmse = 8.731131746645385\n",
      "Epoch 35, Step 14630: Loss = 30225.517578125, best_valid_rmse = 8.731131746645385\n",
      "Epoch 36, Step 15048: Loss = 29822.728515625, best_valid_rmse = 8.721132104087317\n",
      "Epoch 37, Step 15466: Loss = 31687.328125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 38, Step 15884: Loss = 28296.55078125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 39, Step 16302: Loss = 28884.92578125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 40, Step 16720: Loss = 34396.125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 41, Step 17138: Loss = 30390.72265625, best_valid_rmse = 8.721132104087317\n",
      "Epoch 42, Step 17556: Loss = 31533.015625, best_valid_rmse = 8.721132104087317\n",
      "Epoch 43, Step 17974: Loss = 28280.232421875, best_valid_rmse = 8.721132104087317\n",
      "Epoch 44, Step 18392: Loss = 31004.517578125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 45, Step 18810: Loss = 31844.267578125, best_valid_rmse = 8.721132104087317\n",
      "Epoch 46, Step 19228: Loss = 32471.375, best_valid_rmse = 8.721132104087317\n",
      "Epoch 47, Step 19646: Loss = 31636.248046875, best_valid_rmse = 8.721132104087317\n",
      "early stopping, step 19800, validation RMSE = 8.749034518834238\n",
      "H = 180: Test RMSE = 9.00075103931562\n",
      "\n",
      "Epoch 1, Step 418: Loss = 35608.92578125, best_valid_rmse = 9.001491170115884\n",
      "Epoch 2, Step 836: Loss = 35315.859375, best_valid_rmse = 8.878200128113022\n",
      "Epoch 3, Step 1254: Loss = 34476.1484375, best_valid_rmse = 8.819138535186656\n",
      "Epoch 4, Step 1672: Loss = 33937.52734375, best_valid_rmse = 8.783886100209712\n",
      "Epoch 5, Step 2090: Loss = 34343.15625, best_valid_rmse = 8.762482970279985\n",
      "Epoch 6, Step 2508: Loss = 32163.216796875, best_valid_rmse = 8.752194523277046\n",
      "Epoch 7, Step 2926: Loss = 33314.55078125, best_valid_rmse = 8.744276957212167\n",
      "Epoch 8, Step 3344: Loss = 33160.7734375, best_valid_rmse = 8.730190964942219\n",
      "Epoch 9, Step 3762: Loss = 32076.58203125, best_valid_rmse = 8.719205926167838\n",
      "Epoch 10, Step 4180: Loss = 31774.453125, best_valid_rmse = 8.691122455727196\n",
      "Epoch 11, Step 4598: Loss = 31893.72265625, best_valid_rmse = 8.686228282374476\n",
      "Epoch 12, Step 5016: Loss = 33366.546875, best_valid_rmse = 8.686228282374476\n",
      "Epoch 13, Step 5434: Loss = 32553.830078125, best_valid_rmse = 8.686228282374476\n",
      "Epoch 14, Step 5852: Loss = 32460.298828125, best_valid_rmse = 8.686228282374476\n",
      "Epoch 15, Step 6270: Loss = 29339.984375, best_valid_rmse = 8.684844383416356\n",
      "Epoch 16, Step 6688: Loss = 30111.5859375, best_valid_rmse = 8.654968981091756\n",
      "Epoch 17, Step 7106: Loss = 30227.763671875, best_valid_rmse = 8.654968981091756\n",
      "Epoch 18, Step 7524: Loss = 30722.46484375, best_valid_rmse = 8.654968981091756\n",
      "Epoch 19, Step 7942: Loss = 30613.650390625, best_valid_rmse = 8.654968981091756\n",
      "Epoch 20, Step 8360: Loss = 29621.103515625, best_valid_rmse = 8.654968981091756\n",
      "Epoch 21, Step 8778: Loss = 30992.7890625, best_valid_rmse = 8.654968981091756\n",
      "Epoch 22, Step 9196: Loss = 32969.6328125, best_valid_rmse = 8.654968981091756\n",
      "Epoch 23, Step 9614: Loss = 29754.5859375, best_valid_rmse = 8.654968981091756\n",
      "Epoch 24, Step 10032: Loss = 30970.873046875, best_valid_rmse = 8.654968981091756\n",
      "Epoch 25, Step 10450: Loss = 28240.841796875, best_valid_rmse = 8.654968981091756\n",
      "Epoch 26, Step 10868: Loss = 33438.4375, best_valid_rmse = 8.654968981091756\n",
      "Epoch 27, Step 11286: Loss = 30044.19140625, best_valid_rmse = 8.654968981091756\n",
      "early stopping, step 11500, validation RMSE = 8.697770392914155\n",
      "H = 360: Test RMSE = 8.936875654569297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d_hidden in [20, 45, 180, 360]:\n",
    "    \n",
    "    d_input = subtrain_set.Xnp.shape[1]\n",
    "    d_output = 1\n",
    "\n",
    "    mlp = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d_input, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "\n",
    "        torch.nn.Linear(d_hidden, d_output)\n",
    "    )\n",
    "    mlp = mlp.float()\n",
    "    mlp.to(device)\n",
    "    weight_path = './MLP{d_hidden}_drop_weight'\n",
    "\n",
    "    # optimizer\n",
    "    lr = 0.001\n",
    "    momentum = 0\n",
    "    weight_decay = 0\n",
    "    adam_optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # loss\n",
    "    l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    # train\n",
    "    all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=adam_optimizer, loss_f=l2_loss, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=False)        \n",
    "    \n",
    "    # test RMSE\n",
    "    saved_mlp = torch.load(weight_path)\n",
    "    test_rmse = RMSE(saved_mlp, test_loader)\n",
    "    print(f'H = {d_hidden}: Test RMSE = {test_rmse}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H = 20, Test RMSE = 9.384  \n",
    "H = 45, Test RMSE = 9.123  \n",
    "H = 180, Test RMSE = 9.001  \n",
    "H = 360, Test RMSE = 8.937  \n",
    "The test RMSE is the lowest when H = 360."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. L2 + L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, loss_f1, loss_f2, loss_w, max_epoch, max_step, valid_interval, weight_path, train_rmse):\n",
    "    \n",
    "    step_count = 0\n",
    "    best_step_count = 0\n",
    "    best_valid_rmse = np.inf\n",
    "    all_train_rmse = []\n",
    "    all_valid_rmse = []\n",
    "    all_step = []\n",
    "\n",
    "    for epoch in range(1, max_epoch+1):\n",
    "        for batch, (inputs, targets) in enumerate(subtrain_loader):\n",
    "\n",
    "            targets = targets.reshape((-1, 1))\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            model.to(device)\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_w * loss_f1(outputs, targets) + (1-loss_w) * loss_f2(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            step_count += 1\n",
    "\n",
    "            # check train/validation RMSE\n",
    "            if (step_count % valid_interval == 0):\n",
    "\n",
    "                # subtrain, validation RMSE\n",
    "                if train_rmse:\n",
    "                    train_rmse = RMSE(model, subtrain_loader)\n",
    "                    all_train_rmse.append(train_rmse)\n",
    "                valid_rmse = RMSE(model, valid_loader)\n",
    "                all_valid_rmse.append(valid_rmse)\n",
    "                all_step.append(step_count)\n",
    "\n",
    "                # update weight\n",
    "                if valid_rmse < best_valid_rmse:\n",
    "                    best_step_count = step_count\n",
    "                    best_valid_rmse = valid_rmse\n",
    "                    torch.save(model, weight_path)\n",
    "\n",
    "                # early stopping\n",
    "                elif (step_count - best_step_count >= max_step):\n",
    "                    print(f'early stopping, step {step_count}, validation RMSE = {valid_rmse}')\n",
    "                    return all_step, all_train_rmse, all_valid_rmse\n",
    "                \n",
    "        print(f'Epoch {epoch}, Step {step_count}: Loss = {loss.item()}, best_valid_rmse = {best_valid_rmse}')\n",
    "    return all_step, all_train_rmse, all_valid_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 418: Loss = 19323.93359375, best_valid_rmse = 9.200106430690298\n",
      "Epoch 2, Step 836: Loss = 18682.15625, best_valid_rmse = 9.057800825567327\n",
      "Epoch 3, Step 1254: Loss = 19214.287109375, best_valid_rmse = 9.01049673140981\n",
      "Epoch 4, Step 1672: Loss = 17891.123046875, best_valid_rmse = 8.953730029851044\n",
      "Epoch 5, Step 2090: Loss = 18084.068359375, best_valid_rmse = 8.938734696920385\n",
      "Epoch 6, Step 2508: Loss = 18426.744140625, best_valid_rmse = 8.927392707710137\n",
      "Epoch 7, Step 2926: Loss = 18986.419921875, best_valid_rmse = 8.915697666293697\n",
      "Epoch 8, Step 3344: Loss = 18565.76953125, best_valid_rmse = 8.885472023316417\n",
      "Epoch 9, Step 3762: Loss = 18317.76953125, best_valid_rmse = 8.885472023316417\n",
      "Epoch 10, Step 4180: Loss = 18999.03125, best_valid_rmse = 8.876433457642245\n",
      "Epoch 11, Step 4598: Loss = 18376.2890625, best_valid_rmse = 8.865604387099769\n",
      "Epoch 12, Step 5016: Loss = 17357.92578125, best_valid_rmse = 8.865604387099769\n",
      "Epoch 13, Step 5434: Loss = 18525.279296875, best_valid_rmse = 8.865604387099769\n"
     ]
    }
   ],
   "source": [
    "d_hidden = 90\n",
    "d_input = subtrain_set.Xnp.shape[1]\n",
    "d_output = 1\n",
    "\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d_input, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    \n",
    "    torch.nn.Linear(d_hidden, d_output)\n",
    ")\n",
    "mlp = mlp.float()\n",
    "mlp.to(device)\n",
    "weight_path = './MLP90_drop_weight'\n",
    "\n",
    "# optimizer\n",
    "lr = 0.001\n",
    "momentum = 0\n",
    "weight_decay = 0\n",
    "adam_optimizer = torch.optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# loss\n",
    "z = 0.5\n",
    "l2_loss = torch.nn.MSELoss(reduction='sum')\n",
    "l1_loss = torch.nn.L1Loss(reduction='sum')\n",
    "\n",
    "# train\n",
    "all_step, all_train_rmse, all_valid_rmse = train(model=mlp, optim=adam_optimizer, loss_f1=l2_loss, loss_f2=l1_loss, loss_w=z, max_epoch=100, max_step=5000, valid_interval=100, weight_path=weight_path, train_rmse=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
