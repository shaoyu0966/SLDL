{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning and Deep Learning HW5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['train', 'valid' ,'test']\n",
    "labels = ['blazer', 'cardigan', 'coat', 'jacket']\n",
    "base_path = '/tmp2/b06705028/sldl'\n",
    "data_path = f'{base_path}/photos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      blazer cardigan coat jacket  total\n",
      "train     97      237  296    411   1041\n",
      "valid      7       36   27     35    105\n",
      "test       9       42   43     52    146\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=labels, index=datasets)\n",
    "for ds in datasets:\n",
    "    for lb in labels:\n",
    "        basepath = os.path.join(f'{data_path}/{ds}/{lb}/', '*.jpg')\n",
    "        cand_fn = glob.glob(basepath)\n",
    "        df[lb][ds] = len(cand_fn)\n",
    "df['total'] = df.sum(axis=1).astype('int')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio:\n",
      "         blazer  cardigan      coat    jacket\n",
      "train  0.093180  0.227666  0.284342  0.394813\n",
      "valid  0.066667  0.342857  0.257143  0.333333\n",
      "test   0.061644  0.287671  0.294521  0.356164\n"
     ]
    }
   ],
   "source": [
    "print('Ratio:')\n",
    "df = df.drop(['total'], axis=1)\n",
    "print (df.div(df.sum(axis=1), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the number of instances of each image type, I suggest that the accuracy of the classification task will be jacket > coat > cardigan > blazer. This follows the hypothesis that larger number of instances in training set causes higher classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "image_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.RandomResizedCrop(size=(224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=20),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \n",
    "    'valid':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets from folders\n",
    "data = {\n",
    "    'train':\n",
    "        ImageFolder(root=f'{data_path}/train/', transform=image_transforms['train']),\n",
    "    'valid':\n",
    "        ImageFolder(root=f'{data_path}/valid/', transform=image_transforms['valid']),\n",
    "    'test':\n",
    "        ImageFolder(root=f'{data_path}/test/', transform=image_transforms['test'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "batch_size = 32\n",
    "dataloaders = {\n",
    "    'train':\n",
    "        DataLoader(data['train'], batch_size=batch_size, shuffle=True, num_workers=10),\n",
    "    'valid':\n",
    "        DataLoader(data['valid'], batch_size=batch_size, shuffle=True, num_workers=10),\n",
    "    'test':\n",
    "        DataLoader(data['test'], batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'using {device}')\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(model, data_loader):\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss += loss_fn(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optim, model, weight_path, fix_weight=True, early_stop_patient=20, max_epoch=200):\n",
    "    best_valid_loss = np.inf\n",
    "    best_valid_epoch = 0\n",
    "    for epoch in range(max_epoch):\n",
    "        # train\n",
    "        train_loss = 0\n",
    "        for batch, (inputs, targets) in enumerate(dataloaders['train']):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = cross_entropy_loss(model, dataloaders['valid']).cpu().numpy()\n",
    " \n",
    "        # update weight if lower validation loss\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_epoch = epoch\n",
    "            torch.save(model, weight_path)\n",
    "        # early stopping\n",
    "        elif (epoch - best_valid_epoch >= early_stop_patient):\n",
    "            print(f'early stopping at epoch {epoch}')\n",
    "            return\n",
    "        \n",
    "        if (epoch % 10 == 0):\n",
    "            print(f'epoch {epoch}: train_loss = {train_loss:.3f}, valid_loss = {valid_loss:.3f}, best_valid = {best_valid_loss:.3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================\n",
      "train SGD lr = 0.0001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 44.137, valid_loss = 5.291, best_valid = 5.291\n",
      "epoch 10: train_loss = 42.456, valid_loss = 5.137, best_valid = 5.095\n",
      "epoch 20: train_loss = 42.392, valid_loss = 5.001, best_valid = 4.957\n",
      "epoch 30: train_loss = 41.907, valid_loss = 4.882, best_valid = 4.882\n",
      "epoch 40: train_loss = 41.410, valid_loss = 4.913, best_valid = 4.870\n",
      "epoch 50: train_loss = 41.360, valid_loss = 4.836, best_valid = 4.836\n",
      "epoch 60: train_loss = 40.971, valid_loss = 4.916, best_valid = 4.770\n",
      "epoch 70: train_loss = 40.790, valid_loss = 4.813, best_valid = 4.768\n",
      "epoch 80: train_loss = 40.469, valid_loss = 4.840, best_valid = 4.730\n",
      "epoch 90: train_loss = 40.226, valid_loss = 4.887, best_valid = 4.721\n",
      "epoch 100: train_loss = 40.127, valid_loss = 4.795, best_valid = 4.559\n",
      "epoch 110: train_loss = 39.941, valid_loss = 4.727, best_valid = 4.559\n",
      "early stopping at epoch 119\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.0005\n",
      "model at cuda\n",
      "epoch 0: train_loss = 43.348, valid_loss = 5.135, best_valid = 5.135\n",
      "epoch 10: train_loss = 40.885, valid_loss = 4.935, best_valid = 4.885\n",
      "epoch 20: train_loss = 40.142, valid_loss = 4.932, best_valid = 4.755\n",
      "epoch 30: train_loss = 38.999, valid_loss = 4.624, best_valid = 4.579\n",
      "epoch 40: train_loss = 38.019, valid_loss = 4.575, best_valid = 4.518\n",
      "epoch 50: train_loss = 37.552, valid_loss = 4.425, best_valid = 4.343\n",
      "epoch 60: train_loss = 36.920, valid_loss = 4.266, best_valid = 4.230\n",
      "epoch 70: train_loss = 36.613, valid_loss = 4.443, best_valid = 4.162\n",
      "epoch 80: train_loss = 36.672, valid_loss = 4.022, best_valid = 4.022\n",
      "epoch 90: train_loss = 35.910, valid_loss = 4.172, best_valid = 4.022\n",
      "epoch 100: train_loss = 35.704, valid_loss = 4.220, best_valid = 3.993\n",
      "epoch 110: train_loss = 35.208, valid_loss = 3.905, best_valid = 3.876\n",
      "epoch 120: train_loss = 35.322, valid_loss = 4.381, best_valid = 3.876\n",
      "epoch 130: train_loss = 34.766, valid_loss = 4.296, best_valid = 3.797\n",
      "epoch 140: train_loss = 34.586, valid_loss = 4.098, best_valid = 3.797\n",
      "epoch 150: train_loss = 34.521, valid_loss = 3.825, best_valid = 3.662\n",
      "epoch 160: train_loss = 34.047, valid_loss = 4.061, best_valid = 3.662\n",
      "early stopping at epoch 165\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 43.392, valid_loss = 5.233, best_valid = 5.233\n",
      "epoch 10: train_loss = 40.281, valid_loss = 5.022, best_valid = 4.784\n",
      "epoch 20: train_loss = 38.568, valid_loss = 4.560, best_valid = 4.451\n",
      "epoch 30: train_loss = 37.218, valid_loss = 4.404, best_valid = 4.289\n",
      "epoch 40: train_loss = 36.833, valid_loss = 4.298, best_valid = 4.207\n",
      "epoch 50: train_loss = 35.481, valid_loss = 4.202, best_valid = 4.036\n",
      "epoch 60: train_loss = 35.170, valid_loss = 4.058, best_valid = 3.941\n",
      "epoch 70: train_loss = 34.698, valid_loss = 3.946, best_valid = 3.858\n",
      "epoch 80: train_loss = 34.646, valid_loss = 4.168, best_valid = 3.803\n",
      "epoch 90: train_loss = 34.205, valid_loss = 3.681, best_valid = 3.681\n",
      "epoch 100: train_loss = 33.342, valid_loss = 3.990, best_valid = 3.681\n",
      "early stopping at epoch 110\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.005\n",
      "model at cuda\n",
      "epoch 0: train_loss = 42.441, valid_loss = 5.152, best_valid = 5.152\n",
      "epoch 10: train_loss = 36.463, valid_loss = 4.133, best_valid = 3.943\n",
      "epoch 20: train_loss = 33.667, valid_loss = 3.850, best_valid = 3.801\n",
      "epoch 30: train_loss = 33.065, valid_loss = 3.695, best_valid = 3.594\n",
      "epoch 40: train_loss = 32.724, valid_loss = 3.405, best_valid = 3.405\n",
      "epoch 50: train_loss = 30.809, valid_loss = 3.573, best_valid = 3.405\n",
      "early stopping at epoch 60\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.01\n",
      "model at cuda\n",
      "epoch 0: train_loss = 43.065, valid_loss = 5.056, best_valid = 5.056\n",
      "epoch 10: train_loss = 35.189, valid_loss = 3.740, best_valid = 3.740\n",
      "epoch 20: train_loss = 35.321, valid_loss = 4.042, best_valid = 3.648\n",
      "epoch 30: train_loss = 32.163, valid_loss = 3.937, best_valid = 3.454\n",
      "epoch 40: train_loss = 31.817, valid_loss = 3.521, best_valid = 3.396\n",
      "epoch 50: train_loss = 32.023, valid_loss = 4.097, best_valid = 3.396\n",
      "early stopping at epoch 52\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.1\n",
      "model at cuda\n",
      "epoch 0: train_loss = 377.606, valid_loss = 67.024, best_valid = 67.024\n",
      "epoch 10: train_loss = 200.144, valid_loss = 32.612, best_valid = 21.144\n",
      "epoch 20: train_loss = 198.155, valid_loss = 14.938, best_valid = 9.659\n",
      "epoch 30: train_loss = 193.389, valid_loss = 41.125, best_valid = 9.659\n",
      "early stopping at epoch 36\n"
     ]
    }
   ],
   "source": [
    "all_lr = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]\n",
    "for lr in all_lr:\n",
    "    print('\\n============================================')\n",
    "    print(f'train SGD lr = {lr}')\n",
    "    weight_path = f'{base_path}/Q2_weight_{lr}'\n",
    "    \n",
    "    # load pretrained resnet50 and set the output dimension to 4\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "    model.to(device)\n",
    "    print(f'model at {device}')\n",
    "    \n",
    "    # train\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    train(optim, model, weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /tmp2/b06705028/sldl/Q2_weight_0.0001\n",
      "SGD, lr = 0.0001: test accuracy:  0.4452054794520548\n",
      "loading /tmp2/b06705028/sldl/Q2_weight_0.0005\n",
      "SGD, lr = 0.0005: test accuracy:  0.589041095890411\n",
      "loading /tmp2/b06705028/sldl/Q2_weight_0.001\n",
      "SGD, lr = 0.001: test accuracy:  0.5616438356164384\n",
      "loading /tmp2/b06705028/sldl/Q2_weight_0.005\n",
      "SGD, lr = 0.005: test accuracy:  0.636986301369863\n",
      "loading /tmp2/b06705028/sldl/Q2_weight_0.01\n",
      "SGD, lr = 0.01: test accuracy:  0.6301369863013698\n",
      "loading /tmp2/b06705028/sldl/Q2_weight_0.1\n",
      "SGD, lr = 0.1: test accuracy:  0.6027397260273972\n"
     ]
    }
   ],
   "source": [
    "for lr in all_lr:\n",
    "    weight_path = f'{base_path}/Q2_weight_{lr}'\n",
    "    print(f'loading {weight_path}')\n",
    "    saved_model = torch.load(weight_path)\n",
    "    test_size = len(data['test'])\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, targets) in enumerate(dataloaders['test']):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = saved_model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct = (targets==preds).cpu().numpy()\n",
    "            n_correct += np.sum(correct)\n",
    "    print(f'SGD, lr = {lr}: test accuracy: ', n_correct / len(data['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================\n",
      "train Adam lr = 0.0001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 44.324, valid_loss = 5.541, best_valid = 5.541\n",
      "epoch 10: train_loss = 42.422, valid_loss = 5.110, best_valid = 5.042\n",
      "epoch 20: train_loss = 41.866, valid_loss = 5.101, best_valid = 4.947\n",
      "epoch 30: train_loss = 41.413, valid_loss = 5.056, best_valid = 4.947\n",
      "epoch 40: train_loss = 41.364, valid_loss = 5.018, best_valid = 4.908\n",
      "epoch 50: train_loss = 40.940, valid_loss = 5.209, best_valid = 4.860\n",
      "epoch 60: train_loss = 40.510, valid_loss = 5.050, best_valid = 4.848\n",
      "epoch 70: train_loss = 40.604, valid_loss = 5.007, best_valid = 4.779\n",
      "epoch 80: train_loss = 40.410, valid_loss = 5.018, best_valid = 4.678\n",
      "epoch 90: train_loss = 40.117, valid_loss = 4.994, best_valid = 4.678\n",
      "early stopping at epoch 92\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.0005\n",
      "model at cuda\n",
      "epoch 0: train_loss = 44.951, valid_loss = 5.433, best_valid = 5.433\n",
      "epoch 10: train_loss = 41.549, valid_loss = 4.995, best_valid = 4.995\n",
      "epoch 20: train_loss = 40.325, valid_loss = 4.714, best_valid = 4.714\n",
      "epoch 30: train_loss = 39.526, valid_loss = 4.710, best_valid = 4.451\n",
      "epoch 40: train_loss = 38.616, valid_loss = 4.474, best_valid = 4.423\n",
      "epoch 50: train_loss = 37.705, valid_loss = 4.496, best_valid = 4.423\n",
      "epoch 60: train_loss = 37.087, valid_loss = 4.283, best_valid = 4.175\n",
      "epoch 70: train_loss = 36.702, valid_loss = 4.242, best_valid = 4.175\n",
      "epoch 80: train_loss = 37.005, valid_loss = 4.177, best_valid = 4.127\n",
      "epoch 90: train_loss = 36.409, valid_loss = 4.177, best_valid = 4.044\n",
      "epoch 100: train_loss = 35.698, valid_loss = 4.129, best_valid = 3.937\n",
      "epoch 110: train_loss = 35.580, valid_loss = 4.039, best_valid = 3.937\n",
      "epoch 120: train_loss = 35.664, valid_loss = 4.195, best_valid = 3.937\n",
      "epoch 130: train_loss = 35.037, valid_loss = 3.971, best_valid = 3.924\n",
      "epoch 140: train_loss = 34.576, valid_loss = 3.970, best_valid = 3.842\n",
      "epoch 150: train_loss = 35.224, valid_loss = 4.244, best_valid = 3.749\n",
      "epoch 160: train_loss = 34.857, valid_loss = 4.036, best_valid = 3.749\n",
      "early stopping at epoch 165\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 43.117, valid_loss = 5.236, best_valid = 5.236\n",
      "epoch 10: train_loss = 39.879, valid_loss = 4.610, best_valid = 4.610\n",
      "epoch 20: train_loss = 38.366, valid_loss = 4.618, best_valid = 4.384\n",
      "epoch 30: train_loss = 37.069, valid_loss = 4.178, best_valid = 4.178\n",
      "epoch 40: train_loss = 36.342, valid_loss = 4.440, best_valid = 4.106\n",
      "epoch 50: train_loss = 35.160, valid_loss = 4.298, best_valid = 3.969\n",
      "epoch 60: train_loss = 35.160, valid_loss = 4.009, best_valid = 3.846\n",
      "epoch 70: train_loss = 34.066, valid_loss = 3.943, best_valid = 3.824\n",
      "epoch 80: train_loss = 34.629, valid_loss = 4.062, best_valid = 3.824\n",
      "epoch 90: train_loss = 33.424, valid_loss = 3.643, best_valid = 3.599\n",
      "epoch 100: train_loss = 33.448, valid_loss = 3.775, best_valid = 3.599\n",
      "early stopping at epoch 104\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.005\n",
      "model at cuda\n",
      "epoch 0: train_loss = 42.455, valid_loss = 4.735, best_valid = 4.735\n",
      "epoch 10: train_loss = 35.739, valid_loss = 4.313, best_valid = 4.091\n",
      "epoch 20: train_loss = 34.808, valid_loss = 4.020, best_valid = 3.757\n",
      "epoch 30: train_loss = 32.929, valid_loss = 3.393, best_valid = 3.393\n",
      "epoch 40: train_loss = 31.982, valid_loss = 3.799, best_valid = 3.393\n",
      "epoch 50: train_loss = 31.811, valid_loss = 3.912, best_valid = 3.389\n",
      "epoch 60: train_loss = 31.378, valid_loss = 3.724, best_valid = 3.389\n",
      "early stopping at epoch 66\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.01\n",
      "model at cuda\n",
      "epoch 0: train_loss = 44.538, valid_loss = 6.215, best_valid = 6.215\n",
      "epoch 10: train_loss = 36.079, valid_loss = 3.768, best_valid = 3.768\n",
      "epoch 20: train_loss = 34.313, valid_loss = 4.070, best_valid = 3.502\n",
      "epoch 30: train_loss = 31.904, valid_loss = 3.459, best_valid = 3.390\n",
      "epoch 40: train_loss = 32.288, valid_loss = 3.923, best_valid = 3.334\n",
      "epoch 50: train_loss = 33.501, valid_loss = 3.708, best_valid = 3.334\n",
      "early stopping at epoch 54\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.1\n",
      "model at cuda\n",
      "epoch 0: train_loss = 423.191, valid_loss = 15.449, best_valid = 15.449\n",
      "epoch 10: train_loss = 250.591, valid_loss = 39.729, best_valid = 11.492\n",
      "epoch 20: train_loss = 184.656, valid_loss = 16.573, best_valid = 11.492\n",
      "early stopping at epoch 26\n"
     ]
    }
   ],
   "source": [
    "for lr in all_lr:\n",
    "    print('\\n============================================')\n",
    "    print(f'train Adam lr = {lr}')\n",
    "    weight_path = f'{base_path}/Q2_weight_adam_{lr}'\n",
    "    \n",
    "    # load pretrained resnet50 and set the output dimension to 4\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "    model.to(device)\n",
    "    print(f'model at {device}')\n",
    "    \n",
    "    # train\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    train(optim, model, weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /tmp2/b06705028/sldl/Q2_weight_adam_0.0001\n",
      "SGD, lr = 0.0001: valid accuracy:  0.4041095890410959\n",
      "loading /tmp2/b06705028/sldl/Q2_weight_adam_0.0005\n",
      "SGD, lr = 0.0005: valid accuracy:  0.589041095890411\n",
      "loading /tmp2/b06705028/sldl/Q2_weight_adam_0.001\n",
      "SGD, lr = 0.001: valid accuracy:  0.5753424657534246\n",
      "loading /tmp2/b06705028/sldl/Q2_weight_adam_0.005\n",
      "SGD, lr = 0.005: valid accuracy:  0.6164383561643836\n",
      "loading /tmp2/b06705028/sldl/Q2_weight_adam_0.01\n",
      "SGD, lr = 0.01: valid accuracy:  0.6301369863013698\n",
      "loading /tmp2/b06705028/sldl/Q2_weight_adam_0.1\n",
      "SGD, lr = 0.1: valid accuracy:  0.5753424657534246\n"
     ]
    }
   ],
   "source": [
    "for lr in all_lr:\n",
    "    weight_path = f'{base_path}/Q2_weight_adam_{lr}'\n",
    "    print(f'loading {weight_path}')\n",
    "    saved_model = torch.load(weight_path)\n",
    "    test_size = len(data['test'])\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, targets) in enumerate(dataloaders['test']):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = saved_model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct = (targets==preds).cpu().numpy()\n",
    "            n_correct += np.sum(correct)\n",
    "    print(f'SGD, lr = {lr}: valid accuracy: ', n_correct / len(data['test']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
