{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning and Deep Learning HW5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['train', 'valid' ,'test']\n",
    "labels = ['blazer', 'cardigan', 'coat', 'jacket']\n",
    "base_path = '/tmp2/b06705028/sldl'\n",
    "data_path = f'{base_path}/photos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      blazer cardigan coat jacket  total\n",
      "train     97      237  296    411   1041\n",
      "valid      7       36   27     35    105\n",
      "test       9       42   43     52    146\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=labels, index=datasets)\n",
    "for ds in datasets:\n",
    "    for lb in labels:\n",
    "        basepath = os.path.join(f'{data_path}/{ds}/{lb}/', '*.jpg')\n",
    "        cand_fn = glob.glob(basepath)\n",
    "        df[lb][ds] = len(cand_fn)\n",
    "df['total'] = df.sum(axis=1).astype('int')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio:\n",
      "         blazer  cardigan      coat    jacket\n",
      "train  0.093180  0.227666  0.284342  0.394813\n",
      "valid  0.066667  0.342857  0.257143  0.333333\n",
      "test   0.061644  0.287671  0.294521  0.356164\n"
     ]
    }
   ],
   "source": [
    "print('Ratio:')\n",
    "df = df.drop(['total'], axis=1)\n",
    "print (df.div(df.sum(axis=1), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the number of instances of each image type, I suggest that the accuracy of the classification task will be jacket > coat > cardigan > blazer. This follows the hypothesis that larger number of instances in training set causes higher classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "image_transforms = {\n",
    "    'train':\n",
    "        transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.RandomResizedCrop(size=(224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=20),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \n",
    "    'valid':\n",
    "        transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \n",
    "    'test':\n",
    "        transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets from folders\n",
    "data = {\n",
    "    'train': ImageFolder(root=f'{data_path}/train/', transform=image_transforms['train']),\n",
    "    'valid': ImageFolder(root=f'{data_path}/valid/', transform=image_transforms['valid']),\n",
    "    'test': ImageFolder(root=f'{data_path}/test/', transform=image_transforms['test'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "batch_size = 32\n",
    "dataloaders = {\n",
    "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'valid': DataLoader(data['valid'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 0\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'using {device} {torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# hyper-parameters\n",
    "all_lr = [0.001, 0.005, 0.01, 0.05]\n",
    "all_wd = [0, 0.0001, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(model, data_loader):\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss += loss_fn(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optim, model, weight_path, fix_weight=True, early_stop_patient=20, max_epoch=200):\n",
    "    best_valid_loss = np.inf\n",
    "    best_valid_epoch = 0\n",
    "    for epoch in range(max_epoch):\n",
    "        # train\n",
    "        train_loss = 0\n",
    "        for batch, (inputs, targets) in enumerate(dataloaders['train']):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = cross_entropy_loss(model, dataloaders['valid']).cpu().numpy()\n",
    " \n",
    "        # update weight if lower validation loss\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_epoch = epoch\n",
    "            torch.save(model, weight_path)\n",
    "            \n",
    "        # early stopping\n",
    "        elif (epoch - best_valid_epoch >= early_stop_patient):\n",
    "            print(f'early stopping at epoch {epoch}, best_valid = {best_valid_loss:.3f}')\n",
    "            return best_valid_loss\n",
    "        \n",
    "        if (epoch % 20 == 0):\n",
    "            print(f'epoch {epoch}: train_loss = {train_loss:.3f}, valid_loss = {valid_loss:.3f}, best_valid = {best_valid_loss:.3f}')\n",
    "    \n",
    "    return best_valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f64a86de7839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msgd_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_wd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_wd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n============================================'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "sgd_valid = pd.DataFrame(columns=all_lr, index=all_wd)\n",
    "\n",
    "for lr in all_lr:\n",
    "    for wd in all_wd:\n",
    "        print('\\n============================================')\n",
    "        print(f'train SGD lr = {lr}, weight_decay = {wd}')\n",
    "        weight_path = f'{base_path}/Q2_weight_{lr}_{wd}'\n",
    "\n",
    "        # load pretrained resnet50 and set the output dimension to 4\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        # for param in model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "        model.to(device)\n",
    "        print(f'model at {device}')\n",
    "\n",
    "        # train\n",
    "        optim = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        valid_loss = train(optim, model, weight_path)\n",
    "        sgd_valid[lr][wd] = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sgd_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-556262bdb248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(x_axis: learning rate)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(y_axis: weight decay)\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# minimum = sgd_valid.min(axis=0).min()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sgd_valid' is not defined"
     ]
    }
   ],
   "source": [
    "print(sgd_valid)\n",
    "print('(x_axis: learning rate)')\n",
    "print('(y_axis: weight decay)\\n')\n",
    "\n",
    "# minimum = sgd_valid.min(axis=0).min()\n",
    "# sgd_valid = sgd_valid.astype(float)\n",
    "# print(sgd_valid.idxmin(axis=0).idxmin(axis=1))\n",
    "sgd_min_wd = -1\n",
    "sgd_min_lr = -1\n",
    "sgd_minimum = np.inf\n",
    "for lr in sgd_valid.columns:\n",
    "    for wd in sgd_valid.index:\n",
    "        if sgd_valid[lr][wd] < sgd_minimum:\n",
    "            sgd_min_wd = wd\n",
    "            sgd_min_lr = lr\n",
    "            sgd_minimum = sgd_valid[lr][wd]\n",
    "print(f'minimum validation loss {sgd_minimum:.3f} at lr={sgd_min_lr}, wd={sgd_min_wd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lr in all_lr:\n",
    "#     weight_path = f'{base_path}/Q2_weight_{lr}'\n",
    "#     saved_model = torch.load(weight_path)\n",
    "#     test_size = len(data['test'])\n",
    "#     n_correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch, (inputs, targets) in enumerate(dataloaders['test']):\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = saved_model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             correct = (targets==preds).cpu().numpy()\n",
    "#             n_correct += np.sum(correct)\n",
    "#     print(f'SGD, lr = {lr}: test accuracy: ', n_correct / len(data['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = f'{base_path}/Q2_weight_{min_lr}_{min_wd}'\n",
    "print(f'load {weight_path}')\n",
    "saved_model = torch.load(weight_path)\n",
    "test_size = len(data['test'])\n",
    "n_correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch, (inputs, targets) in enumerate(dataloaders['test']):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = saved_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct = (targets==preds).cpu().numpy()\n",
    "        n_correct += np.sum(correct)\n",
    "print(f'(SGD) test accuracy: {n_correct / len(data['test'])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_valid = pd.DataFrame(columns=all_lr, index=all_wd)\n",
    "\n",
    "for lr in all_lr:\n",
    "    for wd in all_wd:\n",
    "        print('\\n============================================')\n",
    "        print(f'train Adam lr = {lr}')\n",
    "        weight_path = f'{base_path}/Q2_weight_adam_{lr}_{wd}'\n",
    "\n",
    "        # load pretrained resnet50 and set the output dimension to 4\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "        model.to(device)\n",
    "        print(f'model at {device}')\n",
    "\n",
    "        # train\n",
    "        optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        adam_valid[lr][wd] = train(optim, model, weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adam_valid)\n",
    "print('(x_axis: learning rate)')\n",
    "print('(y_axis: weight decay)\\n')\n",
    "\n",
    "# minimum = sgd_valid.min(axis=0).min()\n",
    "# sgd_valid = sgd_valid.astype(float)\n",
    "# print(sgd_valid.idxmin(axis=0).idxmin(axis=1))\n",
    "adam_min_wd = -1\n",
    "adam_min_lr = -1\n",
    "adam_minimum = np.inf\n",
    "for lr in adam_valid.columns:\n",
    "    for wd in adam_valid.index:\n",
    "        if adam_valid[lr][wd] < adam_minimum:\n",
    "            adam_min_wd = wd\n",
    "            adam_min_lr = lr\n",
    "            adam_minimum = adam_valid[lr][wd]\n",
    "print(f'minimum validation loss {adam_minimum:.3f} at lr={adam_min_lr}, wd={adam_min_wd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lr in all_lr:\n",
    "#     weight_path = f'{base_path}/Q2_weight_adam_{lr}'\n",
    "#     print(f'loading {weight_path}')\n",
    "#     saved_model = torch.load(weight_path)\n",
    "#     test_size = len(data['test'])\n",
    "#     n_correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch, (inputs, targets) in enumerate(dataloaders['test']):\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = saved_model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             correct = (targets==preds).cpu().numpy()\n",
    "#             n_correct += np.sum(correct)\n",
    "#     print(f'Adam, lr = {lr}: test accuracy: ', n_correct / len(data['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = f'{base_path}/Q2_weight_adam_{min_lr}_{min_wd}'\n",
    "print(f'load {weight_path}')\n",
    "saved_model = torch.load(weight_path)\n",
    "test_size = len(data['test'])\n",
    "n_correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch, (inputs, targets) in enumerate(dataloaders['test']):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = saved_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct = (targets==preds).cpu().numpy()\n",
    "        n_correct += np.sum(correct)\n",
    "print(f'(Adam) test accuracy: {n_correct / len(data['test'])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
