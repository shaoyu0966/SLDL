{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning and Deep Learning HW5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['train', 'valid' ,'test']\n",
    "labels = ['blazer', 'cardigan', 'coat', 'jacket']\n",
    "base_path = '/tmp2/b06705028/sldl'\n",
    "data_path = f'{base_path}/photos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      blazer cardigan coat jacket  total\n",
      "train     97      237  296    411   1041\n",
      "valid      7       36   27     35    105\n",
      "test       9       42   43     52    146\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=labels, index=datasets)\n",
    "for ds in datasets:\n",
    "    for lb in labels:\n",
    "        basepath = os.path.join(f'{data_path}/{ds}/{lb}/', '*.jpg')\n",
    "        cand_fn = glob.glob(basepath)\n",
    "        df[lb][ds] = len(cand_fn)\n",
    "df['total'] = df.sum(axis=1).astype('int')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio:\n",
      "         blazer  cardigan      coat    jacket\n",
      "train  0.093180  0.227666  0.284342  0.394813\n",
      "valid  0.066667  0.342857  0.257143  0.333333\n",
      "test   0.061644  0.287671  0.294521  0.356164\n"
     ]
    }
   ],
   "source": [
    "print('Ratio:')\n",
    "df = df.drop(['total'], axis=1)\n",
    "print (df.div(df.sum(axis=1), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the number of instances of each image type, I suggest that the accuracy of the classification task will be jacket > coat > cardigan > blazer. This follows the hypothesis that larger number of instances in training set causes higher classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "image_transforms = {\n",
    "    'train':\n",
    "        transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.RandomResizedCrop(size=(224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=20),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \n",
    "    'valid':\n",
    "        transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \n",
    "    'test':\n",
    "        transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets from folders\n",
    "data = {\n",
    "    'train': ImageFolder(root=f'{data_path}/train/', transform=image_transforms['train']),\n",
    "    'valid': ImageFolder(root=f'{data_path}/valid/', transform=image_transforms['valid']),\n",
    "    'test': ImageFolder(root=f'{data_path}/test/', transform=image_transforms['test'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "batch_size = 32\n",
    "dataloaders = {\n",
    "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'valid': DataLoader(data['valid'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 0\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'using {device} {torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# hyper-parameters\n",
    "all_lr = [0.001, 0.005, 0.01, 0.05]\n",
    "all_wd = [0, 0.0001, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(model, data_loader):\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss += loss_fn(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optim, model, weight_path, fix_weight=True, early_stop_patient=20, max_epoch=200):\n",
    "    best_valid_loss = np.inf\n",
    "    best_valid_epoch = 0\n",
    "    for epoch in range(max_epoch):\n",
    "        # train\n",
    "        train_loss = 0\n",
    "        for batch, (inputs, targets) in enumerate(dataloaders['train']):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = cross_entropy_loss(model, dataloaders['valid']).cpu().numpy()\n",
    " \n",
    "        # update weight if lower validation loss\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_epoch = epoch\n",
    "            torch.save(model, weight_path)\n",
    "            \n",
    "        # early stopping\n",
    "        elif (epoch - best_valid_epoch >= early_stop_patient):\n",
    "            print(f'early stopping at epoch {epoch}, best_valid = {best_valid_loss:.3f}')\n",
    "            return best_valid_loss\n",
    "        \n",
    "        if (epoch % 20 == 0):\n",
    "            print(f'epoch {epoch}: train_loss = {train_loss:.3f}, valid_loss = {valid_loss:.3f}, best_valid = {best_valid_loss:.3f}')\n",
    "    \n",
    "    return best_valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================\n",
      "train SGD lr = 0.001, weight_decay = 0\n",
      "model at cuda\n",
      "epoch 0: train_loss = 43.169, valid_loss = 5.374, best_valid = 5.374\n",
      "epoch 20: train_loss = 34.604, valid_loss = 4.356, best_valid = 4.337\n",
      "epoch 40: train_loss = 26.801, valid_loss = 3.575, best_valid = 3.533\n",
      "epoch 60: train_loss = 21.203, valid_loss = 3.097, best_valid = 3.097\n",
      "epoch 80: train_loss = 18.192, valid_loss = 3.009, best_valid = 2.931\n",
      "epoch 100: train_loss = 13.852, valid_loss = 2.945, best_valid = 2.719\n",
      "early stopping at epoch 115, best_valid = 2.719\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.001, weight_decay = 0.0001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 43.577, valid_loss = 5.313, best_valid = 5.313\n",
      "epoch 20: train_loss = 34.108, valid_loss = 4.217, best_valid = 4.217\n",
      "epoch 40: train_loss = 27.210, valid_loss = 3.535, best_valid = 3.535\n",
      "epoch 60: train_loss = 22.111, valid_loss = 3.053, best_valid = 2.922\n",
      "epoch 80: train_loss = 18.435, valid_loss = 2.972, best_valid = 2.881\n",
      "epoch 100: train_loss = 13.566, valid_loss = 3.192, best_valid = 2.870\n",
      "early stopping at epoch 104, best_valid = 2.870\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.001, weight_decay = 0.001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 44.052, valid_loss = 5.110, best_valid = 5.110\n",
      "epoch 20: train_loss = 34.612, valid_loss = 4.329, best_valid = 4.329\n",
      "epoch 40: train_loss = 27.439, valid_loss = 3.454, best_valid = 3.454\n",
      "epoch 60: train_loss = 22.677, valid_loss = 3.092, best_valid = 3.092\n",
      "epoch 80: train_loss = 18.600, valid_loss = 2.982, best_valid = 2.888\n",
      "epoch 100: train_loss = 14.275, valid_loss = 2.853, best_valid = 2.829\n",
      "early stopping at epoch 115, best_valid = 2.829\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.005, weight_decay = 0\n",
      "model at cuda\n",
      "epoch 0: train_loss = 42.465, valid_loss = 4.850, best_valid = 4.850\n",
      "epoch 20: train_loss = 16.319, valid_loss = 2.915, best_valid = 2.790\n",
      "epoch 40: train_loss = 8.617, valid_loss = 2.850, best_valid = 2.665\n",
      "early stopping at epoch 51, best_valid = 2.665\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.005, weight_decay = 0.0001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 42.618, valid_loss = 5.127, best_valid = 5.127\n",
      "epoch 20: train_loss = 15.207, valid_loss = 3.280, best_valid = 2.672\n",
      "early stopping at epoch 38, best_valid = 2.672\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.005, weight_decay = 0.001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 42.177, valid_loss = 4.884, best_valid = 4.884\n",
      "epoch 20: train_loss = 16.749, valid_loss = 3.192, best_valid = 3.108\n",
      "epoch 40: train_loss = 9.520, valid_loss = 3.713, best_valid = 2.863\n",
      "early stopping at epoch 49, best_valid = 2.863\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.01, weight_decay = 0\n",
      "model at cuda\n",
      "epoch 0: train_loss = 43.181, valid_loss = 5.005, best_valid = 5.005\n",
      "epoch 20: train_loss = 10.819, valid_loss = 4.213, best_valid = 2.809\n",
      "early stopping at epoch 34, best_valid = 2.809\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.01, weight_decay = 0.0001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 43.638, valid_loss = 5.589, best_valid = 5.589\n",
      "epoch 20: train_loss = 9.976, valid_loss = 3.236, best_valid = 3.236\n",
      "early stopping at epoch 40, best_valid = 3.236\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.01, weight_decay = 0.001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 44.090, valid_loss = 4.941, best_valid = 4.941\n",
      "epoch 20: train_loss = 10.893, valid_loss = 5.480, best_valid = 2.654\n",
      "early stopping at epoch 30, best_valid = 2.654\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.05, weight_decay = 0\n",
      "model at cuda\n",
      "epoch 0: train_loss = 80.968, valid_loss = 4.839, best_valid = 4.839\n",
      "epoch 20: train_loss = 11.612, valid_loss = 4.144, best_valid = 3.124\n",
      "early stopping at epoch 37, best_valid = 3.124\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.05, weight_decay = 0.0001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 74.057, valid_loss = 5.149, best_valid = 5.149\n",
      "epoch 20: train_loss = 13.844, valid_loss = 5.896, best_valid = 3.105\n",
      "epoch 40: train_loss = 7.710, valid_loss = 5.681, best_valid = 2.554\n",
      "early stopping at epoch 41, best_valid = 2.554\n",
      "\n",
      "============================================\n",
      "train SGD lr = 0.05, weight_decay = 0.001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 91.877, valid_loss = 5.211, best_valid = 5.211\n",
      "epoch 20: train_loss = 15.316, valid_loss = 7.214, best_valid = 4.261\n",
      "epoch 40: train_loss = 10.655, valid_loss = 5.325, best_valid = 3.536\n",
      "early stopping at epoch 51, best_valid = 3.536\n"
     ]
    }
   ],
   "source": [
    "sgd_valid = pd.DataFrame(columns=all_lr, index=all_wd)\n",
    "\n",
    "for lr in all_lr:\n",
    "    for wd in all_wd:\n",
    "        print('\\n============================================')\n",
    "        print(f'train SGD lr = {lr}, weight_decay = {wd}')\n",
    "        weight_path = f'{base_path}/Q2_weight_{lr}_{wd}'\n",
    "\n",
    "        # load pretrained resnet50 and set the output dimension to 4\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        # for param in model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "        model.to(device)\n",
    "        print(f'model at {device}')\n",
    "\n",
    "        # train\n",
    "        optim = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        valid_loss = train(optim, model, weight_path)\n",
    "        sgd_valid[lr][wd] = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0.001      0.005      0.010      0.050\n",
      "0.0000   2.718784  2.6649704   2.809202  3.1238027\n",
      "0.0001  2.8700833  2.6715279   3.236063   2.553859\n",
      "0.0010  2.8294187  2.8634071  2.6544569  3.5356686\n",
      "(x_axis: learning rate)\n",
      "(y_axis: weight decay)\n",
      "\n",
      "minimum validation loss 2.554 at lr=0.05, wd=0.0001\n"
     ]
    }
   ],
   "source": [
    "print(sgd_valid)\n",
    "print('(x_axis: learning rate)')\n",
    "print('(y_axis: weight decay)\\n')\n",
    "\n",
    "# minimum = sgd_valid.min(axis=0).min()\n",
    "# sgd_valid = sgd_valid.astype(float)\n",
    "# print(sgd_valid.idxmin(axis=0).idxmin(axis=1))\n",
    "sgd_min_wd = -1\n",
    "sgd_min_lr = -1\n",
    "sgd_minimum = np.inf\n",
    "for lr in sgd_valid.columns:\n",
    "    for wd in sgd_valid.index:\n",
    "        if sgd_valid[lr][wd] < sgd_minimum:\n",
    "            sgd_min_wd = wd\n",
    "            sgd_min_lr = lr\n",
    "            sgd_minimum = sgd_valid[lr][wd]\n",
    "print(f'minimum validation loss {sgd_minimum:.3f} at lr={sgd_min_lr}, wd={sgd_min_wd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lr in all_lr:\n",
    "#     weight_path = f'{base_path}/Q2_weight_{lr}'\n",
    "#     saved_model = torch.load(weight_path)\n",
    "#     test_size = len(data['test'])\n",
    "#     n_correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch, (inputs, targets) in enumerate(dataloaders['test']):\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = saved_model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             correct = (targets==preds).cpu().numpy()\n",
    "#             n_correct += np.sum(correct)\n",
    "#     print(f'SGD, lr = {lr}: test accuracy: ', n_correct / len(data['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /tmp2/b06705028/sldl/Q2_weight_0.05_0.0001\n",
      "(SGD) test accuracy: 0.6986301369863014\n"
     ]
    }
   ],
   "source": [
    "weight_path = f'{base_path}/Q2_weight_{sgd_min_lr}_{sgd_min_wd}'\n",
    "print(f'load {weight_path}')\n",
    "saved_model = torch.load(weight_path)\n",
    "test_size = len(data['test'])\n",
    "n_correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch, (inputs, targets) in enumerate(dataloaders['test']):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = saved_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct = (targets==preds).cpu().numpy()\n",
    "        n_correct += np.sum(correct)\n",
    "print(f'(SGD) test accuracy: {n_correct / test_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================\n",
      "train Adam lr = 0.001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 43.751, valid_loss = 5.362, best_valid = 5.362\n",
      "epoch 20: train_loss = 34.426, valid_loss = 4.378, best_valid = 4.372\n",
      "epoch 40: train_loss = 27.018, valid_loss = 3.588, best_valid = 3.588\n",
      "epoch 60: train_loss = 21.976, valid_loss = 3.235, best_valid = 3.104\n",
      "epoch 80: train_loss = 18.021, valid_loss = 3.291, best_valid = 2.942\n",
      "early stopping at epoch 93, best_valid = 2.942\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 43.332, valid_loss = 5.137, best_valid = 5.137\n",
      "epoch 20: train_loss = 34.711, valid_loss = 4.428, best_valid = 4.279\n",
      "epoch 40: train_loss = 26.713, valid_loss = 3.826, best_valid = 3.685\n",
      "epoch 60: train_loss = 21.772, valid_loss = 3.289, best_valid = 3.134\n",
      "epoch 80: train_loss = 18.394, valid_loss = 3.794, best_valid = 3.124\n",
      "epoch 100: train_loss = 14.286, valid_loss = 3.036, best_valid = 2.890\n",
      "early stopping at epoch 109, best_valid = 2.890\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.001\n",
      "model at cuda\n",
      "epoch 0: train_loss = 42.731, valid_loss = 5.369, best_valid = 5.369\n",
      "epoch 20: train_loss = 33.943, valid_loss = 4.535, best_valid = 4.450\n",
      "epoch 40: train_loss = 25.953, valid_loss = 3.717, best_valid = 3.717\n",
      "epoch 60: train_loss = 22.333, valid_loss = 3.495, best_valid = 3.357\n",
      "epoch 80: train_loss = 18.021, valid_loss = 3.383, best_valid = 3.061\n",
      "early stopping at epoch 96, best_valid = 3.061\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.005\n",
      "model at cuda\n",
      "epoch 0: train_loss = 42.653, valid_loss = 5.248, best_valid = 5.248\n",
      "epoch 20: train_loss = 15.687, valid_loss = 2.852, best_valid = 2.852\n",
      "early stopping at epoch 40, best_valid = 2.852\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.005\n",
      "model at cuda\n",
      "epoch 0: train_loss = 42.887, valid_loss = 5.301, best_valid = 5.301\n",
      "epoch 20: train_loss = 17.078, valid_loss = 2.835, best_valid = 2.835\n",
      "early stopping at epoch 40, best_valid = 2.835\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.005\n",
      "model at cuda\n",
      "epoch 0: train_loss = 42.331, valid_loss = 5.004, best_valid = 5.004\n",
      "epoch 20: train_loss = 17.137, valid_loss = 3.546, best_valid = 3.277\n",
      "early stopping at epoch 33, best_valid = 3.277\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.01\n",
      "model at cuda\n",
      "epoch 0: train_loss = 44.028, valid_loss = 5.053, best_valid = 5.053\n",
      "epoch 20: train_loss = 10.170, valid_loss = 3.161, best_valid = 2.688\n",
      "early stopping at epoch 29, best_valid = 2.688\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.01\n",
      "model at cuda\n",
      "epoch 0: train_loss = 42.969, valid_loss = 5.019, best_valid = 5.019\n",
      "epoch 20: train_loss = 11.070, valid_loss = 4.533, best_valid = 3.032\n",
      "epoch 40: train_loss = 6.561, valid_loss = 3.664, best_valid = 2.857\n",
      "early stopping at epoch 41, best_valid = 2.857\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.01\n",
      "model at cuda\n",
      "epoch 0: train_loss = 43.596, valid_loss = 4.692, best_valid = 4.692\n",
      "epoch 20: train_loss = 10.704, valid_loss = 5.456, best_valid = 3.158\n",
      "early stopping at epoch 30, best_valid = 3.158\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.05\n",
      "model at cuda\n",
      "epoch 0: train_loss = 75.314, valid_loss = 129.731, best_valid = 129.731\n",
      "epoch 20: train_loss = 17.551, valid_loss = 4.454, best_valid = 3.835\n",
      "early stopping at epoch 29, best_valid = 3.835\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.05\n",
      "model at cuda\n",
      "epoch 0: train_loss = 89.676, valid_loss = 8.551, best_valid = 8.551\n",
      "epoch 20: train_loss = 14.740, valid_loss = 4.746, best_valid = 3.437\n",
      "early stopping at epoch 36, best_valid = 3.437\n",
      "\n",
      "============================================\n",
      "train Adam lr = 0.05\n",
      "model at cuda\n",
      "epoch 0: train_loss = 85.591, valid_loss = 5.466, best_valid = 5.466\n",
      "epoch 20: train_loss = 13.302, valid_loss = 5.095, best_valid = 3.939\n",
      "epoch 40: train_loss = 10.435, valid_loss = 5.808, best_valid = 3.465\n",
      "early stopping at epoch 43, best_valid = 3.465\n"
     ]
    }
   ],
   "source": [
    "adam_valid = pd.DataFrame(columns=all_lr, index=all_wd)\n",
    "\n",
    "for lr in all_lr:\n",
    "    for wd in all_wd:\n",
    "        print('\\n============================================')\n",
    "        print(f'train Adam lr = {lr}')\n",
    "        weight_path = f'{base_path}/Q2_weight_adam_{lr}_{wd}'\n",
    "\n",
    "        # load pretrained resnet50 and set the output dimension to 4\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "        model.to(device)\n",
    "        print(f'model at {device}')\n",
    "\n",
    "        # train\n",
    "        optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        adam_valid[lr][wd] = train(optim, model, weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0.001      0.005      0.010      0.050\n",
      "0.0000  2.9420996  2.8518026  2.6880324  3.8346212\n",
      "0.0001  2.8902116  2.8352294  2.8571548  3.4373655\n",
      "0.0010     3.0606  3.2767634   3.158164  3.4652069\n",
      "(x_axis: learning rate)\n",
      "(y_axis: weight decay)\n",
      "\n",
      "minimum validation loss 2.688 at lr=0.01, wd=0.0\n"
     ]
    }
   ],
   "source": [
    "print(adam_valid)\n",
    "print('(x_axis: learning rate)')\n",
    "print('(y_axis: weight decay)\\n')\n",
    "\n",
    "# minimum = sgd_valid.min(axis=0).min()\n",
    "# sgd_valid = sgd_valid.astype(float)\n",
    "# print(sgd_valid.idxmin(axis=0).idxmin(axis=1))\n",
    "adam_min_wd = -1\n",
    "adam_min_lr = -1\n",
    "adam_minimum = np.inf\n",
    "for lr in adam_valid.columns:\n",
    "    for wd in adam_valid.index:\n",
    "        if adam_valid[lr][wd] < adam_minimum:\n",
    "            adam_min_wd = wd\n",
    "            adam_min_lr = lr\n",
    "            adam_minimum = adam_valid[lr][wd]\n",
    "print(f'minimum validation loss {adam_minimum:.3f} at lr={adam_min_lr}, wd={adam_min_wd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lr in all_lr:\n",
    "#     weight_path = f'{base_path}/Q2_weight_adam_{lr}'\n",
    "#     print(f'loading {weight_path}')\n",
    "#     saved_model = torch.load(weight_path)\n",
    "#     test_size = len(data['test'])\n",
    "#     n_correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch, (inputs, targets) in enumerate(dataloaders['test']):\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = saved_model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             correct = (targets==preds).cpu().numpy()\n",
    "#             n_correct += np.sum(correct)\n",
    "#     print(f'Adam, lr = {lr}: test accuracy: ', n_correct / len(data['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load /tmp2/b06705028/sldl/Q2_weight_adam_0.01_0\n",
      "(Adam) test accuracy: 0.726027397260274\n"
     ]
    }
   ],
   "source": [
    "weight_path = f'{base_path}/Q2_weight_adam_{adam_min_lr}_{adam_min_wd}'\n",
    "print(f'load {weight_path}')\n",
    "saved_model = torch.load(weight_path)\n",
    "test_size = len(data['test'])\n",
    "n_correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch, (inputs, targets) in enumerate(dataloaders['test']):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = saved_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct = (targets==preds).cpu().numpy()\n",
    "        n_correct += np.sum(correct)\n",
    "print(f'(Adam) test accuracy: {n_correct / test_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
